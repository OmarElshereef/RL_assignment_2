{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cca7845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: wandb in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: gymnasium in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: filelock in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: click>=8.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (6.33.0)\n",
      "Requirement already satisfied: pydantic<3 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (2.12.4)\n",
      "Requirement already satisfied: pyyaml in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (2.44.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gymnasium) (3.1.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: colorama in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from click>=8.0.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch wandb gymnasium numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ec446f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁███████████████████████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇████</td></tr><tr><td>loss</td><td>█▅▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>1</td></tr><tr><td>epoch</td><td>10000</td></tr><tr><td>loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">assignment2_run1</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/dg0usjot' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/dg0usjot</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_194800-dg0usjot\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251112_195544-wdmq7j6l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/wdmq7j6l' target=\"_blank\">assignment2_run1</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/wdmq7j6l' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/wdmq7j6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/wdmq7j6l?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x24ffed3a850>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"\")\n",
    "wandb.init(project=\"RL assignment 2\", name=\"assignment2_run1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "668b7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    A simple replay buffer, as described in the DQN paper and lecture.\n",
    "    Stores transitions and allows for random sampling of batches.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # Use a deque as the memory. It automatically handles max length.\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition (state, action, next_state, reward)\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Select a random batch of transitions for training\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c731fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    The Deep Q-Network model.\n",
    "    It's a simple feed-forward neural network.\n",
    "    Input: State (s)\n",
    "    Output: Q-value for each possible action Q(s, a)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # Define the layers, matching the lecture slide\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "        Uses ReLU activation functions as shown in the lecture\n",
    "        \"\"\"\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        # The final layer returns raw Q-values (no activation)\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfba3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the GPU if available, otherwise use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def select_action(state, policy_net, n_actions, epsilon):\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy policy.\n",
    "    With probability (1-epsilon), it exploits (picks the best action).\n",
    "    With probability (epsilon), it explores (picks a random action).\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    if sample > epsilon:\n",
    "        # EXPLOITATION: Get the best action from the policy_net\n",
    "        with torch.no_grad():\n",
    "            # policy_net(state) returns Q-values for all actions\n",
    "            # .max(1)[1] gets the *index* of the max Q-value\n",
    "            # .view(1, 1) reshapes it to [[action]]\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # EXPLORATION: Pick a random action\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e92cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizeActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wraps the Pendulum-v1 environment to discretize its continuous action space.\n",
    "    The continuous action is a torque in [-2.0, 2.0].\n",
    "    We will map 3 discrete actions {0, 1, 2} to {-2.0, 0.0, 2.0}.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, num_discrete_actions=3):\n",
    "        super().__init__(env)\n",
    "        self.num_actions = num_discrete_actions\n",
    "        # Redefine the action space as discrete\n",
    "        self.action_space = gym.spaces.Discrete(self.num_actions)\n",
    "        # Create a mapping from discrete action index to continuous torque value\n",
    "        self.action_map = np.linspace(\n",
    "            self.env.action_space.low[0],\n",
    "            self.env.action_space.high[0],\n",
    "            self.num_actions\n",
    "        )\n",
    "\n",
    "    def action(self, action_index):\n",
    "        # Map the discrete action index back to a continuous value\n",
    "        continuous_action = [self.action_map[action_index]]\n",
    "        return np.array(continuous_action, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma, loss_fn):\n",
    "    \"\"\"\n",
    "    Performs one step of DQN optimization (backpropagation).\n",
    "    Samples a batch, computes the loss, and updates the policy_net.\n",
    "    \"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return  # Not enough experiences in memory to sample a batch\n",
    "\n",
    "    # Sample a batch of transitions from replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Create tensors for states, actions, and rewards\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Identify non-final next states\n",
    "    non_final_mask = torch.tensor(tuple(s is not None for s in batch.next_state), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # Compute Q_current: Q(s, a) for the actions taken\n",
    "    Q_current = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute Q_target: R + gamma * max_a' Q(s', a')\n",
    "    Q_target_next = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        # DQN: Get max Q-value from target network\n",
    "        Q_target_next[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    Q_target = reward_batch + (Q_target_next * gamma)\n",
    "\n",
    "    # Compute loss (Smooth L1 / Huber Loss)\n",
    "    loss = loss_fn(Q_current, Q_target.unsqueeze(1))\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Training Function ---\n",
    "def run_experiment(\n",
    "    env_name,\n",
    "    num_episodes=500,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    eps_decay=1000,\n",
    "    tau=0.005,\n",
    "    lr=1e-4,\n",
    "    memory_size=10000,\n",
    "    wandb_run_name=None,\n",
    "    wandb_group=None\n",
    "):\n",
    "    eps_start = 0.9\n",
    "    eps_end = 0.05\n",
    "\n",
    "    # Initialize wandb\n",
    "    if wandb_run_name is None:\n",
    "        wandb_run_name = f\"{env_name}_DQN_{int(time.time())}\"\n",
    "\n",
    "    config = {\n",
    "        \"env_name\": env_name,\n",
    "        \"model_type\": \"DQN\",\n",
    "        \"num_episodes\": num_episodes,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"eps_start\": eps_start,\n",
    "        \"eps_end\": eps_end,\n",
    "        \"eps_decay\": eps_decay,\n",
    "        \"tau\": tau,\n",
    "        \"lr\": lr,\n",
    "        \"memory_size\": memory_size,\n",
    "    }\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"cmps458_assignment2\",\n",
    "        name=wandb_run_name,\n",
    "        group=wandb_group,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # Setup Environment\n",
    "    if env_name == \"Pendulum-v1\":\n",
    "        env = gym.make(env_name)\n",
    "        env = DiscretizeActionWrapper(env, num_discrete_actions=5)\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    state, info = env.reset()\n",
    "    n_observations = len(state)\n",
    "\n",
    "    # Initialize Networks and Optimizer\n",
    "    policy_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "    memory = ReplayMemory(memory_size)\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    print(f\"--- Starting Training: {wandb_run_name} (Group: {wandb_group}) ---\")\n",
    "\n",
    "    # Training Loop\n",
    "    steps_done = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        episode_duration = 0\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            epsilon = eps_end + (eps_start - eps_end) * math.exp(-1. * steps_done / eps_decay)\n",
    "            action = select_action(state, policy_net, n_actions, epsilon)\n",
    "            steps_done += 1\n",
    "\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            episode_duration += 1\n",
    "            episode_reward += reward\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_state = None if terminated else torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            # Optimize model\n",
    "            optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma, loss_fn)\n",
    "\n",
    "            # Soft update of target network (more efficient)\n",
    "            with torch.no_grad():\n",
    "                for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "                    target_param.data.mul_(1 - tau).add_(policy_param.data, alpha=tau)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Log episode metrics\n",
    "        wandb.log({\n",
    "            \"episode\": i_episode,\n",
    "            \"duration\": episode_duration,\n",
    "            \"episode_reward\": episode_reward,\n",
    "            \"epsilon\": epsilon\n",
    "        })\n",
    "\n",
    "    print(f\"--- Training Complete: {wandb_run_name} ---\")\n",
    "    model_path = f\"{wandb_run_name}.pth\"\n",
    "    torch.save(policy_net.state_dict(), model_path)\n",
    "    run.finish()\n",
    "    return model_path, config\n",
    "\n",
    "\n",
    "# --- Test & Record Function ---\n",
    "def test_and_record(\n",
    "    env_name,\n",
    "    model_type,\n",
    "    model_path,\n",
    "    num_tests=100,\n",
    "    wandb_run_name=None,\n",
    "    wandb_group=None,\n",
    "    config=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Tests a trained DQN agent and records video of first episode.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Testing {model_type} on {env_name} ---\")\n",
    "\n",
    "    if wandb_run_name is None:\n",
    "        wandb_run_name = f\"test_{model_type}\"\n",
    "\n",
    "    if config is None:\n",
    "        config = {\"env_name\": env_name, \"model_type\": model_type, \"num_tests\": num_tests}\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"cmps458_assignment2_tests\",\n",
    "        name=wandb_run_name,\n",
    "        group=wandb_group,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # Setup Environment\n",
    "    if env_name == \"Pendulum-v1\":\n",
    "        base_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        env = DiscretizeActionWrapper(base_env, num_discrete_actions=5)\n",
    "    else:\n",
    "        env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "    # Wrap for Video Recording - use simpler path for Colab compatibility\n",
    "    video_folder = f\"./videos_{wandb_run_name}\"\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=video_folder,\n",
    "        episode_trigger=lambda e: e == 0,\n",
    "        name_prefix=f\"{model_type}\",\n",
    "        disable_logger=True  # Disable verbose logging\n",
    "    )\n",
    "\n",
    "    # Load Model\n",
    "    n_actions = env.action_space.n\n",
    "    state, info = env.reset()\n",
    "    n_observations = len(state)\n",
    "\n",
    "    model = DQN(n_observations, n_actions).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Run Tests\n",
    "    test_durations = []\n",
    "    test_rewards = []\n",
    "    \n",
    "    for i_episode in range(num_tests):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        episode_duration = 0\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            action = select_action(state, model, n_actions, epsilon=0.0)  # Greedy\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            episode_duration += 1\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        wandb.log({\n",
    "            \"test_episode\": i_episode,\n",
    "            \"test_duration\": episode_duration,\n",
    "            \"test_reward\": episode_reward\n",
    "        })\n",
    "        test_durations.append(episode_duration)\n",
    "        test_rewards.append(episode_reward)\n",
    "\n",
    "    # Close environment to ensure video is written\n",
    "    env.close()\n",
    "    \n",
    "    avg_duration = sum(test_durations) / num_tests\n",
    "    avg_reward = sum(test_rewards) / num_tests\n",
    "    \n",
    "    print(f\"Testing complete. Video saved in '{video_folder}'\")\n",
    "    print(f\"Average duration: {avg_duration:.2f}, Average reward: {avg_reward:.2f}\")\n",
    "\n",
    "    # Upload video to wandb if it exists\n",
    "    video_files = [f for f in os.listdir(video_folder) if f.endswith('.mp4')]\n",
    "    if video_files:\n",
    "        video_path = os.path.join(video_folder, video_files[0])\n",
    "        wandb.log({\"test_video\": wandb.Video(video_path, fps=30, format=\"mp4\")})\n",
    "        print(f\"Video uploaded to wandb: {video_files[0]}\")\n",
    "\n",
    "    wandb.log({\"avg_test_duration\": avg_duration, \"avg_test_reward\": avg_reward})\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89500c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">assignment2_run1</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/wdmq7j6l' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/wdmq7j6l</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_195544-wdmq7j6l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251112_195546-fcjygl1c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2/runs/fcjygl1c' target=\"_blank\">CartPole_DQN_lr-0.001</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2/runs/fcjygl1c' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2/runs/fcjygl1c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training for CartPole_DQN_lr-0.001 (Group: CartPole_LR_Test) ---\n",
      "Config: {'env_name': 'CartPole-v1', 'model_type': 'DQN', 'num_episodes': 500, 'batch_size': 128, 'gamma': 0.99, 'eps_start': 0.9, 'eps_end': 0.05, 'eps_decay': 1000, 'tau': 0.005, 'lr': 0.0001, 'memory_size': 10000}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m group_name = \u001b[33m\"\u001b[39m\u001b[33mCartPole_LR_Test\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m run_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCartPole_DQN_lr-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m0.001\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model_path, config = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCartPole-v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_ddqn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_group\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_name\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m test_and_record(\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCartPole-v1\u001b[39m\u001b[33m\"\u001b[39m, run_name, model_path,\n\u001b[32m     10\u001b[39m     wandb_run_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, wandb_group=group_name, config=config\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[33;03m\"\"\" # --- CartPole-v1: DDQN ---\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mcartpole_ddqn_path,config_cartPole_ddqn = run_experiment(\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    env_name=\"CartPole-v1\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03mtest_and_record(\"CartPole-v1\", \"DDQN\", cartpole_ddqn_path) \"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(env_name, is_ddqn, num_episodes, batch_size, gamma, eps_start, eps_end, eps_decay, tau, lr, memory_size, wandb_run_name, wandb_group)\u001b[39m\n\u001b[32m     93\u001b[39m memory.push(state, action, next_state, reward)\n\u001b[32m     94\u001b[39m state = next_state\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_ddqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[32m     99\u001b[39m target_net_state_dict = target_net.state_dict()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36moptimize_model\u001b[39m\u001b[34m(is_ddqn, policy_net, target_net, optimizer, memory, batch_size, gamma, loss_fn)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Gradient Clipping (as mentioned in lecture)\u001b[39;00m\n\u001b[32m     55\u001b[39m torch.nn.utils.clip_grad_value_(policy_net.parameters(), \u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:411\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_decay != \u001b[32m0\u001b[39m:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoupled_weight_decay:\n\u001b[32m    410\u001b[39m         \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m         \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    413\u001b[39m         \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    414\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight_decay, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- MountainCar-v0: DQN ---\n",
    "group_name = \"MountainCar_Test\"\n",
    "run_name = f\"MountainCar_DQN_lr-{0.001}\"\n",
    "model_path, config = run_experiment(\n",
    "    env_name=\"MountainCar-v0\",\n",
    "    num_episodes=500,\n",
    "    wandb_run_name=run_name,\n",
    "    wandb_group=group_name\n",
    ")\n",
    "test_and_record(\n",
    "    \"MountainCar-v0\", run_name, model_path,\n",
    "    wandb_run_name=f\"test_{run_name}\",\n",
    "    wandb_group=group_name,\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3235a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MountainCar-v0 Hyperparameter Sweep ---\n",
    "\n",
    "# 1. Test Learning Rates (lr)\n",
    "lr_tests = [1e-3, 1e-4]\n",
    "for lr in lr_tests:\n",
    "    group_name = \"MountainCar_LR_Test\"\n",
    "    run_name = f\"MountainCar_DQN_lr-{lr}\"\n",
    "\n",
    "    model_path, config = run_experiment(\n",
    "        env_name=\"MountainCar-v0\",\n",
    "        num_episodes=500,\n",
    "        lr=lr,  # <-- Variable we are testing\n",
    "        wandb_run_name=run_name,\n",
    "        wandb_group=group_name\n",
    "    )\n",
    "    test_and_record(\n",
    "        \"MountainCar-v0\", run_name, model_path,\n",
    "        wandb_run_name=f\"test_{run_name}\",\n",
    "        wandb_group=group_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "# 2. Test Discount Factors (gamma)\n",
    "gamma_tests = [0.99, 0.9]\n",
    "for gamma in gamma_tests:\n",
    "    group_name = \"MountainCar_Gamma_Test\"\n",
    "    run_name = f\"MountainCar_DQN_gamma-{gamma}\"\n",
    "\n",
    "    model_path, config = run_experiment(\n",
    "        env_name=\"MountainCar-v0\",\n",
    "        num_episodes=500,\n",
    "        gamma=gamma,  # <-- Variable we are testing\n",
    "        wandb_run_name=run_name,\n",
    "        wandb_group=group_name\n",
    "    )\n",
    "    test_and_record(\n",
    "        \"MountainCar-v0\", run_name, model_path,\n",
    "        wandb_run_name=f\"test_{run_name}\",\n",
    "        wandb_group=group_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "\n",
    "# 3. Test Epsilon Decay Rates\n",
    "eps_decay_tests = [1000, 5000]\n",
    "for eps_decay in eps_decay_tests:\n",
    "    group_name = \"MountainCar_EpsDecay_Test\"\n",
    "    run_name = f\"MountainCar_DQN_eps-{eps_decay}\"\n",
    "\n",
    "    model_path, config = run_experiment(\n",
    "        env_name=\"MountainCar-v0\",\n",
    "        num_episodes=1000,\n",
    "        lr=1e-3,\n",
    "        eps_decay=eps_decay,  # <-- Variable we are testing\n",
    "        wandb_run_name=run_name,\n",
    "        wandb_group=group_name\n",
    "    )\n",
    "    test_and_record(\n",
    "        \"MountainCar-v0\", run_name, model_path,\n",
    "        wandb_run_name=f\"test_{run_name}\",\n",
    "        wandb_group=group_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "# 4. Test Replay Memory Sizes\n",
    "memory_tests = [10000, 50000]\n",
    "for mem_size in memory_tests:\n",
    "    group_name = \"MountainCar_Memory_Test\"\n",
    "    run_name = f\"MountainCar_DQN_mem-{mem_size}\"\n",
    "\n",
    "    model_path, config = run_experiment(\n",
    "        env_name=\"MountainCar-v0\",\n",
    "        num_episodes=1000,\n",
    "        lr=1e-3,\n",
    "        memory_size=mem_size,  # <-- Variable we are testing\n",
    "        wandb_run_name=run_name,\n",
    "        wandb_group=group_name\n",
    "    )\n",
    "    test_and_record(\n",
    "        \"MountainCar-v0\", run_name, model_path,\n",
    "        wandb_run_name=f\"test_{run_name}\",\n",
    "        wandb_group=group_name,\n",
    "        config=config\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
