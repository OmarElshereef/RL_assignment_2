{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cca7845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: wandb in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: gymnasium in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: filelock in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: click>=8.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (6.33.0)\n",
      "Requirement already satisfied: pydantic<3 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (2.12.4)\n",
      "Requirement already satisfied: pyyaml in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from wandb) (2.44.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from pydantic<3->wandb) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gymnasium) (3.1.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: moviepy>=1.0.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gymnasium[other]) (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gymnasium[other]) (3.10.7)\n",
      "Requirement already satisfied: opencv-python>=3.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gymnasium[other]) (4.12.0.88)\n",
      "Requirement already satisfied: seaborn>=0.13 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gymnasium[other]) (0.13.2)\n",
      "Collecting pygame>=2.1.3 (from gymnasium[classic-control])\n",
      "  Using cached pygame-2.6.1-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from click>=8.0.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from matplotlib>=3.0->gymnasium[other]) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from matplotlib>=3.0->gymnasium[other]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from matplotlib>=3.0->gymnasium[other]) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from matplotlib>=3.0->gymnasium[other]) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from matplotlib>=3.0->gymnasium[other]) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from matplotlib>=3.0->gymnasium[other]) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from matplotlib>=3.0->gymnasium[other]) (2.9.0.post0)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[other]) (5.2.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[other]) (2.37.2)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[other]) (0.6.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[other]) (0.1.12)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from moviepy>=1.0.0->gymnasium[other]) (1.2.1)\n",
      "Requirement already satisfied: tqdm in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from proglog<=1.0.0->moviepy>=1.0.0->gymnasium[other]) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[other]) (1.17.0)\n",
      "Requirement already satisfied: pandas>=1.2 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from seaborn>=0.13->gymnasium[other]) (2.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn>=0.13->gymnasium[other]) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\materials for college\\fall 2025-2026\\reinforcement learning\\assignments\\assignment 2\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached pygame-2.6.1-cp313-cp313-win_amd64.whl (10.6 MB)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch wandb gymnasium numpy gymnasium[other] gymnasium[classic-control]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec446f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import time\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\medor\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33momarelshereef\u001b[0m (\u001b[33momarelshereef-cairo-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_061614-j7c142hj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/j7c142hj' target=\"_blank\">assignment2_run1</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/j7c142hj' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/j7c142hj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/j7c142hj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2d9b671c6e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"\")\n",
    "wandb.init(project=\"RL assignment 2\", name=\"assignment2_run1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668b7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    A simple replay buffer, as described in the DQN paper and lecture.\n",
    "    Stores transitions and allows for random sampling of batches.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # Use a deque as the memory. It automatically handles max length.\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition (state, action, next_state, reward)\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Select a random batch of transitions for training\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c731fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    The Deep Q-Network model.\n",
    "    It's a simple feed-forward neural network.\n",
    "    Input: State (s)\n",
    "    Output: Q-value for each possible action Q(s, a)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # Define the layers, matching the lecture slide\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "        Uses ReLU activation functions as shown in the lecture\n",
    "        \"\"\"\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        # The final layer returns raw Q-values (no activation)\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfba3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the GPU if available, otherwise use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def select_action(state, policy_net, n_actions, epsilon):\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy policy.\n",
    "    With probability (1-epsilon), it exploits (picks the best action).\n",
    "    With probability (epsilon), it explores (picks a random action).\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    if sample > epsilon:\n",
    "        # EXPLOITATION: Get the best action from the policy_net\n",
    "        with torch.no_grad():\n",
    "            # policy_net(state) returns Q-values for all actions\n",
    "            # .max(1)[1] gets the *index* of the max Q-value\n",
    "            # .view(1, 1) reshapes it to [[action]]\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # EXPLORATION: Pick a random action\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e92cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizeActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wraps the Pendulum-v1 environment to discretize its continuous action space.\n",
    "    The continuous action is a torque in [-2.0, 2.0].\n",
    "    We will map 3 discrete actions {0, 1, 2} to {-2.0, 0.0, 2.0}.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, num_discrete_actions=3):\n",
    "        super().__init__(env)\n",
    "        self.num_actions = num_discrete_actions\n",
    "        # Redefine the action space as discrete\n",
    "        self.action_space = gym.spaces.Discrete(self.num_actions)\n",
    "        # Create a mapping from discrete action index to continuous torque value\n",
    "        self.action_map = np.linspace(\n",
    "            self.env.action_space.low[0],\n",
    "            self.env.action_space.high[0],\n",
    "            self.num_actions\n",
    "        )\n",
    "\n",
    "    def action(self, action_index):\n",
    "        # Map the discrete action index back to a continuous value\n",
    "        continuous_action = [self.action_map[action_index]]\n",
    "        return np.array(continuous_action, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f32a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma, loss_fn):\n",
    "    \"\"\"\n",
    "    Performs one step of DQN optimization (backpropagation).\n",
    "    Samples a batch, computes the loss, and updates the policy_net.\n",
    "    \"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return  # Not enough experiences in memory to sample a batch\n",
    "\n",
    "    # Sample a batch of transitions from replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Create tensors for states, actions, and rewards\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Identify non-final next states\n",
    "    non_final_mask = torch.tensor(tuple(s is not None for s in batch.next_state), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # Compute Q_current: Q(s, a) for the actions taken\n",
    "    Q_current = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute Q_target: R + gamma * max_a' Q(s', a')\n",
    "    Q_target_next = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        # DQN: Get max Q-value from target network\n",
    "        Q_target_next[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    Q_target = reward_batch + (Q_target_next * gamma)\n",
    "\n",
    "    # Compute loss (Smooth L1 / Huber Loss)\n",
    "    loss = loss_fn(Q_current, Q_target.unsqueeze(1))\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a5a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Training Function ---\n",
    "def run_experiment(\n",
    "    env_name,\n",
    "    num_episodes=500,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    eps_decay=1000,\n",
    "    tau=0.005,\n",
    "    lr=1e-4,\n",
    "    memory_size=10000,\n",
    "    wandb_run_name=None,\n",
    "    wandb_group=None\n",
    "):\n",
    "    eps_start = 0.9\n",
    "    eps_end = 0.05\n",
    "\n",
    "    # Initialize wandb\n",
    "    if wandb_run_name is None:\n",
    "        wandb_run_name = f\"{env_name}_DQN_{int(time.time())}\"\n",
    "\n",
    "    config = {\n",
    "        \"env_name\": env_name,\n",
    "        \"model_type\": \"DQN\",\n",
    "        \"num_episodes\": num_episodes,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"eps_start\": eps_start,\n",
    "        \"eps_end\": eps_end,\n",
    "        \"eps_decay\": eps_decay,\n",
    "        \"tau\": tau,\n",
    "        \"lr\": lr,\n",
    "        \"memory_size\": memory_size,\n",
    "    }\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"cmps458_assignment2\",\n",
    "        name=wandb_run_name,\n",
    "        group=wandb_group,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # Setup Environment\n",
    "    if env_name == \"Pendulum-v1\":\n",
    "        env = gym.make(env_name)\n",
    "        env = DiscretizeActionWrapper(env, num_discrete_actions=5)\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    state, info = env.reset()\n",
    "    n_observations = len(state)\n",
    "\n",
    "    # Initialize Networks and Optimizer\n",
    "    policy_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "    memory = ReplayMemory(memory_size)\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    print(f\"--- Starting Training: {wandb_run_name} (Group: {wandb_group}) ---\")\n",
    "\n",
    "    # Training Loop\n",
    "    steps_done = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        episode_duration = 0\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            epsilon = eps_end + (eps_start - eps_end) * math.exp(-1. * steps_done / eps_decay)\n",
    "            action = select_action(state, policy_net, n_actions, epsilon)\n",
    "            steps_done += 1\n",
    "\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            episode_duration += 1\n",
    "            episode_reward += reward\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_state = None if terminated else torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            # Optimize model\n",
    "            optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma, loss_fn)\n",
    "\n",
    "            # Soft update of target network (more efficient)\n",
    "            with torch.no_grad():\n",
    "                for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "                    target_param.data.mul_(1 - tau).add_(policy_param.data, alpha=tau)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Log episode metrics\n",
    "        wandb.log({\n",
    "            \"episode\": i_episode,\n",
    "            \"duration\": episode_duration,\n",
    "            \"episode_reward\": episode_reward,\n",
    "            \"epsilon\": epsilon\n",
    "        })\n",
    "\n",
    "    print(f\"--- Training Complete: {wandb_run_name} ---\")\n",
    "    model_path = f\"{wandb_run_name}.pth\"\n",
    "    torch.save(policy_net.state_dict(), model_path)\n",
    "    run.finish()\n",
    "    return model_path, config\n",
    "\n",
    "\n",
    "# --- Test & Record Function ---\n",
    "def test_and_record(\n",
    "    env_name,\n",
    "    model_type,\n",
    "    model_path,\n",
    "    num_tests=100,\n",
    "    wandb_run_name=None,\n",
    "    wandb_group=None,\n",
    "    config=None\n",
    "):\n",
    "    print(f\"\\n--- Testing {model_type} on {env_name} ---\")\n",
    "\n",
    "    if wandb_run_name is None:\n",
    "        wandb_run_name = f\"test_{model_type}\"\n",
    "\n",
    "    if config is None:\n",
    "        config = {\"env_name\": env_name, \"model_type\": model_type, \"num_tests\": num_tests}\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"cmps458_assignment2_tests\",\n",
    "        name=wandb_run_name,\n",
    "        group=wandb_group,\n",
    "        config=config,\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    # Setup Environment\n",
    "    base_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    env = RecordVideo(\n",
    "        base_env,\n",
    "        video_folder=f\"videos/{wandb_run_name}_{int(time.time())}\",\n",
    "        episode_trigger=lambda e: e == 0,\n",
    "        name_prefix=f\"{model_type}\",\n",
    "        disable_logger=True\n",
    "    )\n",
    "\n",
    "    # Load Model\n",
    "    state, info = env.reset()\n",
    "    env.render()  # ✅ Initialize render buffer once here\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    n_observations = len(state)\n",
    "    model = DQN(n_observations, n_actions).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    model.eval()\n",
    "\n",
    "    test_durations, test_rewards = [], []\n",
    "    for i_episode in range(num_tests):\n",
    "        state, info = env.reset()\n",
    "        env.render()  # ✅ Ensure rendering is active for each episode\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        episode_duration = 0\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "            with torch.no_grad():\n",
    "                action = select_action(state, model, n_actions, epsilon=0.0)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            env.render()  # ✅ Capture frame\n",
    "\n",
    "            episode_duration += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        wandb.log({\"test_episode\": i_episode, \"test_duration\": episode_duration, \"test_reward\": episode_reward})\n",
    "        test_durations.append(episode_duration)\n",
    "        test_rewards.append(episode_reward)\n",
    "\n",
    "    env.close()  # ✅ Properly close to finalize video\n",
    "\n",
    "    video_folder = env.video_folder\n",
    "    video_files = [f for f in os.listdir(video_folder) if f.endswith('.mp4')]\n",
    "    if video_files:\n",
    "        video_path = os.path.join(video_folder, video_files[0])\n",
    "        print(f\"Video file: {video_path}\")\n",
    "        wandb.log({\"test_video\": wandb.Video(video_path, fps=30, format=\"mp4\")})\n",
    "\n",
    "    avg_duration = sum(test_durations) / num_tests\n",
    "    avg_reward = sum(test_rewards) / num_tests\n",
    "    print(f\"\\nTesting complete. Video saved in '{video_folder}'\")\n",
    "    print(f\"Average duration: {avg_duration:.2f}, Average reward: {avg_reward:.2f}\")\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89500c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">assignment2_run1</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/wdmq7j6l' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202/runs/wdmq7j6l</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20assignment%202</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251112_195544-wdmq7j6l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251112_195546-fcjygl1c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2/runs/fcjygl1c' target=\"_blank\">CartPole_DQN_lr-0.001</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2/runs/fcjygl1c' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/cmps458_assignment2/runs/fcjygl1c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training for CartPole_DQN_lr-0.001 (Group: CartPole_LR_Test) ---\n",
      "Config: {'env_name': 'CartPole-v1', 'model_type': 'DQN', 'num_episodes': 500, 'batch_size': 128, 'gamma': 0.99, 'eps_start': 0.9, 'eps_end': 0.05, 'eps_decay': 1000, 'tau': 0.005, 'lr': 0.0001, 'memory_size': 10000}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m group_name = \u001b[33m\"\u001b[39m\u001b[33mCartPole_LR_Test\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m run_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCartPole_DQN_lr-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m0.001\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model_path, config = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCartPole-v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_ddqn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwandb_run_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_group\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_name\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m test_and_record(\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCartPole-v1\u001b[39m\u001b[33m\"\u001b[39m, run_name, model_path,\n\u001b[32m     10\u001b[39m     wandb_run_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, wandb_group=group_name, config=config\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[33;03m\"\"\" # --- CartPole-v1: DDQN ---\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mcartpole_ddqn_path,config_cartPole_ddqn = run_experiment(\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    env_name=\"CartPole-v1\",\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03mtest_and_record(\"CartPole-v1\", \"DDQN\", cartpole_ddqn_path) \"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(env_name, is_ddqn, num_episodes, batch_size, gamma, eps_start, eps_end, eps_decay, tau, lr, memory_size, wandb_run_name, wandb_group)\u001b[39m\n\u001b[32m     93\u001b[39m memory.push(state, action, next_state, reward)\n\u001b[32m     94\u001b[39m state = next_state\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_ddqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[32m     99\u001b[39m target_net_state_dict = target_net.state_dict()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36moptimize_model\u001b[39m\u001b[34m(is_ddqn, policy_net, target_net, optimizer, memory, batch_size, gamma, loss_fn)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Gradient Clipping (as mentioned in lecture)\u001b[39;00m\n\u001b[32m     55\u001b[39m torch.nn.utils.clip_grad_value_(policy_net.parameters(), \u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:411\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_decay != \u001b[32m0\u001b[39m:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoupled_weight_decay:\n\u001b[32m    410\u001b[39m         \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m         \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    413\u001b[39m         \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    414\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight_decay, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- acrobot-v1: DQN ---\n",
    "group_name = \"Acrobot_test\"\n",
    "run_name = f\"Acrobot_test\"\n",
    "model_path, config = run_experiment(\n",
    "    env_name=\"Acrobot-v1\",\n",
    "    num_episodes=500,\n",
    "    wandb_run_name=run_name,\n",
    "    wandb_group=group_name\n",
    ")\n",
    "test_and_record(\n",
    "    \"Acrobot-v1\", run_name, model_path,\n",
    "    wandb_run_name=f\"test_{run_name}\",\n",
    "    wandb_group=group_name,\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3235a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Acrobot-v1 Hyperparameter Sweep ---\n",
    "\n",
    "# 1. Test Learning Rates (lr)\n",
    "lr_tests = [1e-3, 1e-4]\n",
    "for lr in lr_tests:\n",
    "    group_name = \"Acrobot_LR_Test\"\n",
    "    run_name = f\"Acrobot_DQN_lr-{lr}\"\n",
    "\n",
    "    model_path, config = run_experiment(\n",
    "        env_name=\"Acrobot-v1\",\n",
    "        num_episodes=500,\n",
    "        lr=lr,  # <-- Variable we are testing\n",
    "        wandb_run_name=run_name,\n",
    "        wandb_group=group_name\n",
    "    )\n",
    "    test_and_record(\n",
    "        \"Acrobot-v1\", run_name, model_path,\n",
    "        wandb_run_name=f\"test_{run_name}\",\n",
    "        wandb_group=group_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "# 2. Test Discount Factors (gamma)\n",
    "gamma_tests = [0.99, 0.9]\n",
    "for gamma in gamma_tests:\n",
    "    group_name = \"Acrobot_Gamma_Test\"\n",
    "    run_name = f\"Acrobot_DQN_gamma-{gamma}\"\n",
    "\n",
    "    model_path, config = run_experiment(\n",
    "        env_name=\"Acrobot-v1\",\n",
    "        num_episodes=500,\n",
    "        gamma=gamma,  # <-- Variable we are testing\n",
    "        wandb_run_name=run_name,\n",
    "        wandb_group=group_name\n",
    "    )\n",
    "    test_and_record(\n",
    "        \"Acrobot-v1\", run_name, model_path,\n",
    "        wandb_run_name=f\"test_{run_name}\",\n",
    "        wandb_group=group_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "# 1. Test Epsilon Decay Rates\n",
    "eps_decay_tests = [1000, 5000]\n",
    "for eps_decay in eps_decay_tests:\n",
    "    group_name = \"Acrobot_EpsDecay_Test\"\n",
    "    run_name = f\"Acrobot_DDQN_eps-{eps_decay}\" # Let's use DDQN, it's more stable\n",
    "\n",
    "    model_path, config = run_experiment(\n",
    "        env_name=\"Acrobot-v1\", num_episodes=1000, lr=1e-3,\n",
    "        eps_decay=eps_decay, # <-- Variable we are testing\n",
    "        wandb_run_name=run_name, wandb_group=group_name\n",
    "    )\n",
    "    test_and_record(\n",
    "        \"Acrobot-v1\", run_name, model_path,\n",
    "        wandb_run_name=f\"test_{run_name}\", wandb_group=group_name, config=config\n",
    "    )\n",
    "\n",
    "# 2. Test Replay Memory Sizes\n",
    "memory_tests = [10000, 50000]\n",
    "for mem_size in memory_tests:\n",
    "    group_name = \"Acrobot_Memory_Test\"\n",
    "    run_name = f\"Acrobot_DDQN_mem-{mem_size}\"\n",
    "\n",
    "    model_path, config = run_experiment(\n",
    "        env_name=\"Acrobot-v1\", num_episodes=1000, lr=1e-3,\n",
    "        memory_size=mem_size, # <-- Variable we are testing\n",
    "        wandb_run_name=run_name, wandb_group=group_name\n",
    "    )\n",
    "    test_and_record(\n",
    "        \"Acrobot-v1\", run_name, model_path,\n",
    "        wandb_run_name=f\"test_{run_name}\", wandb_group=group_name, config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_model(model_path, env_name, num_discrete_actions=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Setup environment ---\n",
    "    base_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "    # If continuous, apply discretizer\n",
    "    if isinstance(base_env.action_space, gym.spaces.Box):\n",
    "        env = DiscretizeActionWrapper(base_env, num_discrete_actions=num_discrete_actions)\n",
    "    else:\n",
    "        env = base_env\n",
    "\n",
    "    env = RecordVideo(env, video_folder=f\"videos/{env_name}\", episode_trigger=lambda e: True)\n",
    "\n",
    "    # --- Reset and get sizes ---\n",
    "    state, _ = env.reset()\n",
    "    env.render()  # important to initialize frame buffer\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    n_observations = len(state)\n",
    "\n",
    "    # --- Load model ---\n",
    "    model = DQN(n_observations, n_actions).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    model.eval()\n",
    "\n",
    "    # --- Run one episode ---\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward, done = 0, False\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    env.close()\n",
    "    print(f\"✅ Video saved in ./videos/{env_name}/ (total reward = {total_reward:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1b38b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Video saved in ./videos/Pendulum-v1/ (total reward = -124.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:434: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
     ]
    }
   ],
   "source": [
    "record_model(\"./Pendulum_DQN_eps-1000.pth\", \"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fa92755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_models_in_folder(models_folder, env_name, num_discrete_actions=5, num_tests=100):\n",
    "    \"\"\"\n",
    "    Loads all DQN models in a folder, runs multiple test episodes per model,\n",
    "    and records one random episode per model as a video.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- Get all model paths ---\n",
    "    model_files = [f for f in os.listdir(models_folder) if f.endswith(\".pth\")]\n",
    "    if not model_files:\n",
    "        print(\"⚠️ No .pth models found in the folder!\")\n",
    "        return\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(models_folder, model_file)\n",
    "        print(f\"\\n🎬 Testing and recording model: {model_file}\")\n",
    "\n",
    "        # --- Create environment ---\n",
    "        base_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "        # Apply discretization if needed\n",
    "        if isinstance(base_env.action_space, gym.spaces.Box):\n",
    "            env = DiscretizeActionWrapper(base_env, num_discrete_actions=num_discrete_actions)\n",
    "        else:\n",
    "            env = base_env\n",
    "\n",
    "        # --- Setup model ---\n",
    "        sample_state, _ = env.reset()\n",
    "        n_observations = len(sample_state)\n",
    "        n_actions = env.action_space.n\n",
    "\n",
    "        model = DQN(n_observations, n_actions).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        # --- Run test episodes ---\n",
    "        rewards = []\n",
    "        episodes_data = []\n",
    "\n",
    "        for ep in range(num_tests):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            episode_states, episode_actions = [], []\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state_tensor)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                done = terminated or truncated\n",
    "                state = next_state\n",
    "\n",
    "                # Save episode trajectory for later if we want to record this one\n",
    "                episode_states.append(state)\n",
    "                episode_actions.append(action)\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            episodes_data.append((episode_states, episode_actions))\n",
    "\n",
    "            print(f\"  Episode {ep+1}/{num_tests}: reward = {total_reward:.2f}\")\n",
    "\n",
    "        avg_reward = np.mean(rewards)\n",
    "        print(f\"✅ Model {model_file}: avg reward = {avg_reward:.2f}\")\n",
    "\n",
    "        # --- Record a random episode ---\n",
    "        random_ep = random.randint(0, num_tests - 1)\n",
    "        print(f\"🎥 Recording episode {random_ep + 1} for {model_file}...\")\n",
    "\n",
    "        # Recreate env with video recording enabled\n",
    "        video_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        if isinstance(video_env.action_space, gym.spaces.Box):\n",
    "            video_env = DiscretizeActionWrapper(video_env, num_discrete_actions=num_discrete_actions)\n",
    "\n",
    "        video_env = RecordVideo(\n",
    "            video_env,\n",
    "            video_folder=f\"videos/{env_name}/{model_file[:-4]}\",\n",
    "            episode_trigger=lambda e: True,\n",
    "            name_prefix=f\"{model_file[:-4]}\"\n",
    "        )\n",
    "\n",
    "        # Replay the chosen episode\n",
    "        video_env.reset()\n",
    "        done = False\n",
    "        state, _ = video_env.reset()\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_tensor)\n",
    "                action = torch.argmax(q_values).item()\n",
    "            next_state, _, terminated, truncated, _ = video_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "        video_env.close()\n",
    "        print(f\"🎞️ Saved video for {model_file} at ./videos/{env_name}/{model_file[:-4]}/\")\n",
    "\n",
    "    print(\"\\n🏁 All models tested and recorded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425634c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_batch-128.pth\n",
      "  Episode 1/100: reward = -93.00\n",
      "  Episode 2/100: reward = -143.00\n",
      "  Episode 3/100: reward = -91.00\n",
      "  Episode 4/100: reward = -98.00\n",
      "  Episode 5/100: reward = -114.00\n",
      "  Episode 6/100: reward = -110.00\n",
      "  Episode 7/100: reward = -125.00\n",
      "  Episode 8/100: reward = -82.00\n",
      "  Episode 9/100: reward = -100.00\n",
      "  Episode 10/100: reward = -85.00\n",
      "  Episode 11/100: reward = -100.00\n",
      "  Episode 12/100: reward = -89.00\n",
      "  Episode 13/100: reward = -86.00\n",
      "  Episode 14/100: reward = -96.00\n",
      "  Episode 15/100: reward = -98.00\n",
      "  Episode 16/100: reward = -175.00\n",
      "  Episode 17/100: reward = -119.00\n",
      "  Episode 18/100: reward = -82.00\n",
      "  Episode 19/100: reward = -129.00\n",
      "  Episode 20/100: reward = -108.00\n",
      "  Episode 21/100: reward = -92.00\n",
      "  Episode 22/100: reward = -96.00\n",
      "  Episode 23/100: reward = -87.00\n",
      "  Episode 24/100: reward = -81.00\n",
      "  Episode 25/100: reward = -98.00\n",
      "  Episode 26/100: reward = -113.00\n",
      "  Episode 27/100: reward = -97.00\n",
      "  Episode 28/100: reward = -124.00\n",
      "  Episode 29/100: reward = -117.00\n",
      "  Episode 30/100: reward = -102.00\n",
      "  Episode 31/100: reward = -127.00\n",
      "  Episode 32/100: reward = -98.00\n",
      "  Episode 33/100: reward = -99.00\n",
      "  Episode 34/100: reward = -78.00\n",
      "  Episode 35/100: reward = -113.00\n",
      "  Episode 36/100: reward = -89.00\n",
      "  Episode 37/100: reward = -103.00\n",
      "  Episode 38/100: reward = -106.00\n",
      "  Episode 39/100: reward = -102.00\n",
      "  Episode 40/100: reward = -106.00\n",
      "  Episode 41/100: reward = -110.00\n",
      "  Episode 42/100: reward = -95.00\n",
      "  Episode 43/100: reward = -102.00\n",
      "  Episode 44/100: reward = -124.00\n",
      "  Episode 45/100: reward = -111.00\n",
      "  Episode 46/100: reward = -96.00\n",
      "  Episode 47/100: reward = -118.00\n",
      "  Episode 48/100: reward = -118.00\n",
      "  Episode 49/100: reward = -113.00\n",
      "  Episode 50/100: reward = -127.00\n",
      "  Episode 51/100: reward = -142.00\n",
      "  Episode 52/100: reward = -147.00\n",
      "  Episode 53/100: reward = -146.00\n",
      "  Episode 54/100: reward = -115.00\n",
      "  Episode 55/100: reward = -86.00\n",
      "  Episode 56/100: reward = -113.00\n",
      "  Episode 57/100: reward = -84.00\n",
      "  Episode 58/100: reward = -105.00\n",
      "  Episode 59/100: reward = -89.00\n",
      "  Episode 60/100: reward = -107.00\n",
      "  Episode 61/100: reward = -106.00\n",
      "  Episode 62/100: reward = -101.00\n",
      "  Episode 63/100: reward = -92.00\n",
      "  Episode 64/100: reward = -84.00\n",
      "  Episode 65/100: reward = -106.00\n",
      "  Episode 66/100: reward = -139.00\n",
      "  Episode 67/100: reward = -133.00\n",
      "  Episode 68/100: reward = -111.00\n",
      "  Episode 69/100: reward = -95.00\n",
      "  Episode 70/100: reward = -124.00\n",
      "  Episode 71/100: reward = -102.00\n",
      "  Episode 72/100: reward = -93.00\n",
      "  Episode 73/100: reward = -96.00\n",
      "  Episode 74/100: reward = -118.00\n",
      "  Episode 75/100: reward = -500.00\n",
      "  Episode 76/100: reward = -107.00\n",
      "  Episode 77/100: reward = -88.00\n",
      "  Episode 78/100: reward = -84.00\n",
      "  Episode 79/100: reward = -99.00\n",
      "  Episode 80/100: reward = -102.00\n",
      "  Episode 81/100: reward = -99.00\n",
      "  Episode 82/100: reward = -121.00\n",
      "  Episode 83/100: reward = -106.00\n",
      "  Episode 84/100: reward = -99.00\n",
      "  Episode 85/100: reward = -106.00\n",
      "  Episode 86/100: reward = -97.00\n",
      "  Episode 87/100: reward = -96.00\n",
      "  Episode 88/100: reward = -117.00\n",
      "  Episode 89/100: reward = -117.00\n",
      "  Episode 90/100: reward = -80.00\n",
      "  Episode 91/100: reward = -90.00\n",
      "  Episode 92/100: reward = -192.00\n",
      "  Episode 93/100: reward = -500.00\n",
      "  Episode 94/100: reward = -104.00\n",
      "  Episode 95/100: reward = -114.00\n",
      "  Episode 96/100: reward = -106.00\n",
      "  Episode 97/100: reward = -95.00\n",
      "  Episode 98/100: reward = -91.00\n",
      "  Episode 99/100: reward = -125.00\n",
      "  Episode 100/100: reward = -500.00\n",
      "✅ Model Acrobot_DQN_batch-128.pth: avg reward = -118.34\n",
      "🎥 Recording episode 64 for Acrobot_DQN_batch-128.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Acrobot_DQN_batch-128.pth at ./videos/Acrobot-v1/Acrobot_DQN_batch-128/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_batch-256.pth\n",
      "  Episode 1/100: reward = -69.00\n",
      "  Episode 2/100: reward = -75.00\n",
      "  Episode 3/100: reward = -73.00\n",
      "  Episode 4/100: reward = -128.00\n",
      "  Episode 5/100: reward = -152.00\n",
      "  Episode 6/100: reward = -92.00\n",
      "  Episode 7/100: reward = -114.00\n",
      "  Episode 8/100: reward = -83.00\n",
      "  Episode 9/100: reward = -90.00\n",
      "  Episode 10/100: reward = -104.00\n",
      "  Episode 11/100: reward = -61.00\n",
      "  Episode 12/100: reward = -97.00\n",
      "  Episode 13/100: reward = -76.00\n",
      "  Episode 14/100: reward = -90.00\n",
      "  Episode 15/100: reward = -87.00\n",
      "  Episode 16/100: reward = -136.00\n",
      "  Episode 17/100: reward = -89.00\n",
      "  Episode 18/100: reward = -84.00\n",
      "  Episode 19/100: reward = -114.00\n",
      "  Episode 20/100: reward = -74.00\n",
      "  Episode 21/100: reward = -77.00\n",
      "  Episode 22/100: reward = -94.00\n",
      "  Episode 23/100: reward = -87.00\n",
      "  Episode 24/100: reward = -78.00\n",
      "  Episode 25/100: reward = -109.00\n",
      "  Episode 26/100: reward = -118.00\n",
      "  Episode 27/100: reward = -101.00\n",
      "  Episode 28/100: reward = -181.00\n",
      "  Episode 29/100: reward = -73.00\n",
      "  Episode 30/100: reward = -92.00\n",
      "  Episode 31/100: reward = -68.00\n",
      "  Episode 32/100: reward = -74.00\n",
      "  Episode 33/100: reward = -89.00\n",
      "  Episode 34/100: reward = -94.00\n",
      "  Episode 35/100: reward = -96.00\n",
      "  Episode 36/100: reward = -84.00\n",
      "  Episode 37/100: reward = -89.00\n",
      "  Episode 38/100: reward = -89.00\n",
      "  Episode 39/100: reward = -73.00\n",
      "  Episode 40/100: reward = -83.00\n",
      "  Episode 41/100: reward = -74.00\n",
      "  Episode 42/100: reward = -125.00\n",
      "  Episode 43/100: reward = -76.00\n",
      "  Episode 44/100: reward = -74.00\n",
      "  Episode 45/100: reward = -92.00\n",
      "  Episode 46/100: reward = -104.00\n",
      "  Episode 47/100: reward = -73.00\n",
      "  Episode 48/100: reward = -98.00\n",
      "  Episode 49/100: reward = -96.00\n",
      "  Episode 50/100: reward = -106.00\n",
      "  Episode 51/100: reward = -68.00\n",
      "  Episode 52/100: reward = -77.00\n",
      "  Episode 53/100: reward = -86.00\n",
      "  Episode 54/100: reward = -69.00\n",
      "  Episode 55/100: reward = -80.00\n",
      "  Episode 56/100: reward = -73.00\n",
      "  Episode 57/100: reward = -79.00\n",
      "  Episode 58/100: reward = -94.00\n",
      "  Episode 59/100: reward = -77.00\n",
      "  Episode 60/100: reward = -77.00\n",
      "  Episode 61/100: reward = -88.00\n",
      "  Episode 62/100: reward = -108.00\n",
      "  Episode 63/100: reward = -87.00\n",
      "  Episode 64/100: reward = -101.00\n",
      "  Episode 65/100: reward = -81.00\n",
      "  Episode 66/100: reward = -118.00\n",
      "  Episode 67/100: reward = -87.00\n",
      "  Episode 68/100: reward = -89.00\n",
      "  Episode 69/100: reward = -88.00\n",
      "  Episode 70/100: reward = -99.00\n",
      "  Episode 71/100: reward = -71.00\n",
      "  Episode 72/100: reward = -132.00\n",
      "  Episode 73/100: reward = -91.00\n",
      "  Episode 74/100: reward = -95.00\n",
      "  Episode 75/100: reward = -94.00\n",
      "  Episode 76/100: reward = -136.00\n",
      "  Episode 77/100: reward = -71.00\n",
      "  Episode 78/100: reward = -71.00\n",
      "  Episode 79/100: reward = -178.00\n",
      "  Episode 80/100: reward = -86.00\n",
      "  Episode 81/100: reward = -71.00\n",
      "  Episode 82/100: reward = -75.00\n",
      "  Episode 83/100: reward = -99.00\n",
      "  Episode 84/100: reward = -239.00\n",
      "  Episode 85/100: reward = -77.00\n",
      "  Episode 86/100: reward = -74.00\n",
      "  Episode 87/100: reward = -93.00\n",
      "  Episode 88/100: reward = -90.00\n",
      "  Episode 89/100: reward = -119.00\n",
      "  Episode 90/100: reward = -77.00\n",
      "  Episode 91/100: reward = -99.00\n",
      "  Episode 92/100: reward = -81.00\n",
      "  Episode 93/100: reward = -117.00\n",
      "  Episode 94/100: reward = -90.00\n",
      "  Episode 95/100: reward = -102.00\n",
      "  Episode 96/100: reward = -304.00\n",
      "  Episode 97/100: reward = -89.00\n",
      "  Episode 98/100: reward = -129.00\n",
      "  Episode 99/100: reward = -152.00\n",
      "  Episode 100/100: reward = -87.00\n",
      "✅ Model Acrobot_DQN_batch-256.pth: avg reward = -96.70\n",
      "🎥 Recording episode 67 for Acrobot_DQN_batch-256.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_batch-256.pth at ./videos/Acrobot-v1/Acrobot_DQN_batch-256/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_batch-64.pth\n",
      "  Episode 1/100: reward = -71.00\n",
      "  Episode 2/100: reward = -70.00\n",
      "  Episode 3/100: reward = -74.00\n",
      "  Episode 4/100: reward = -71.00\n",
      "  Episode 5/100: reward = -70.00\n",
      "  Episode 6/100: reward = -71.00\n",
      "  Episode 7/100: reward = -78.00\n",
      "  Episode 8/100: reward = -71.00\n",
      "  Episode 9/100: reward = -350.00\n",
      "  Episode 10/100: reward = -154.00\n",
      "  Episode 11/100: reward = -70.00\n",
      "  Episode 12/100: reward = -71.00\n",
      "  Episode 13/100: reward = -79.00\n",
      "  Episode 14/100: reward = -105.00\n",
      "  Episode 15/100: reward = -96.00\n",
      "  Episode 16/100: reward = -74.00\n",
      "  Episode 17/100: reward = -70.00\n",
      "  Episode 18/100: reward = -88.00\n",
      "  Episode 19/100: reward = -88.00\n",
      "  Episode 20/100: reward = -71.00\n",
      "  Episode 21/100: reward = -95.00\n",
      "  Episode 22/100: reward = -80.00\n",
      "  Episode 23/100: reward = -76.00\n",
      "  Episode 24/100: reward = -71.00\n",
      "  Episode 25/100: reward = -71.00\n",
      "  Episode 26/100: reward = -89.00\n",
      "  Episode 27/100: reward = -83.00\n",
      "  Episode 28/100: reward = -74.00\n",
      "  Episode 29/100: reward = -71.00\n",
      "  Episode 30/100: reward = -93.00\n",
      "  Episode 31/100: reward = -72.00\n",
      "  Episode 32/100: reward = -74.00\n",
      "  Episode 33/100: reward = -110.00\n",
      "  Episode 34/100: reward = -92.00\n",
      "  Episode 35/100: reward = -96.00\n",
      "  Episode 36/100: reward = -70.00\n",
      "  Episode 37/100: reward = -95.00\n",
      "  Episode 38/100: reward = -72.00\n",
      "  Episode 39/100: reward = -84.00\n",
      "  Episode 40/100: reward = -62.00\n",
      "  Episode 41/100: reward = -86.00\n",
      "  Episode 42/100: reward = -207.00\n",
      "  Episode 43/100: reward = -89.00\n",
      "  Episode 44/100: reward = -127.00\n",
      "  Episode 45/100: reward = -82.00\n",
      "  Episode 46/100: reward = -72.00\n",
      "  Episode 47/100: reward = -70.00\n",
      "  Episode 48/100: reward = -79.00\n",
      "  Episode 49/100: reward = -74.00\n",
      "  Episode 50/100: reward = -87.00\n",
      "  Episode 51/100: reward = -74.00\n",
      "  Episode 52/100: reward = -108.00\n",
      "  Episode 53/100: reward = -89.00\n",
      "  Episode 54/100: reward = -74.00\n",
      "  Episode 55/100: reward = -71.00\n",
      "  Episode 56/100: reward = -75.00\n",
      "  Episode 57/100: reward = -84.00\n",
      "  Episode 58/100: reward = -87.00\n",
      "  Episode 59/100: reward = -91.00\n",
      "  Episode 60/100: reward = -91.00\n",
      "  Episode 61/100: reward = -75.00\n",
      "  Episode 62/100: reward = -88.00\n",
      "  Episode 63/100: reward = -123.00\n",
      "  Episode 64/100: reward = -87.00\n",
      "  Episode 65/100: reward = -69.00\n",
      "  Episode 66/100: reward = -70.00\n",
      "  Episode 67/100: reward = -75.00\n",
      "  Episode 68/100: reward = -70.00\n",
      "  Episode 69/100: reward = -65.00\n",
      "  Episode 70/100: reward = -87.00\n",
      "  Episode 71/100: reward = -84.00\n",
      "  Episode 72/100: reward = -71.00\n",
      "  Episode 73/100: reward = -95.00\n",
      "  Episode 74/100: reward = -75.00\n",
      "  Episode 75/100: reward = -79.00\n",
      "  Episode 76/100: reward = -63.00\n",
      "  Episode 77/100: reward = -77.00\n",
      "  Episode 78/100: reward = -105.00\n",
      "  Episode 79/100: reward = -73.00\n",
      "  Episode 80/100: reward = -77.00\n",
      "  Episode 81/100: reward = -70.00\n",
      "  Episode 82/100: reward = -113.00\n",
      "  Episode 83/100: reward = -62.00\n",
      "  Episode 84/100: reward = -62.00\n",
      "  Episode 85/100: reward = -79.00\n",
      "  Episode 86/100: reward = -70.00\n",
      "  Episode 87/100: reward = -94.00\n",
      "  Episode 88/100: reward = -79.00\n",
      "  Episode 89/100: reward = -70.00\n",
      "  Episode 90/100: reward = -70.00\n",
      "  Episode 91/100: reward = -95.00\n",
      "  Episode 92/100: reward = -71.00\n",
      "  Episode 93/100: reward = -93.00\n",
      "  Episode 94/100: reward = -71.00\n",
      "  Episode 95/100: reward = -70.00\n",
      "  Episode 96/100: reward = -70.00\n",
      "  Episode 97/100: reward = -71.00\n",
      "  Episode 98/100: reward = -109.00\n",
      "  Episode 99/100: reward = -74.00\n",
      "  Episode 100/100: reward = -74.00\n",
      "✅ Model Acrobot_DQN_batch-64.pth: avg reward = -85.14\n",
      "🎥 Recording episode 61 for Acrobot_DQN_batch-64.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_batch-64.pth at ./videos/Acrobot-v1/Acrobot_DQN_batch-64/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_eps-1000.pth\n",
      "  Episode 1/100: reward = -95.00\n",
      "  Episode 2/100: reward = -70.00\n",
      "  Episode 3/100: reward = -76.00\n",
      "  Episode 4/100: reward = -76.00\n",
      "  Episode 5/100: reward = -144.00\n",
      "  Episode 6/100: reward = -100.00\n",
      "  Episode 7/100: reward = -70.00\n",
      "  Episode 8/100: reward = -88.00\n",
      "  Episode 9/100: reward = -86.00\n",
      "  Episode 10/100: reward = -70.00\n",
      "  Episode 11/100: reward = -88.00\n",
      "  Episode 12/100: reward = -96.00\n",
      "  Episode 13/100: reward = -77.00\n",
      "  Episode 14/100: reward = -74.00\n",
      "  Episode 15/100: reward = -75.00\n",
      "  Episode 16/100: reward = -90.00\n",
      "  Episode 17/100: reward = -105.00\n",
      "  Episode 18/100: reward = -89.00\n",
      "  Episode 19/100: reward = -84.00\n",
      "  Episode 20/100: reward = -108.00\n",
      "  Episode 21/100: reward = -84.00\n",
      "  Episode 22/100: reward = -83.00\n",
      "  Episode 23/100: reward = -92.00\n",
      "  Episode 24/100: reward = -127.00\n",
      "  Episode 25/100: reward = -118.00\n",
      "  Episode 26/100: reward = -62.00\n",
      "  Episode 27/100: reward = -77.00\n",
      "  Episode 28/100: reward = -88.00\n",
      "  Episode 29/100: reward = -71.00\n",
      "  Episode 30/100: reward = -74.00\n",
      "  Episode 31/100: reward = -79.00\n",
      "  Episode 32/100: reward = -88.00\n",
      "  Episode 33/100: reward = -75.00\n",
      "  Episode 34/100: reward = -62.00\n",
      "  Episode 35/100: reward = -106.00\n",
      "  Episode 36/100: reward = -107.00\n",
      "  Episode 37/100: reward = -101.00\n",
      "  Episode 38/100: reward = -69.00\n",
      "  Episode 39/100: reward = -88.00\n",
      "  Episode 40/100: reward = -94.00\n",
      "  Episode 41/100: reward = -63.00\n",
      "  Episode 42/100: reward = -78.00\n",
      "  Episode 43/100: reward = -77.00\n",
      "  Episode 44/100: reward = -75.00\n",
      "  Episode 45/100: reward = -63.00\n",
      "  Episode 46/100: reward = -93.00\n",
      "  Episode 47/100: reward = -79.00\n",
      "  Episode 48/100: reward = -77.00\n",
      "  Episode 49/100: reward = -86.00\n",
      "  Episode 50/100: reward = -92.00\n",
      "  Episode 51/100: reward = -62.00\n",
      "  Episode 52/100: reward = -86.00\n",
      "  Episode 53/100: reward = -125.00\n",
      "  Episode 54/100: reward = -78.00\n",
      "  Episode 55/100: reward = -98.00\n",
      "  Episode 56/100: reward = -176.00\n",
      "  Episode 57/100: reward = -94.00\n",
      "  Episode 58/100: reward = -74.00\n",
      "  Episode 59/100: reward = -74.00\n",
      "  Episode 60/100: reward = -76.00\n",
      "  Episode 61/100: reward = -62.00\n",
      "  Episode 62/100: reward = -100.00\n",
      "  Episode 63/100: reward = -71.00\n",
      "  Episode 64/100: reward = -77.00\n",
      "  Episode 65/100: reward = -62.00\n",
      "  Episode 66/100: reward = -89.00\n",
      "  Episode 67/100: reward = -70.00\n",
      "  Episode 68/100: reward = -86.00\n",
      "  Episode 69/100: reward = -75.00\n",
      "  Episode 70/100: reward = -71.00\n",
      "  Episode 71/100: reward = -97.00\n",
      "  Episode 72/100: reward = -75.00\n",
      "  Episode 73/100: reward = -74.00\n",
      "  Episode 74/100: reward = -78.00\n",
      "  Episode 75/100: reward = -101.00\n",
      "  Episode 76/100: reward = -69.00\n",
      "  Episode 77/100: reward = -70.00\n",
      "  Episode 78/100: reward = -77.00\n",
      "  Episode 79/100: reward = -74.00\n",
      "  Episode 80/100: reward = -88.00\n",
      "  Episode 81/100: reward = -71.00\n",
      "  Episode 82/100: reward = -69.00\n",
      "  Episode 83/100: reward = -76.00\n",
      "  Episode 84/100: reward = -101.00\n",
      "  Episode 85/100: reward = -77.00\n",
      "  Episode 86/100: reward = -62.00\n",
      "  Episode 87/100: reward = -76.00\n",
      "  Episode 88/100: reward = -69.00\n",
      "  Episode 89/100: reward = -93.00\n",
      "  Episode 90/100: reward = -110.00\n",
      "  Episode 91/100: reward = -62.00\n",
      "  Episode 92/100: reward = -81.00\n",
      "  Episode 93/100: reward = -77.00\n",
      "  Episode 94/100: reward = -70.00\n",
      "  Episode 95/100: reward = -62.00\n",
      "  Episode 96/100: reward = -86.00\n",
      "  Episode 97/100: reward = -85.00\n",
      "  Episode 98/100: reward = -76.00\n",
      "  Episode 99/100: reward = -77.00\n",
      "  Episode 100/100: reward = -69.00\n",
      "✅ Model Acrobot_DQN_eps-1000.pth: avg reward = -83.47\n",
      "🎥 Recording episode 88 for Acrobot_DQN_eps-1000.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_eps-1000.pth at ./videos/Acrobot-v1/Acrobot_DQN_eps-1000/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_eps-2500.pth\n",
      "  Episode 1/100: reward = -79.00\n",
      "  Episode 2/100: reward = -73.00\n",
      "  Episode 3/100: reward = -85.00\n",
      "  Episode 4/100: reward = -107.00\n",
      "  Episode 5/100: reward = -132.00\n",
      "  Episode 6/100: reward = -79.00\n",
      "  Episode 7/100: reward = -79.00\n",
      "  Episode 8/100: reward = -85.00\n",
      "  Episode 9/100: reward = -79.00\n",
      "  Episode 10/100: reward = -100.00\n",
      "  Episode 11/100: reward = -86.00\n",
      "  Episode 12/100: reward = -76.00\n",
      "  Episode 13/100: reward = -124.00\n",
      "  Episode 14/100: reward = -101.00\n",
      "  Episode 15/100: reward = -74.00\n",
      "  Episode 16/100: reward = -79.00\n",
      "  Episode 17/100: reward = -78.00\n",
      "  Episode 18/100: reward = -62.00\n",
      "  Episode 19/100: reward = -84.00\n",
      "  Episode 20/100: reward = -94.00\n",
      "  Episode 21/100: reward = -128.00\n",
      "  Episode 22/100: reward = -75.00\n",
      "  Episode 23/100: reward = -89.00\n",
      "  Episode 24/100: reward = -87.00\n",
      "  Episode 25/100: reward = -77.00\n",
      "  Episode 26/100: reward = -107.00\n",
      "  Episode 27/100: reward = -91.00\n",
      "  Episode 28/100: reward = -61.00\n",
      "  Episode 29/100: reward = -87.00\n",
      "  Episode 30/100: reward = -79.00\n",
      "  Episode 31/100: reward = -79.00\n",
      "  Episode 32/100: reward = -96.00\n",
      "  Episode 33/100: reward = -79.00\n",
      "  Episode 34/100: reward = -74.00\n",
      "  Episode 35/100: reward = -63.00\n",
      "  Episode 36/100: reward = -70.00\n",
      "  Episode 37/100: reward = -96.00\n",
      "  Episode 38/100: reward = -70.00\n",
      "  Episode 39/100: reward = -75.00\n",
      "  Episode 40/100: reward = -62.00\n",
      "  Episode 41/100: reward = -76.00\n",
      "  Episode 42/100: reward = -153.00\n",
      "  Episode 43/100: reward = -75.00\n",
      "  Episode 44/100: reward = -95.00\n",
      "  Episode 45/100: reward = -74.00\n",
      "  Episode 46/100: reward = -62.00\n",
      "  Episode 47/100: reward = -75.00\n",
      "  Episode 48/100: reward = -70.00\n",
      "  Episode 49/100: reward = -89.00\n",
      "  Episode 50/100: reward = -99.00\n",
      "  Episode 51/100: reward = -89.00\n",
      "  Episode 52/100: reward = -70.00\n",
      "  Episode 53/100: reward = -78.00\n",
      "  Episode 54/100: reward = -87.00\n",
      "  Episode 55/100: reward = -92.00\n",
      "  Episode 56/100: reward = -79.00\n",
      "  Episode 57/100: reward = -71.00\n",
      "  Episode 58/100: reward = -93.00\n",
      "  Episode 59/100: reward = -85.00\n",
      "  Episode 60/100: reward = -128.00\n",
      "  Episode 61/100: reward = -93.00\n",
      "  Episode 62/100: reward = -76.00\n",
      "  Episode 63/100: reward = -79.00\n",
      "  Episode 64/100: reward = -75.00\n",
      "  Episode 65/100: reward = -169.00\n",
      "  Episode 66/100: reward = -74.00\n",
      "  Episode 67/100: reward = -105.00\n",
      "  Episode 68/100: reward = -86.00\n",
      "  Episode 69/100: reward = -77.00\n",
      "  Episode 70/100: reward = -86.00\n",
      "  Episode 71/100: reward = -78.00\n",
      "  Episode 72/100: reward = -108.00\n",
      "  Episode 73/100: reward = -93.00\n",
      "  Episode 74/100: reward = -78.00\n",
      "  Episode 75/100: reward = -120.00\n",
      "  Episode 76/100: reward = -79.00\n",
      "  Episode 77/100: reward = -79.00\n",
      "  Episode 78/100: reward = -79.00\n",
      "  Episode 79/100: reward = -61.00\n",
      "  Episode 80/100: reward = -95.00\n",
      "  Episode 81/100: reward = -73.00\n",
      "  Episode 82/100: reward = -74.00\n",
      "  Episode 83/100: reward = -78.00\n",
      "  Episode 84/100: reward = -71.00\n",
      "  Episode 85/100: reward = -94.00\n",
      "  Episode 86/100: reward = -98.00\n",
      "  Episode 87/100: reward = -63.00\n",
      "  Episode 88/100: reward = -74.00\n",
      "  Episode 89/100: reward = -89.00\n",
      "  Episode 90/100: reward = -75.00\n",
      "  Episode 91/100: reward = -74.00\n",
      "  Episode 92/100: reward = -80.00\n",
      "  Episode 93/100: reward = -114.00\n",
      "  Episode 94/100: reward = -111.00\n",
      "  Episode 95/100: reward = -79.00\n",
      "  Episode 96/100: reward = -74.00\n",
      "  Episode 97/100: reward = -79.00\n",
      "  Episode 98/100: reward = -94.00\n",
      "  Episode 99/100: reward = -62.00\n",
      "  Episode 100/100: reward = -79.00\n",
      "✅ Model Acrobot_DQN_eps-2500.pth: avg reward = -85.66\n",
      "🎥 Recording episode 17 for Acrobot_DQN_eps-2500.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_eps-2500.pth at ./videos/Acrobot-v1/Acrobot_DQN_eps-2500/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_eps-500.pth\n",
      "  Episode 1/100: reward = -79.00\n",
      "  Episode 2/100: reward = -71.00\n",
      "  Episode 3/100: reward = -71.00\n",
      "  Episode 4/100: reward = -70.00\n",
      "  Episode 5/100: reward = -70.00\n",
      "  Episode 6/100: reward = -63.00\n",
      "  Episode 7/100: reward = -62.00\n",
      "  Episode 8/100: reward = -85.00\n",
      "  Episode 9/100: reward = -62.00\n",
      "  Episode 10/100: reward = -77.00\n",
      "  Episode 11/100: reward = -79.00\n",
      "  Episode 12/100: reward = -75.00\n",
      "  Episode 13/100: reward = -87.00\n",
      "  Episode 14/100: reward = -63.00\n",
      "  Episode 15/100: reward = -71.00\n",
      "  Episode 16/100: reward = -94.00\n",
      "  Episode 17/100: reward = -80.00\n",
      "  Episode 18/100: reward = -70.00\n",
      "  Episode 19/100: reward = -63.00\n",
      "  Episode 20/100: reward = -90.00\n",
      "  Episode 21/100: reward = -68.00\n",
      "  Episode 22/100: reward = -79.00\n",
      "  Episode 23/100: reward = -70.00\n",
      "  Episode 24/100: reward = -76.00\n",
      "  Episode 25/100: reward = -75.00\n",
      "  Episode 26/100: reward = -71.00\n",
      "  Episode 27/100: reward = -72.00\n",
      "  Episode 28/100: reward = -74.00\n",
      "  Episode 29/100: reward = -82.00\n",
      "  Episode 30/100: reward = -70.00\n",
      "  Episode 31/100: reward = -74.00\n",
      "  Episode 32/100: reward = -74.00\n",
      "  Episode 33/100: reward = -98.00\n",
      "  Episode 34/100: reward = -104.00\n",
      "  Episode 35/100: reward = -70.00\n",
      "  Episode 36/100: reward = -94.00\n",
      "  Episode 37/100: reward = -86.00\n",
      "  Episode 38/100: reward = -75.00\n",
      "  Episode 39/100: reward = -70.00\n",
      "  Episode 40/100: reward = -78.00\n",
      "  Episode 41/100: reward = -76.00\n",
      "  Episode 42/100: reward = -70.00\n",
      "  Episode 43/100: reward = -63.00\n",
      "  Episode 44/100: reward = -63.00\n",
      "  Episode 45/100: reward = -71.00\n",
      "  Episode 46/100: reward = -75.00\n",
      "  Episode 47/100: reward = -86.00\n",
      "  Episode 48/100: reward = -62.00\n",
      "  Episode 49/100: reward = -73.00\n",
      "  Episode 50/100: reward = -71.00\n",
      "  Episode 51/100: reward = -77.00\n",
      "  Episode 52/100: reward = -71.00\n",
      "  Episode 53/100: reward = -71.00\n",
      "  Episode 54/100: reward = -71.00\n",
      "  Episode 55/100: reward = -63.00\n",
      "  Episode 56/100: reward = -78.00\n",
      "  Episode 57/100: reward = -71.00\n",
      "  Episode 58/100: reward = -71.00\n",
      "  Episode 59/100: reward = -76.00\n",
      "  Episode 60/100: reward = -66.00\n",
      "  Episode 61/100: reward = -86.00\n",
      "  Episode 62/100: reward = -87.00\n",
      "  Episode 63/100: reward = -81.00\n",
      "  Episode 64/100: reward = -70.00\n",
      "  Episode 65/100: reward = -75.00\n",
      "  Episode 66/100: reward = -70.00\n",
      "  Episode 67/100: reward = -72.00\n",
      "  Episode 68/100: reward = -70.00\n",
      "  Episode 69/100: reward = -69.00\n",
      "  Episode 70/100: reward = -76.00\n",
      "  Episode 71/100: reward = -64.00\n",
      "  Episode 72/100: reward = -73.00\n",
      "  Episode 73/100: reward = -71.00\n",
      "  Episode 74/100: reward = -71.00\n",
      "  Episode 75/100: reward = -71.00\n",
      "  Episode 76/100: reward = -70.00\n",
      "  Episode 77/100: reward = -71.00\n",
      "  Episode 78/100: reward = -63.00\n",
      "  Episode 79/100: reward = -70.00\n",
      "  Episode 80/100: reward = -71.00\n",
      "  Episode 81/100: reward = -73.00\n",
      "  Episode 82/100: reward = -69.00\n",
      "  Episode 83/100: reward = -110.00\n",
      "  Episode 84/100: reward = -71.00\n",
      "  Episode 85/100: reward = -197.00\n",
      "  Episode 86/100: reward = -63.00\n",
      "  Episode 87/100: reward = -74.00\n",
      "  Episode 88/100: reward = -63.00\n",
      "  Episode 89/100: reward = -95.00\n",
      "  Episode 90/100: reward = -75.00\n",
      "  Episode 91/100: reward = -74.00\n",
      "  Episode 92/100: reward = -72.00\n",
      "  Episode 93/100: reward = -73.00\n",
      "  Episode 94/100: reward = -83.00\n",
      "  Episode 95/100: reward = -73.00\n",
      "  Episode 96/100: reward = -88.00\n",
      "  Episode 97/100: reward = -71.00\n",
      "  Episode 98/100: reward = -70.00\n",
      "  Episode 99/100: reward = -70.00\n",
      "  Episode 100/100: reward = -102.00\n",
      "✅ Model Acrobot_DQN_eps-500.pth: avg reward = -75.84\n",
      "🎥 Recording episode 34 for Acrobot_DQN_eps-500.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_eps-500.pth at ./videos/Acrobot-v1/Acrobot_DQN_eps-500/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_eps-5000.pth\n",
      "  Episode 1/100: reward = -62.00\n",
      "  Episode 2/100: reward = -70.00\n",
      "  Episode 3/100: reward = -63.00\n",
      "  Episode 4/100: reward = -62.00\n",
      "  Episode 5/100: reward = -70.00\n",
      "  Episode 6/100: reward = -282.00\n",
      "  Episode 7/100: reward = -63.00\n",
      "  Episode 8/100: reward = -78.00\n",
      "  Episode 9/100: reward = -70.00\n",
      "  Episode 10/100: reward = -75.00\n",
      "  Episode 11/100: reward = -63.00\n",
      "  Episode 12/100: reward = -82.00\n",
      "  Episode 13/100: reward = -88.00\n",
      "  Episode 14/100: reward = -62.00\n",
      "  Episode 15/100: reward = -62.00\n",
      "  Episode 16/100: reward = -86.00\n",
      "  Episode 17/100: reward = -62.00\n",
      "  Episode 18/100: reward = -71.00\n",
      "  Episode 19/100: reward = -71.00\n",
      "  Episode 20/100: reward = -88.00\n",
      "  Episode 21/100: reward = -72.00\n",
      "  Episode 22/100: reward = -62.00\n",
      "  Episode 23/100: reward = -63.00\n",
      "  Episode 24/100: reward = -75.00\n",
      "  Episode 25/100: reward = -63.00\n",
      "  Episode 26/100: reward = -70.00\n",
      "  Episode 27/100: reward = -62.00\n",
      "  Episode 28/100: reward = -62.00\n",
      "  Episode 29/100: reward = -116.00\n",
      "  Episode 30/100: reward = -70.00\n",
      "  Episode 31/100: reward = -62.00\n",
      "  Episode 32/100: reward = -88.00\n",
      "  Episode 33/100: reward = -63.00\n",
      "  Episode 34/100: reward = -70.00\n",
      "  Episode 35/100: reward = -62.00\n",
      "  Episode 36/100: reward = -62.00\n",
      "  Episode 37/100: reward = -70.00\n",
      "  Episode 38/100: reward = -62.00\n",
      "  Episode 39/100: reward = -96.00\n",
      "  Episode 40/100: reward = -86.00\n",
      "  Episode 41/100: reward = -71.00\n",
      "  Episode 42/100: reward = -63.00\n",
      "  Episode 43/100: reward = -70.00\n",
      "  Episode 44/100: reward = -62.00\n",
      "  Episode 45/100: reward = -70.00\n",
      "  Episode 46/100: reward = -63.00\n",
      "  Episode 47/100: reward = -63.00\n",
      "  Episode 48/100: reward = -119.00\n",
      "  Episode 49/100: reward = -71.00\n",
      "  Episode 50/100: reward = -96.00\n",
      "  Episode 51/100: reward = -63.00\n",
      "  Episode 52/100: reward = -62.00\n",
      "  Episode 53/100: reward = -63.00\n",
      "  Episode 54/100: reward = -62.00\n",
      "  Episode 55/100: reward = -63.00\n",
      "  Episode 56/100: reward = -62.00\n",
      "  Episode 57/100: reward = -87.00\n",
      "  Episode 58/100: reward = -62.00\n",
      "  Episode 59/100: reward = -61.00\n",
      "  Episode 60/100: reward = -71.00\n",
      "  Episode 61/100: reward = -87.00\n",
      "  Episode 62/100: reward = -72.00\n",
      "  Episode 63/100: reward = -71.00\n",
      "  Episode 64/100: reward = -70.00\n",
      "  Episode 65/100: reward = -74.00\n",
      "  Episode 66/100: reward = -62.00\n",
      "  Episode 67/100: reward = -72.00\n",
      "  Episode 68/100: reward = -62.00\n",
      "  Episode 69/100: reward = -78.00\n",
      "  Episode 70/100: reward = -62.00\n",
      "  Episode 71/100: reward = -62.00\n",
      "  Episode 72/100: reward = -71.00\n",
      "  Episode 73/100: reward = -62.00\n",
      "  Episode 74/100: reward = -63.00\n",
      "  Episode 75/100: reward = -70.00\n",
      "  Episode 76/100: reward = -70.00\n",
      "  Episode 77/100: reward = -72.00\n",
      "  Episode 78/100: reward = -62.00\n",
      "  Episode 79/100: reward = -63.00\n",
      "  Episode 80/100: reward = -70.00\n",
      "  Episode 81/100: reward = -62.00\n",
      "  Episode 82/100: reward = -63.00\n",
      "  Episode 83/100: reward = -63.00\n",
      "  Episode 84/100: reward = -62.00\n",
      "  Episode 85/100: reward = -62.00\n",
      "  Episode 86/100: reward = -83.00\n",
      "  Episode 87/100: reward = -62.00\n",
      "  Episode 88/100: reward = -62.00\n",
      "  Episode 89/100: reward = -62.00\n",
      "  Episode 90/100: reward = -70.00\n",
      "  Episode 91/100: reward = -101.00\n",
      "  Episode 92/100: reward = -63.00\n",
      "  Episode 93/100: reward = -95.00\n",
      "  Episode 94/100: reward = -62.00\n",
      "  Episode 95/100: reward = -82.00\n",
      "  Episode 96/100: reward = -70.00\n",
      "  Episode 97/100: reward = -62.00\n",
      "  Episode 98/100: reward = -62.00\n",
      "  Episode 99/100: reward = -63.00\n",
      "  Episode 100/100: reward = -62.00\n",
      "✅ Model Acrobot_DQN_eps-5000.pth: avg reward = -72.18\n",
      "🎥 Recording episode 21 for Acrobot_DQN_eps-5000.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_eps-5000.pth at ./videos/Acrobot-v1/Acrobot_DQN_eps-5000/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_gamma-0.1.pth\n",
      "  Episode 1/100: reward = -500.00\n",
      "  Episode 2/100: reward = -500.00\n",
      "  Episode 3/100: reward = -500.00\n",
      "  Episode 4/100: reward = -500.00\n",
      "  Episode 5/100: reward = -500.00\n",
      "  Episode 6/100: reward = -500.00\n",
      "  Episode 7/100: reward = -500.00\n",
      "  Episode 8/100: reward = -500.00\n",
      "  Episode 9/100: reward = -500.00\n",
      "  Episode 10/100: reward = -500.00\n",
      "  Episode 11/100: reward = -500.00\n",
      "  Episode 12/100: reward = -500.00\n",
      "  Episode 13/100: reward = -500.00\n",
      "  Episode 14/100: reward = -500.00\n",
      "  Episode 15/100: reward = -500.00\n",
      "  Episode 16/100: reward = -500.00\n",
      "  Episode 17/100: reward = -500.00\n",
      "  Episode 18/100: reward = -500.00\n",
      "  Episode 19/100: reward = -500.00\n",
      "  Episode 20/100: reward = -500.00\n",
      "  Episode 21/100: reward = -500.00\n",
      "  Episode 22/100: reward = -500.00\n",
      "  Episode 23/100: reward = -500.00\n",
      "  Episode 24/100: reward = -500.00\n",
      "  Episode 25/100: reward = -500.00\n",
      "  Episode 26/100: reward = -500.00\n",
      "  Episode 27/100: reward = -500.00\n",
      "  Episode 28/100: reward = -500.00\n",
      "  Episode 29/100: reward = -500.00\n",
      "  Episode 30/100: reward = -500.00\n",
      "  Episode 31/100: reward = -500.00\n",
      "  Episode 32/100: reward = -500.00\n",
      "  Episode 33/100: reward = -500.00\n",
      "  Episode 34/100: reward = -500.00\n",
      "  Episode 35/100: reward = -500.00\n",
      "  Episode 36/100: reward = -500.00\n",
      "  Episode 37/100: reward = -500.00\n",
      "  Episode 38/100: reward = -500.00\n",
      "  Episode 39/100: reward = -500.00\n",
      "  Episode 40/100: reward = -500.00\n",
      "  Episode 41/100: reward = -500.00\n",
      "  Episode 42/100: reward = -500.00\n",
      "  Episode 43/100: reward = -500.00\n",
      "  Episode 44/100: reward = -500.00\n",
      "  Episode 45/100: reward = -500.00\n",
      "  Episode 46/100: reward = -500.00\n",
      "  Episode 47/100: reward = -500.00\n",
      "  Episode 48/100: reward = -500.00\n",
      "  Episode 49/100: reward = -500.00\n",
      "  Episode 50/100: reward = -500.00\n",
      "  Episode 51/100: reward = -500.00\n",
      "  Episode 52/100: reward = -500.00\n",
      "  Episode 53/100: reward = -500.00\n",
      "  Episode 54/100: reward = -500.00\n",
      "  Episode 55/100: reward = -500.00\n",
      "  Episode 56/100: reward = -500.00\n",
      "  Episode 57/100: reward = -500.00\n",
      "  Episode 58/100: reward = -500.00\n",
      "  Episode 59/100: reward = -500.00\n",
      "  Episode 60/100: reward = -500.00\n",
      "  Episode 61/100: reward = -500.00\n",
      "  Episode 62/100: reward = -500.00\n",
      "  Episode 63/100: reward = -500.00\n",
      "  Episode 64/100: reward = -500.00\n",
      "  Episode 65/100: reward = -500.00\n",
      "  Episode 66/100: reward = -500.00\n",
      "  Episode 67/100: reward = -500.00\n",
      "  Episode 68/100: reward = -500.00\n",
      "  Episode 69/100: reward = -500.00\n",
      "  Episode 70/100: reward = -500.00\n",
      "  Episode 71/100: reward = -500.00\n",
      "  Episode 72/100: reward = -500.00\n",
      "  Episode 73/100: reward = -500.00\n",
      "  Episode 74/100: reward = -500.00\n",
      "  Episode 75/100: reward = -500.00\n",
      "  Episode 76/100: reward = -500.00\n",
      "  Episode 77/100: reward = -500.00\n",
      "  Episode 78/100: reward = -500.00\n",
      "  Episode 79/100: reward = -500.00\n",
      "  Episode 80/100: reward = -500.00\n",
      "  Episode 81/100: reward = -500.00\n",
      "  Episode 82/100: reward = -500.00\n",
      "  Episode 83/100: reward = -500.00\n",
      "  Episode 84/100: reward = -500.00\n",
      "  Episode 85/100: reward = -500.00\n",
      "  Episode 86/100: reward = -500.00\n",
      "  Episode 87/100: reward = -500.00\n",
      "  Episode 88/100: reward = -500.00\n",
      "  Episode 89/100: reward = -500.00\n",
      "  Episode 90/100: reward = -500.00\n",
      "  Episode 91/100: reward = -500.00\n",
      "  Episode 92/100: reward = -500.00\n",
      "  Episode 93/100: reward = -500.00\n",
      "  Episode 94/100: reward = -500.00\n",
      "  Episode 95/100: reward = -500.00\n",
      "  Episode 96/100: reward = -500.00\n",
      "  Episode 97/100: reward = -500.00\n",
      "  Episode 98/100: reward = -500.00\n",
      "  Episode 99/100: reward = -500.00\n",
      "  Episode 100/100: reward = -500.00\n",
      "✅ Model Acrobot_DQN_gamma-0.1.pth: avg reward = -500.00\n",
      "🎥 Recording episode 85 for Acrobot_DQN_gamma-0.1.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_gamma-0.1.pth at ./videos/Acrobot-v1/Acrobot_DQN_gamma-0.1/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_gamma-0.5.pth\n",
      "  Episode 1/100: reward = -500.00\n",
      "  Episode 2/100: reward = -500.00\n",
      "  Episode 3/100: reward = -500.00\n",
      "  Episode 4/100: reward = -500.00\n",
      "  Episode 5/100: reward = -500.00\n",
      "  Episode 6/100: reward = -500.00\n",
      "  Episode 7/100: reward = -500.00\n",
      "  Episode 8/100: reward = -500.00\n",
      "  Episode 9/100: reward = -500.00\n",
      "  Episode 10/100: reward = -500.00\n",
      "  Episode 11/100: reward = -500.00\n",
      "  Episode 12/100: reward = -500.00\n",
      "  Episode 13/100: reward = -500.00\n",
      "  Episode 14/100: reward = -500.00\n",
      "  Episode 15/100: reward = -500.00\n",
      "  Episode 16/100: reward = -500.00\n",
      "  Episode 17/100: reward = -500.00\n",
      "  Episode 18/100: reward = -500.00\n",
      "  Episode 19/100: reward = -500.00\n",
      "  Episode 20/100: reward = -500.00\n",
      "  Episode 21/100: reward = -500.00\n",
      "  Episode 22/100: reward = -500.00\n",
      "  Episode 23/100: reward = -500.00\n",
      "  Episode 24/100: reward = -500.00\n",
      "  Episode 25/100: reward = -500.00\n",
      "  Episode 26/100: reward = -500.00\n",
      "  Episode 27/100: reward = -500.00\n",
      "  Episode 28/100: reward = -500.00\n",
      "  Episode 29/100: reward = -500.00\n",
      "  Episode 30/100: reward = -500.00\n",
      "  Episode 31/100: reward = -500.00\n",
      "  Episode 32/100: reward = -500.00\n",
      "  Episode 33/100: reward = -500.00\n",
      "  Episode 34/100: reward = -500.00\n",
      "  Episode 35/100: reward = -500.00\n",
      "  Episode 36/100: reward = -500.00\n",
      "  Episode 37/100: reward = -500.00\n",
      "  Episode 38/100: reward = -500.00\n",
      "  Episode 39/100: reward = -500.00\n",
      "  Episode 40/100: reward = -500.00\n",
      "  Episode 41/100: reward = -500.00\n",
      "  Episode 42/100: reward = -500.00\n",
      "  Episode 43/100: reward = -500.00\n",
      "  Episode 44/100: reward = -500.00\n",
      "  Episode 45/100: reward = -500.00\n",
      "  Episode 46/100: reward = -500.00\n",
      "  Episode 47/100: reward = -500.00\n",
      "  Episode 48/100: reward = -500.00\n",
      "  Episode 49/100: reward = -500.00\n",
      "  Episode 50/100: reward = -500.00\n",
      "  Episode 51/100: reward = -500.00\n",
      "  Episode 52/100: reward = -500.00\n",
      "  Episode 53/100: reward = -500.00\n",
      "  Episode 54/100: reward = -500.00\n",
      "  Episode 55/100: reward = -500.00\n",
      "  Episode 56/100: reward = -500.00\n",
      "  Episode 57/100: reward = -500.00\n",
      "  Episode 58/100: reward = -500.00\n",
      "  Episode 59/100: reward = -500.00\n",
      "  Episode 60/100: reward = -500.00\n",
      "  Episode 61/100: reward = -500.00\n",
      "  Episode 62/100: reward = -500.00\n",
      "  Episode 63/100: reward = -500.00\n",
      "  Episode 64/100: reward = -500.00\n",
      "  Episode 65/100: reward = -500.00\n",
      "  Episode 66/100: reward = -500.00\n",
      "  Episode 67/100: reward = -500.00\n",
      "  Episode 68/100: reward = -500.00\n",
      "  Episode 69/100: reward = -500.00\n",
      "  Episode 70/100: reward = -500.00\n",
      "  Episode 71/100: reward = -500.00\n",
      "  Episode 72/100: reward = -500.00\n",
      "  Episode 73/100: reward = -500.00\n",
      "  Episode 74/100: reward = -500.00\n",
      "  Episode 75/100: reward = -500.00\n",
      "  Episode 76/100: reward = -500.00\n",
      "  Episode 77/100: reward = -500.00\n",
      "  Episode 78/100: reward = -500.00\n",
      "  Episode 79/100: reward = -500.00\n",
      "  Episode 80/100: reward = -500.00\n",
      "  Episode 81/100: reward = -500.00\n",
      "  Episode 82/100: reward = -500.00\n",
      "  Episode 83/100: reward = -500.00\n",
      "  Episode 84/100: reward = -500.00\n",
      "  Episode 85/100: reward = -500.00\n",
      "  Episode 86/100: reward = -500.00\n",
      "  Episode 87/100: reward = -500.00\n",
      "  Episode 88/100: reward = -500.00\n",
      "  Episode 89/100: reward = -500.00\n",
      "  Episode 90/100: reward = -500.00\n",
      "  Episode 91/100: reward = -500.00\n",
      "  Episode 92/100: reward = -500.00\n",
      "  Episode 93/100: reward = -500.00\n",
      "  Episode 94/100: reward = -500.00\n",
      "  Episode 95/100: reward = -500.00\n",
      "  Episode 96/100: reward = -500.00\n",
      "  Episode 97/100: reward = -500.00\n",
      "  Episode 98/100: reward = -500.00\n",
      "  Episode 99/100: reward = -500.00\n",
      "  Episode 100/100: reward = -500.00\n",
      "✅ Model Acrobot_DQN_gamma-0.5.pth: avg reward = -500.00\n",
      "🎥 Recording episode 2 for Acrobot_DQN_gamma-0.5.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_gamma-0.5.pth at ./videos/Acrobot-v1/Acrobot_DQN_gamma-0.5/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_gamma-0.9.pth\n",
      "  Episode 1/100: reward = -500.00\n",
      "  Episode 2/100: reward = -500.00\n",
      "  Episode 3/100: reward = -500.00\n",
      "  Episode 4/100: reward = -500.00\n",
      "  Episode 5/100: reward = -500.00\n",
      "  Episode 6/100: reward = -500.00\n",
      "  Episode 7/100: reward = -500.00\n",
      "  Episode 8/100: reward = -500.00\n",
      "  Episode 9/100: reward = -500.00\n",
      "  Episode 10/100: reward = -500.00\n",
      "  Episode 11/100: reward = -500.00\n",
      "  Episode 12/100: reward = -500.00\n",
      "  Episode 13/100: reward = -500.00\n",
      "  Episode 14/100: reward = -440.00\n",
      "  Episode 15/100: reward = -500.00\n",
      "  Episode 16/100: reward = -500.00\n",
      "  Episode 17/100: reward = -500.00\n",
      "  Episode 18/100: reward = -500.00\n",
      "  Episode 19/100: reward = -500.00\n",
      "  Episode 20/100: reward = -500.00\n",
      "  Episode 21/100: reward = -500.00\n",
      "  Episode 22/100: reward = -500.00\n",
      "  Episode 23/100: reward = -500.00\n",
      "  Episode 24/100: reward = -500.00\n",
      "  Episode 25/100: reward = -500.00\n",
      "  Episode 26/100: reward = -500.00\n",
      "  Episode 27/100: reward = -500.00\n",
      "  Episode 28/100: reward = -500.00\n",
      "  Episode 29/100: reward = -500.00\n",
      "  Episode 30/100: reward = -500.00\n",
      "  Episode 31/100: reward = -500.00\n",
      "  Episode 32/100: reward = -500.00\n",
      "  Episode 33/100: reward = -500.00\n",
      "  Episode 34/100: reward = -500.00\n",
      "  Episode 35/100: reward = -500.00\n",
      "  Episode 36/100: reward = -500.00\n",
      "  Episode 37/100: reward = -500.00\n",
      "  Episode 38/100: reward = -500.00\n",
      "  Episode 39/100: reward = -500.00\n",
      "  Episode 40/100: reward = -500.00\n",
      "  Episode 41/100: reward = -500.00\n",
      "  Episode 42/100: reward = -500.00\n",
      "  Episode 43/100: reward = -500.00\n",
      "  Episode 44/100: reward = -500.00\n",
      "  Episode 45/100: reward = -500.00\n",
      "  Episode 46/100: reward = -500.00\n",
      "  Episode 47/100: reward = -500.00\n",
      "  Episode 48/100: reward = -500.00\n",
      "  Episode 49/100: reward = -500.00\n",
      "  Episode 50/100: reward = -500.00\n",
      "  Episode 51/100: reward = -500.00\n",
      "  Episode 52/100: reward = -500.00\n",
      "  Episode 53/100: reward = -500.00\n",
      "  Episode 54/100: reward = -500.00\n",
      "  Episode 55/100: reward = -500.00\n",
      "  Episode 56/100: reward = -500.00\n",
      "  Episode 57/100: reward = -500.00\n",
      "  Episode 58/100: reward = -500.00\n",
      "  Episode 59/100: reward = -500.00\n",
      "  Episode 60/100: reward = -500.00\n",
      "  Episode 61/100: reward = -500.00\n",
      "  Episode 62/100: reward = -500.00\n",
      "  Episode 63/100: reward = -500.00\n",
      "  Episode 64/100: reward = -500.00\n",
      "  Episode 65/100: reward = -500.00\n",
      "  Episode 66/100: reward = -500.00\n",
      "  Episode 67/100: reward = -500.00\n",
      "  Episode 68/100: reward = -500.00\n",
      "  Episode 69/100: reward = -500.00\n",
      "  Episode 70/100: reward = -500.00\n",
      "  Episode 71/100: reward = -500.00\n",
      "  Episode 72/100: reward = -500.00\n",
      "  Episode 73/100: reward = -500.00\n",
      "  Episode 74/100: reward = -500.00\n",
      "  Episode 75/100: reward = -316.00\n",
      "  Episode 76/100: reward = -500.00\n",
      "  Episode 77/100: reward = -500.00\n",
      "  Episode 78/100: reward = -435.00\n",
      "  Episode 79/100: reward = -500.00\n",
      "  Episode 80/100: reward = -500.00\n",
      "  Episode 81/100: reward = -500.00\n",
      "  Episode 82/100: reward = -500.00\n",
      "  Episode 83/100: reward = -500.00\n",
      "  Episode 84/100: reward = -500.00\n",
      "  Episode 85/100: reward = -500.00\n",
      "  Episode 86/100: reward = -500.00\n",
      "  Episode 87/100: reward = -492.00\n",
      "  Episode 88/100: reward = -500.00\n",
      "  Episode 89/100: reward = -500.00\n",
      "  Episode 90/100: reward = -500.00\n",
      "  Episode 91/100: reward = -500.00\n",
      "  Episode 92/100: reward = -500.00\n",
      "  Episode 93/100: reward = -500.00\n",
      "  Episode 94/100: reward = -500.00\n",
      "  Episode 95/100: reward = -500.00\n",
      "  Episode 96/100: reward = -500.00\n",
      "  Episode 97/100: reward = -500.00\n",
      "  Episode 98/100: reward = -500.00\n",
      "  Episode 99/100: reward = -500.00\n",
      "  Episode 100/100: reward = -500.00\n",
      "✅ Model Acrobot_DQN_gamma-0.9.pth: avg reward = -496.83\n",
      "🎥 Recording episode 39 for Acrobot_DQN_gamma-0.9.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_gamma-0.9 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Acrobot_DQN_gamma-0.9.pth at ./videos/Acrobot-v1/Acrobot_DQN_gamma-0.9/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_gamma-0.99.pth\n",
      "  Episode 1/100: reward = -82.00\n",
      "  Episode 2/100: reward = -195.00\n",
      "  Episode 3/100: reward = -103.00\n",
      "  Episode 4/100: reward = -63.00\n",
      "  Episode 5/100: reward = -87.00\n",
      "  Episode 6/100: reward = -124.00\n",
      "  Episode 7/100: reward = -94.00\n",
      "  Episode 8/100: reward = -70.00\n",
      "  Episode 9/100: reward = -86.00\n",
      "  Episode 10/100: reward = -69.00\n",
      "  Episode 11/100: reward = -73.00\n",
      "  Episode 12/100: reward = -103.00\n",
      "  Episode 13/100: reward = -76.00\n",
      "  Episode 14/100: reward = -93.00\n",
      "  Episode 15/100: reward = -86.00\n",
      "  Episode 16/100: reward = -74.00\n",
      "  Episode 17/100: reward = -88.00\n",
      "  Episode 18/100: reward = -72.00\n",
      "  Episode 19/100: reward = -78.00\n",
      "  Episode 20/100: reward = -78.00\n",
      "  Episode 21/100: reward = -78.00\n",
      "  Episode 22/100: reward = -83.00\n",
      "  Episode 23/100: reward = -167.00\n",
      "  Episode 24/100: reward = -88.00\n",
      "  Episode 25/100: reward = -62.00\n",
      "  Episode 26/100: reward = -70.00\n",
      "  Episode 27/100: reward = -69.00\n",
      "  Episode 28/100: reward = -77.00\n",
      "  Episode 29/100: reward = -69.00\n",
      "  Episode 30/100: reward = -101.00\n",
      "  Episode 31/100: reward = -69.00\n",
      "  Episode 32/100: reward = -70.00\n",
      "  Episode 33/100: reward = -87.00\n",
      "  Episode 34/100: reward = -78.00\n",
      "  Episode 35/100: reward = -71.00\n",
      "  Episode 36/100: reward = -71.00\n",
      "  Episode 37/100: reward = -97.00\n",
      "  Episode 38/100: reward = -84.00\n",
      "  Episode 39/100: reward = -89.00\n",
      "  Episode 40/100: reward = -86.00\n",
      "  Episode 41/100: reward = -102.00\n",
      "  Episode 42/100: reward = -105.00\n",
      "  Episode 43/100: reward = -97.00\n",
      "  Episode 44/100: reward = -79.00\n",
      "  Episode 45/100: reward = -69.00\n",
      "  Episode 46/100: reward = -74.00\n",
      "  Episode 47/100: reward = -94.00\n",
      "  Episode 48/100: reward = -71.00\n",
      "  Episode 49/100: reward = -123.00\n",
      "  Episode 50/100: reward = -84.00\n",
      "  Episode 51/100: reward = -69.00\n",
      "  Episode 52/100: reward = -79.00\n",
      "  Episode 53/100: reward = -86.00\n",
      "  Episode 54/100: reward = -74.00\n",
      "  Episode 55/100: reward = -78.00\n",
      "  Episode 56/100: reward = -75.00\n",
      "  Episode 57/100: reward = -88.00\n",
      "  Episode 58/100: reward = -106.00\n",
      "  Episode 59/100: reward = -93.00\n",
      "  Episode 60/100: reward = -83.00\n",
      "  Episode 61/100: reward = -87.00\n",
      "  Episode 62/100: reward = -69.00\n",
      "  Episode 63/100: reward = -80.00\n",
      "  Episode 64/100: reward = -94.00\n",
      "  Episode 65/100: reward = -71.00\n",
      "  Episode 66/100: reward = -87.00\n",
      "  Episode 67/100: reward = -73.00\n",
      "  Episode 68/100: reward = -69.00\n",
      "  Episode 69/100: reward = -71.00\n",
      "  Episode 70/100: reward = -82.00\n",
      "  Episode 71/100: reward = -117.00\n",
      "  Episode 72/100: reward = -108.00\n",
      "  Episode 73/100: reward = -105.00\n",
      "  Episode 74/100: reward = -93.00\n",
      "  Episode 75/100: reward = -71.00\n",
      "  Episode 76/100: reward = -96.00\n",
      "  Episode 77/100: reward = -70.00\n",
      "  Episode 78/100: reward = -81.00\n",
      "  Episode 79/100: reward = -102.00\n",
      "  Episode 80/100: reward = -123.00\n",
      "  Episode 81/100: reward = -95.00\n",
      "  Episode 82/100: reward = -70.00\n",
      "  Episode 83/100: reward = -91.00\n",
      "  Episode 84/100: reward = -70.00\n",
      "  Episode 85/100: reward = -70.00\n",
      "  Episode 86/100: reward = -114.00\n",
      "  Episode 87/100: reward = -88.00\n",
      "  Episode 88/100: reward = -179.00\n",
      "  Episode 89/100: reward = -85.00\n",
      "  Episode 90/100: reward = -69.00\n",
      "  Episode 91/100: reward = -94.00\n",
      "  Episode 92/100: reward = -97.00\n",
      "  Episode 93/100: reward = -63.00\n",
      "  Episode 94/100: reward = -87.00\n",
      "  Episode 95/100: reward = -93.00\n",
      "  Episode 96/100: reward = -69.00\n",
      "  Episode 97/100: reward = -85.00\n",
      "  Episode 98/100: reward = -87.00\n",
      "  Episode 99/100: reward = -84.00\n",
      "  Episode 100/100: reward = -147.00\n",
      "✅ Model Acrobot_DQN_gamma-0.99.pth: avg reward = -87.75\n",
      "🎥 Recording episode 9 for Acrobot_DQN_gamma-0.99.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_gamma-0.99 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Acrobot_DQN_gamma-0.99.pth at ./videos/Acrobot-v1/Acrobot_DQN_gamma-0.99/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_lr-0.0003.pth\n",
      "  Episode 1/100: reward = -76.00\n",
      "  Episode 2/100: reward = -110.00\n",
      "  Episode 3/100: reward = -81.00\n",
      "  Episode 4/100: reward = -88.00\n",
      "  Episode 5/100: reward = -87.00\n",
      "  Episode 6/100: reward = -76.00\n",
      "  Episode 7/100: reward = -86.00\n",
      "  Episode 8/100: reward = -77.00\n",
      "  Episode 9/100: reward = -73.00\n",
      "  Episode 10/100: reward = -77.00\n",
      "  Episode 11/100: reward = -101.00\n",
      "  Episode 12/100: reward = -81.00\n",
      "  Episode 13/100: reward = -93.00\n",
      "  Episode 14/100: reward = -72.00\n",
      "  Episode 15/100: reward = -86.00\n",
      "  Episode 16/100: reward = -71.00\n",
      "  Episode 17/100: reward = -75.00\n",
      "  Episode 18/100: reward = -69.00\n",
      "  Episode 19/100: reward = -88.00\n",
      "  Episode 20/100: reward = -73.00\n",
      "  Episode 21/100: reward = -73.00\n",
      "  Episode 22/100: reward = -85.00\n",
      "  Episode 23/100: reward = -94.00\n",
      "  Episode 24/100: reward = -81.00\n",
      "  Episode 25/100: reward = -77.00\n",
      "  Episode 26/100: reward = -77.00\n",
      "  Episode 27/100: reward = -70.00\n",
      "  Episode 28/100: reward = -90.00\n",
      "  Episode 29/100: reward = -71.00\n",
      "  Episode 30/100: reward = -71.00\n",
      "  Episode 31/100: reward = -83.00\n",
      "  Episode 32/100: reward = -87.00\n",
      "  Episode 33/100: reward = -102.00\n",
      "  Episode 34/100: reward = -73.00\n",
      "  Episode 35/100: reward = -82.00\n",
      "  Episode 36/100: reward = -74.00\n",
      "  Episode 37/100: reward = -73.00\n",
      "  Episode 38/100: reward = -74.00\n",
      "  Episode 39/100: reward = -75.00\n",
      "  Episode 40/100: reward = -84.00\n",
      "  Episode 41/100: reward = -81.00\n",
      "  Episode 42/100: reward = -63.00\n",
      "  Episode 43/100: reward = -82.00\n",
      "  Episode 44/100: reward = -69.00\n",
      "  Episode 45/100: reward = -69.00\n",
      "  Episode 46/100: reward = -76.00\n",
      "  Episode 47/100: reward = -91.00\n",
      "  Episode 48/100: reward = -62.00\n",
      "  Episode 49/100: reward = -85.00\n",
      "  Episode 50/100: reward = -99.00\n",
      "  Episode 51/100: reward = -88.00\n",
      "  Episode 52/100: reward = -62.00\n",
      "  Episode 53/100: reward = -74.00\n",
      "  Episode 54/100: reward = -82.00\n",
      "  Episode 55/100: reward = -90.00\n",
      "  Episode 56/100: reward = -73.00\n",
      "  Episode 57/100: reward = -76.00\n",
      "  Episode 58/100: reward = -69.00\n",
      "  Episode 59/100: reward = -73.00\n",
      "  Episode 60/100: reward = -73.00\n",
      "  Episode 61/100: reward = -81.00\n",
      "  Episode 62/100: reward = -73.00\n",
      "  Episode 63/100: reward = -81.00\n",
      "  Episode 64/100: reward = -77.00\n",
      "  Episode 65/100: reward = -71.00\n",
      "  Episode 66/100: reward = -70.00\n",
      "  Episode 67/100: reward = -108.00\n",
      "  Episode 68/100: reward = -73.00\n",
      "  Episode 69/100: reward = -81.00\n",
      "  Episode 70/100: reward = -73.00\n",
      "  Episode 71/100: reward = -92.00\n",
      "  Episode 72/100: reward = -71.00\n",
      "  Episode 73/100: reward = -173.00\n",
      "  Episode 74/100: reward = -73.00\n",
      "  Episode 75/100: reward = -81.00\n",
      "  Episode 76/100: reward = -73.00\n",
      "  Episode 77/100: reward = -89.00\n",
      "  Episode 78/100: reward = -73.00\n",
      "  Episode 79/100: reward = -74.00\n",
      "  Episode 80/100: reward = -88.00\n",
      "  Episode 81/100: reward = -104.00\n",
      "  Episode 82/100: reward = -81.00\n",
      "  Episode 83/100: reward = -76.00\n",
      "  Episode 84/100: reward = -90.00\n",
      "  Episode 85/100: reward = -69.00\n",
      "  Episode 86/100: reward = -89.00\n",
      "  Episode 87/100: reward = -77.00\n",
      "  Episode 88/100: reward = -88.00\n",
      "  Episode 89/100: reward = -90.00\n",
      "  Episode 90/100: reward = -81.00\n",
      "  Episode 91/100: reward = -71.00\n",
      "  Episode 92/100: reward = -77.00\n",
      "  Episode 93/100: reward = -73.00\n",
      "  Episode 94/100: reward = -77.00\n",
      "  Episode 95/100: reward = -77.00\n",
      "  Episode 96/100: reward = -94.00\n",
      "  Episode 97/100: reward = -80.00\n",
      "  Episode 98/100: reward = -70.00\n",
      "  Episode 99/100: reward = -73.00\n",
      "  Episode 100/100: reward = -87.00\n",
      "✅ Model Acrobot_DQN_lr-0.0003.pth: avg reward = -80.72\n",
      "🎥 Recording episode 15 for Acrobot_DQN_lr-0.0003.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_lr-0.0003.pth at ./videos/Acrobot-v1/Acrobot_DQN_lr-0.0003/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_lr-0.001.pth\n",
      "  Episode 1/100: reward = -70.00\n",
      "  Episode 2/100: reward = -98.00\n",
      "  Episode 3/100: reward = -92.00\n",
      "  Episode 4/100: reward = -62.00\n",
      "  Episode 5/100: reward = -115.00\n",
      "  Episode 6/100: reward = -86.00\n",
      "  Episode 7/100: reward = -74.00\n",
      "  Episode 8/100: reward = -62.00\n",
      "  Episode 9/100: reward = -69.00\n",
      "  Episode 10/100: reward = -61.00\n",
      "  Episode 11/100: reward = -88.00\n",
      "  Episode 12/100: reward = -90.00\n",
      "  Episode 13/100: reward = -82.00\n",
      "  Episode 14/100: reward = -85.00\n",
      "  Episode 15/100: reward = -69.00\n",
      "  Episode 16/100: reward = -89.00\n",
      "  Episode 17/100: reward = -74.00\n",
      "  Episode 18/100: reward = -75.00\n",
      "  Episode 19/100: reward = -62.00\n",
      "  Episode 20/100: reward = -116.00\n",
      "  Episode 21/100: reward = -91.00\n",
      "  Episode 22/100: reward = -90.00\n",
      "  Episode 23/100: reward = -70.00\n",
      "  Episode 24/100: reward = -111.00\n",
      "  Episode 25/100: reward = -62.00\n",
      "  Episode 26/100: reward = -69.00\n",
      "  Episode 27/100: reward = -86.00\n",
      "  Episode 28/100: reward = -74.00\n",
      "  Episode 29/100: reward = -62.00\n",
      "  Episode 30/100: reward = -69.00\n",
      "  Episode 31/100: reward = -81.00\n",
      "  Episode 32/100: reward = -77.00\n",
      "  Episode 33/100: reward = -70.00\n",
      "  Episode 34/100: reward = -93.00\n",
      "  Episode 35/100: reward = -90.00\n",
      "  Episode 36/100: reward = -71.00\n",
      "  Episode 37/100: reward = -70.00\n",
      "  Episode 38/100: reward = -61.00\n",
      "  Episode 39/100: reward = -73.00\n",
      "  Episode 40/100: reward = -141.00\n",
      "  Episode 41/100: reward = -87.00\n",
      "  Episode 42/100: reward = -94.00\n",
      "  Episode 43/100: reward = -71.00\n",
      "  Episode 44/100: reward = -78.00\n",
      "  Episode 45/100: reward = -69.00\n",
      "  Episode 46/100: reward = -81.00\n",
      "  Episode 47/100: reward = -103.00\n",
      "  Episode 48/100: reward = -81.00\n",
      "  Episode 49/100: reward = -74.00\n",
      "  Episode 50/100: reward = -76.00\n",
      "  Episode 51/100: reward = -87.00\n",
      "  Episode 52/100: reward = -83.00\n",
      "  Episode 53/100: reward = -74.00\n",
      "  Episode 54/100: reward = -89.00\n",
      "  Episode 55/100: reward = -93.00\n",
      "  Episode 56/100: reward = -86.00\n",
      "  Episode 57/100: reward = -75.00\n",
      "  Episode 58/100: reward = -80.00\n",
      "  Episode 59/100: reward = -81.00\n",
      "  Episode 60/100: reward = -83.00\n",
      "  Episode 61/100: reward = -77.00\n",
      "  Episode 62/100: reward = -91.00\n",
      "  Episode 63/100: reward = -78.00\n",
      "  Episode 64/100: reward = -93.00\n",
      "  Episode 65/100: reward = -62.00\n",
      "  Episode 66/100: reward = -70.00\n",
      "  Episode 67/100: reward = -91.00\n",
      "  Episode 68/100: reward = -68.00\n",
      "  Episode 69/100: reward = -82.00\n",
      "  Episode 70/100: reward = -78.00\n",
      "  Episode 71/100: reward = -100.00\n",
      "  Episode 72/100: reward = -179.00\n",
      "  Episode 73/100: reward = -78.00\n",
      "  Episode 74/100: reward = -73.00\n",
      "  Episode 75/100: reward = -62.00\n",
      "  Episode 76/100: reward = -91.00\n",
      "  Episode 77/100: reward = -83.00\n",
      "  Episode 78/100: reward = -78.00\n",
      "  Episode 79/100: reward = -92.00\n",
      "  Episode 80/100: reward = -87.00\n",
      "  Episode 81/100: reward = -68.00\n",
      "  Episode 82/100: reward = -72.00\n",
      "  Episode 83/100: reward = -92.00\n",
      "  Episode 84/100: reward = -125.00\n",
      "  Episode 85/100: reward = -81.00\n",
      "  Episode 86/100: reward = -79.00\n",
      "  Episode 87/100: reward = -78.00\n",
      "  Episode 88/100: reward = -97.00\n",
      "  Episode 89/100: reward = -97.00\n",
      "  Episode 90/100: reward = -77.00\n",
      "  Episode 91/100: reward = -76.00\n",
      "  Episode 92/100: reward = -72.00\n",
      "  Episode 93/100: reward = -92.00\n",
      "  Episode 94/100: reward = -70.00\n",
      "  Episode 95/100: reward = -71.00\n",
      "  Episode 96/100: reward = -150.00\n",
      "  Episode 97/100: reward = -74.00\n",
      "  Episode 98/100: reward = -82.00\n",
      "  Episode 99/100: reward = -71.00\n",
      "  Episode 100/100: reward = -76.00\n",
      "✅ Model Acrobot_DQN_lr-0.001.pth: avg reward = -82.88\n",
      "🎥 Recording episode 23 for Acrobot_DQN_lr-0.001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_lr-0.001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Acrobot_DQN_lr-0.001.pth at ./videos/Acrobot-v1/Acrobot_DQN_lr-0.001/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_lr-0.01.pth\n",
      "  Episode 1/100: reward = -101.00\n",
      "  Episode 2/100: reward = -73.00\n",
      "  Episode 3/100: reward = -110.00\n",
      "  Episode 4/100: reward = -70.00\n",
      "  Episode 5/100: reward = -80.00\n",
      "  Episode 6/100: reward = -92.00\n",
      "  Episode 7/100: reward = -83.00\n",
      "  Episode 8/100: reward = -70.00\n",
      "  Episode 9/100: reward = -71.00\n",
      "  Episode 10/100: reward = -96.00\n",
      "  Episode 11/100: reward = -81.00\n",
      "  Episode 12/100: reward = -76.00\n",
      "  Episode 13/100: reward = -82.00\n",
      "  Episode 14/100: reward = -90.00\n",
      "  Episode 15/100: reward = -69.00\n",
      "  Episode 16/100: reward = -72.00\n",
      "  Episode 17/100: reward = -80.00\n",
      "  Episode 18/100: reward = -92.00\n",
      "  Episode 19/100: reward = -77.00\n",
      "  Episode 20/100: reward = -81.00\n",
      "  Episode 21/100: reward = -62.00\n",
      "  Episode 22/100: reward = -71.00\n",
      "  Episode 23/100: reward = -80.00\n",
      "  Episode 24/100: reward = -70.00\n",
      "  Episode 25/100: reward = -62.00\n",
      "  Episode 26/100: reward = -96.00\n",
      "  Episode 27/100: reward = -83.00\n",
      "  Episode 28/100: reward = -93.00\n",
      "  Episode 29/100: reward = -92.00\n",
      "  Episode 30/100: reward = -104.00\n",
      "  Episode 31/100: reward = -71.00\n",
      "  Episode 32/100: reward = -88.00\n",
      "  Episode 33/100: reward = -77.00\n",
      "  Episode 34/100: reward = -70.00\n",
      "  Episode 35/100: reward = -89.00\n",
      "  Episode 36/100: reward = -98.00\n",
      "  Episode 37/100: reward = -79.00\n",
      "  Episode 38/100: reward = -90.00\n",
      "  Episode 39/100: reward = -92.00\n",
      "  Episode 40/100: reward = -92.00\n",
      "  Episode 41/100: reward = -62.00\n",
      "  Episode 42/100: reward = -107.00\n",
      "  Episode 43/100: reward = -62.00\n",
      "  Episode 44/100: reward = -75.00\n",
      "  Episode 45/100: reward = -80.00\n",
      "  Episode 46/100: reward = -82.00\n",
      "  Episode 47/100: reward = -118.00\n",
      "  Episode 48/100: reward = -82.00\n",
      "  Episode 49/100: reward = -84.00\n",
      "  Episode 50/100: reward = -76.00\n",
      "  Episode 51/100: reward = -78.00\n",
      "  Episode 52/100: reward = -74.00\n",
      "  Episode 53/100: reward = -80.00\n",
      "  Episode 54/100: reward = -71.00\n",
      "  Episode 55/100: reward = -86.00\n",
      "  Episode 56/100: reward = -72.00\n",
      "  Episode 57/100: reward = -69.00\n",
      "  Episode 58/100: reward = -69.00\n",
      "  Episode 59/100: reward = -70.00\n",
      "  Episode 60/100: reward = -97.00\n",
      "  Episode 61/100: reward = -78.00\n",
      "  Episode 62/100: reward = -183.00\n",
      "  Episode 63/100: reward = -69.00\n",
      "  Episode 64/100: reward = -70.00\n",
      "  Episode 65/100: reward = -78.00\n",
      "  Episode 66/100: reward = -90.00\n",
      "  Episode 67/100: reward = -79.00\n",
      "  Episode 68/100: reward = -73.00\n",
      "  Episode 69/100: reward = -107.00\n",
      "  Episode 70/100: reward = -124.00\n",
      "  Episode 71/100: reward = -83.00\n",
      "  Episode 72/100: reward = -86.00\n",
      "  Episode 73/100: reward = -92.00\n",
      "  Episode 74/100: reward = -77.00\n",
      "  Episode 75/100: reward = -72.00\n",
      "  Episode 76/100: reward = -69.00\n",
      "  Episode 77/100: reward = -79.00\n",
      "  Episode 78/100: reward = -112.00\n",
      "  Episode 79/100: reward = -99.00\n",
      "  Episode 80/100: reward = -79.00\n",
      "  Episode 81/100: reward = -108.00\n",
      "  Episode 82/100: reward = -101.00\n",
      "  Episode 83/100: reward = -62.00\n",
      "  Episode 84/100: reward = -71.00\n",
      "  Episode 85/100: reward = -90.00\n",
      "  Episode 86/100: reward = -63.00\n",
      "  Episode 87/100: reward = -80.00\n",
      "  Episode 88/100: reward = -96.00\n",
      "  Episode 89/100: reward = -62.00\n",
      "  Episode 90/100: reward = -81.00\n",
      "  Episode 91/100: reward = -87.00\n",
      "  Episode 92/100: reward = -109.00\n",
      "  Episode 93/100: reward = -86.00\n",
      "  Episode 94/100: reward = -63.00\n",
      "  Episode 95/100: reward = -72.00\n",
      "  Episode 96/100: reward = -70.00\n",
      "  Episode 97/100: reward = -122.00\n",
      "  Episode 98/100: reward = -73.00\n",
      "  Episode 99/100: reward = -80.00\n",
      "  Episode 100/100: reward = -69.00\n",
      "✅ Model Acrobot_DQN_lr-0.01.pth: avg reward = -83.43\n",
      "🎥 Recording episode 29 for Acrobot_DQN_lr-0.01.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_lr-0.01.pth at ./videos/Acrobot-v1/Acrobot_DQN_lr-0.01/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_lr-1e-05.pth\n",
      "  Episode 1/100: reward = -500.00\n",
      "  Episode 2/100: reward = -500.00\n",
      "  Episode 3/100: reward = -500.00\n",
      "  Episode 4/100: reward = -500.00\n",
      "  Episode 5/100: reward = -500.00\n",
      "  Episode 6/100: reward = -500.00\n",
      "  Episode 7/100: reward = -500.00\n",
      "  Episode 8/100: reward = -500.00\n",
      "  Episode 9/100: reward = -500.00\n",
      "  Episode 10/100: reward = -500.00\n",
      "  Episode 11/100: reward = -500.00\n",
      "  Episode 12/100: reward = -500.00\n",
      "  Episode 13/100: reward = -500.00\n",
      "  Episode 14/100: reward = -500.00\n",
      "  Episode 15/100: reward = -500.00\n",
      "  Episode 16/100: reward = -500.00\n",
      "  Episode 17/100: reward = -500.00\n",
      "  Episode 18/100: reward = -500.00\n",
      "  Episode 19/100: reward = -500.00\n",
      "  Episode 20/100: reward = -500.00\n",
      "  Episode 21/100: reward = -500.00\n",
      "  Episode 22/100: reward = -500.00\n",
      "  Episode 23/100: reward = -500.00\n",
      "  Episode 24/100: reward = -500.00\n",
      "  Episode 25/100: reward = -500.00\n",
      "  Episode 26/100: reward = -500.00\n",
      "  Episode 27/100: reward = -500.00\n",
      "  Episode 28/100: reward = -500.00\n",
      "  Episode 29/100: reward = -500.00\n",
      "  Episode 30/100: reward = -500.00\n",
      "  Episode 31/100: reward = -500.00\n",
      "  Episode 32/100: reward = -500.00\n",
      "  Episode 33/100: reward = -500.00\n",
      "  Episode 34/100: reward = -500.00\n",
      "  Episode 35/100: reward = -500.00\n",
      "  Episode 36/100: reward = -500.00\n",
      "  Episode 37/100: reward = -500.00\n",
      "  Episode 38/100: reward = -500.00\n",
      "  Episode 39/100: reward = -500.00\n",
      "  Episode 40/100: reward = -500.00\n",
      "  Episode 41/100: reward = -500.00\n",
      "  Episode 42/100: reward = -500.00\n",
      "  Episode 43/100: reward = -500.00\n",
      "  Episode 44/100: reward = -500.00\n",
      "  Episode 45/100: reward = -500.00\n",
      "  Episode 46/100: reward = -500.00\n",
      "  Episode 47/100: reward = -500.00\n",
      "  Episode 48/100: reward = -500.00\n",
      "  Episode 49/100: reward = -500.00\n",
      "  Episode 50/100: reward = -500.00\n",
      "  Episode 51/100: reward = -500.00\n",
      "  Episode 52/100: reward = -500.00\n",
      "  Episode 53/100: reward = -500.00\n",
      "  Episode 54/100: reward = -500.00\n",
      "  Episode 55/100: reward = -500.00\n",
      "  Episode 56/100: reward = -500.00\n",
      "  Episode 57/100: reward = -500.00\n",
      "  Episode 58/100: reward = -500.00\n",
      "  Episode 59/100: reward = -500.00\n",
      "  Episode 60/100: reward = -500.00\n",
      "  Episode 61/100: reward = -500.00\n",
      "  Episode 62/100: reward = -500.00\n",
      "  Episode 63/100: reward = -500.00\n",
      "  Episode 64/100: reward = -500.00\n",
      "  Episode 65/100: reward = -500.00\n",
      "  Episode 66/100: reward = -500.00\n",
      "  Episode 67/100: reward = -500.00\n",
      "  Episode 68/100: reward = -500.00\n",
      "  Episode 69/100: reward = -500.00\n",
      "  Episode 70/100: reward = -500.00\n",
      "  Episode 71/100: reward = -500.00\n",
      "  Episode 72/100: reward = -500.00\n",
      "  Episode 73/100: reward = -500.00\n",
      "  Episode 74/100: reward = -500.00\n",
      "  Episode 75/100: reward = -500.00\n",
      "  Episode 76/100: reward = -500.00\n",
      "  Episode 77/100: reward = -500.00\n",
      "  Episode 78/100: reward = -500.00\n",
      "  Episode 79/100: reward = -500.00\n",
      "  Episode 80/100: reward = -500.00\n",
      "  Episode 81/100: reward = -500.00\n",
      "  Episode 82/100: reward = -500.00\n",
      "  Episode 83/100: reward = -500.00\n",
      "  Episode 84/100: reward = -500.00\n",
      "  Episode 85/100: reward = -500.00\n",
      "  Episode 86/100: reward = -500.00\n",
      "  Episode 87/100: reward = -500.00\n",
      "  Episode 88/100: reward = -500.00\n",
      "  Episode 89/100: reward = -500.00\n",
      "  Episode 90/100: reward = -500.00\n",
      "  Episode 91/100: reward = -500.00\n",
      "  Episode 92/100: reward = -500.00\n",
      "  Episode 93/100: reward = -500.00\n",
      "  Episode 94/100: reward = -500.00\n",
      "  Episode 95/100: reward = -500.00\n",
      "  Episode 96/100: reward = -500.00\n",
      "  Episode 97/100: reward = -500.00\n",
      "  Episode 98/100: reward = -500.00\n",
      "  Episode 99/100: reward = -500.00\n",
      "  Episode 100/100: reward = -500.00\n",
      "✅ Model Acrobot_DQN_lr-1e-05.pth: avg reward = -500.00\n",
      "🎥 Recording episode 60 for Acrobot_DQN_lr-1e-05.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_lr-1e-05.pth at ./videos/Acrobot-v1/Acrobot_DQN_lr-1e-05/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_mem-10000.pth\n",
      "  Episode 1/100: reward = -72.00\n",
      "  Episode 2/100: reward = -70.00\n",
      "  Episode 3/100: reward = -86.00\n",
      "  Episode 4/100: reward = -86.00\n",
      "  Episode 5/100: reward = -69.00\n",
      "  Episode 6/100: reward = -108.00\n",
      "  Episode 7/100: reward = -109.00\n",
      "  Episode 8/100: reward = -114.00\n",
      "  Episode 9/100: reward = -68.00\n",
      "  Episode 10/100: reward = -74.00\n",
      "  Episode 11/100: reward = -75.00\n",
      "  Episode 12/100: reward = -76.00\n",
      "  Episode 13/100: reward = -69.00\n",
      "  Episode 14/100: reward = -94.00\n",
      "  Episode 15/100: reward = -110.00\n",
      "  Episode 16/100: reward = -80.00\n",
      "  Episode 17/100: reward = -93.00\n",
      "  Episode 18/100: reward = -79.00\n",
      "  Episode 19/100: reward = -71.00\n",
      "  Episode 20/100: reward = -62.00\n",
      "  Episode 21/100: reward = -102.00\n",
      "  Episode 22/100: reward = -95.00\n",
      "  Episode 23/100: reward = -73.00\n",
      "  Episode 24/100: reward = -70.00\n",
      "  Episode 25/100: reward = -76.00\n",
      "  Episode 26/100: reward = -97.00\n",
      "  Episode 27/100: reward = -76.00\n",
      "  Episode 28/100: reward = -70.00\n",
      "  Episode 29/100: reward = -73.00\n",
      "  Episode 30/100: reward = -78.00\n",
      "  Episode 31/100: reward = -89.00\n",
      "  Episode 32/100: reward = -61.00\n",
      "  Episode 33/100: reward = -95.00\n",
      "  Episode 34/100: reward = -75.00\n",
      "  Episode 35/100: reward = -82.00\n",
      "  Episode 36/100: reward = -71.00\n",
      "  Episode 37/100: reward = -73.00\n",
      "  Episode 38/100: reward = -69.00\n",
      "  Episode 39/100: reward = -70.00\n",
      "  Episode 40/100: reward = -72.00\n",
      "  Episode 41/100: reward = -74.00\n",
      "  Episode 42/100: reward = -104.00\n",
      "  Episode 43/100: reward = -88.00\n",
      "  Episode 44/100: reward = -90.00\n",
      "  Episode 45/100: reward = -69.00\n",
      "  Episode 46/100: reward = -70.00\n",
      "  Episode 47/100: reward = -89.00\n",
      "  Episode 48/100: reward = -80.00\n",
      "  Episode 49/100: reward = -70.00\n",
      "  Episode 50/100: reward = -75.00\n",
      "  Episode 51/100: reward = -73.00\n",
      "  Episode 52/100: reward = -69.00\n",
      "  Episode 53/100: reward = -76.00\n",
      "  Episode 54/100: reward = -81.00\n",
      "  Episode 55/100: reward = -92.00\n",
      "  Episode 56/100: reward = -93.00\n",
      "  Episode 57/100: reward = -76.00\n",
      "  Episode 58/100: reward = -91.00\n",
      "  Episode 59/100: reward = -71.00\n",
      "  Episode 60/100: reward = -77.00\n",
      "  Episode 61/100: reward = -74.00\n",
      "  Episode 62/100: reward = -76.00\n",
      "  Episode 63/100: reward = -93.00\n",
      "  Episode 64/100: reward = -77.00\n",
      "  Episode 65/100: reward = -74.00\n",
      "  Episode 66/100: reward = -74.00\n",
      "  Episode 67/100: reward = -90.00\n",
      "  Episode 68/100: reward = -61.00\n",
      "  Episode 69/100: reward = -70.00\n",
      "  Episode 70/100: reward = -87.00\n",
      "  Episode 71/100: reward = -81.00\n",
      "  Episode 72/100: reward = -78.00\n",
      "  Episode 73/100: reward = -69.00\n",
      "  Episode 74/100: reward = -79.00\n",
      "  Episode 75/100: reward = -70.00\n",
      "  Episode 76/100: reward = -71.00\n",
      "  Episode 77/100: reward = -84.00\n",
      "  Episode 78/100: reward = -75.00\n",
      "  Episode 79/100: reward = -103.00\n",
      "  Episode 80/100: reward = -135.00\n",
      "  Episode 81/100: reward = -81.00\n",
      "  Episode 82/100: reward = -76.00\n",
      "  Episode 83/100: reward = -74.00\n",
      "  Episode 84/100: reward = -81.00\n",
      "  Episode 85/100: reward = -76.00\n",
      "  Episode 86/100: reward = -61.00\n",
      "  Episode 87/100: reward = -70.00\n",
      "  Episode 88/100: reward = -69.00\n",
      "  Episode 89/100: reward = -70.00\n",
      "  Episode 90/100: reward = -69.00\n",
      "  Episode 91/100: reward = -80.00\n",
      "  Episode 92/100: reward = -74.00\n",
      "  Episode 93/100: reward = -94.00\n",
      "  Episode 94/100: reward = -74.00\n",
      "  Episode 95/100: reward = -73.00\n",
      "  Episode 96/100: reward = -93.00\n",
      "  Episode 97/100: reward = -73.00\n",
      "  Episode 98/100: reward = -100.00\n",
      "  Episode 99/100: reward = -75.00\n",
      "  Episode 100/100: reward = -74.00\n",
      "✅ Model Acrobot_DQN_mem-10000.pth: avg reward = -79.98\n",
      "🎥 Recording episode 92 for Acrobot_DQN_mem-10000.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_mem-10000.pth at ./videos/Acrobot-v1/Acrobot_DQN_mem-10000/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_mem-500.pth\n",
      "  Episode 1/100: reward = -81.00\n",
      "  Episode 2/100: reward = -75.00\n",
      "  Episode 3/100: reward = -62.00\n",
      "  Episode 4/100: reward = -69.00\n",
      "  Episode 5/100: reward = -91.00\n",
      "  Episode 6/100: reward = -94.00\n",
      "  Episode 7/100: reward = -91.00\n",
      "  Episode 8/100: reward = -145.00\n",
      "  Episode 9/100: reward = -90.00\n",
      "  Episode 10/100: reward = -96.00\n",
      "  Episode 11/100: reward = -84.00\n",
      "  Episode 12/100: reward = -62.00\n",
      "  Episode 13/100: reward = -70.00\n",
      "  Episode 14/100: reward = -69.00\n",
      "  Episode 15/100: reward = -69.00\n",
      "  Episode 16/100: reward = -95.00\n",
      "  Episode 17/100: reward = -86.00\n",
      "  Episode 18/100: reward = -75.00\n",
      "  Episode 19/100: reward = -87.00\n",
      "  Episode 20/100: reward = -74.00\n",
      "  Episode 21/100: reward = -83.00\n",
      "  Episode 22/100: reward = -68.00\n",
      "  Episode 23/100: reward = -89.00\n",
      "  Episode 24/100: reward = -92.00\n",
      "  Episode 25/100: reward = -73.00\n",
      "  Episode 26/100: reward = -71.00\n",
      "  Episode 27/100: reward = -81.00\n",
      "  Episode 28/100: reward = -62.00\n",
      "  Episode 29/100: reward = -86.00\n",
      "  Episode 30/100: reward = -88.00\n",
      "  Episode 31/100: reward = -69.00\n",
      "  Episode 32/100: reward = -87.00\n",
      "  Episode 33/100: reward = -75.00\n",
      "  Episode 34/100: reward = -70.00\n",
      "  Episode 35/100: reward = -62.00\n",
      "  Episode 36/100: reward = -74.00\n",
      "  Episode 37/100: reward = -74.00\n",
      "  Episode 38/100: reward = -70.00\n",
      "  Episode 39/100: reward = -122.00\n",
      "  Episode 40/100: reward = -74.00\n",
      "  Episode 41/100: reward = -81.00\n",
      "  Episode 42/100: reward = -61.00\n",
      "  Episode 43/100: reward = -70.00\n",
      "  Episode 44/100: reward = -62.00\n",
      "  Episode 45/100: reward = -70.00\n",
      "  Episode 46/100: reward = -62.00\n",
      "  Episode 47/100: reward = -68.00\n",
      "  Episode 48/100: reward = -92.00\n",
      "  Episode 49/100: reward = -75.00\n",
      "  Episode 50/100: reward = -61.00\n",
      "  Episode 51/100: reward = -95.00\n",
      "  Episode 52/100: reward = -71.00\n",
      "  Episode 53/100: reward = -109.00\n",
      "  Episode 54/100: reward = -112.00\n",
      "  Episode 55/100: reward = -74.00\n",
      "  Episode 56/100: reward = -74.00\n",
      "  Episode 57/100: reward = -72.00\n",
      "  Episode 58/100: reward = -88.00\n",
      "  Episode 59/100: reward = -95.00\n",
      "  Episode 60/100: reward = -62.00\n",
      "  Episode 61/100: reward = -93.00\n",
      "  Episode 62/100: reward = -71.00\n",
      "  Episode 63/100: reward = -70.00\n",
      "  Episode 64/100: reward = -69.00\n",
      "  Episode 65/100: reward = -62.00\n",
      "  Episode 66/100: reward = -63.00\n",
      "  Episode 67/100: reward = -62.00\n",
      "  Episode 68/100: reward = -62.00\n",
      "  Episode 69/100: reward = -94.00\n",
      "  Episode 70/100: reward = -180.00\n",
      "  Episode 71/100: reward = -61.00\n",
      "  Episode 72/100: reward = -90.00\n",
      "  Episode 73/100: reward = -83.00\n",
      "  Episode 74/100: reward = -76.00\n",
      "  Episode 75/100: reward = -81.00\n",
      "  Episode 76/100: reward = -69.00\n",
      "  Episode 77/100: reward = -62.00\n",
      "  Episode 78/100: reward = -82.00\n",
      "  Episode 79/100: reward = -62.00\n",
      "  Episode 80/100: reward = -62.00\n",
      "  Episode 81/100: reward = -62.00\n",
      "  Episode 82/100: reward = -87.00\n",
      "  Episode 83/100: reward = -61.00\n",
      "  Episode 84/100: reward = -63.00\n",
      "  Episode 85/100: reward = -62.00\n",
      "  Episode 86/100: reward = -71.00\n",
      "  Episode 87/100: reward = -69.00\n",
      "  Episode 88/100: reward = -84.00\n",
      "  Episode 89/100: reward = -86.00\n",
      "  Episode 90/100: reward = -90.00\n",
      "  Episode 91/100: reward = -87.00\n",
      "  Episode 92/100: reward = -69.00\n",
      "  Episode 93/100: reward = -78.00\n",
      "  Episode 94/100: reward = -68.00\n",
      "  Episode 95/100: reward = -80.00\n",
      "  Episode 96/100: reward = -69.00\n",
      "  Episode 97/100: reward = -74.00\n",
      "  Episode 98/100: reward = -99.00\n",
      "  Episode 99/100: reward = -75.00\n",
      "  Episode 100/100: reward = -71.00\n",
      "✅ Model Acrobot_DQN_mem-500.pth: avg reward = -78.48\n",
      "🎥 Recording episode 75 for Acrobot_DQN_mem-500.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_mem-500.pth at ./videos/Acrobot-v1/Acrobot_DQN_mem-500/\n",
      "\n",
      "🎬 Testing and recording model: Acrobot_DQN_mem-50000.pth\n",
      "  Episode 1/100: reward = -88.00\n",
      "  Episode 2/100: reward = -117.00\n",
      "  Episode 3/100: reward = -100.00\n",
      "  Episode 4/100: reward = -75.00\n",
      "  Episode 5/100: reward = -122.00\n",
      "  Episode 6/100: reward = -107.00\n",
      "  Episode 7/100: reward = -97.00\n",
      "  Episode 8/100: reward = -102.00\n",
      "  Episode 9/100: reward = -70.00\n",
      "  Episode 10/100: reward = -112.00\n",
      "  Episode 11/100: reward = -79.00\n",
      "  Episode 12/100: reward = -109.00\n",
      "  Episode 13/100: reward = -75.00\n",
      "  Episode 14/100: reward = -112.00\n",
      "  Episode 15/100: reward = -64.00\n",
      "  Episode 16/100: reward = -71.00\n",
      "  Episode 17/100: reward = -102.00\n",
      "  Episode 18/100: reward = -76.00\n",
      "  Episode 19/100: reward = -112.00\n",
      "  Episode 20/100: reward = -87.00\n",
      "  Episode 21/100: reward = -79.00\n",
      "  Episode 22/100: reward = -70.00\n",
      "  Episode 23/100: reward = -69.00\n",
      "  Episode 24/100: reward = -70.00\n",
      "  Episode 25/100: reward = -76.00\n",
      "  Episode 26/100: reward = -86.00\n",
      "  Episode 27/100: reward = -96.00\n",
      "  Episode 28/100: reward = -64.00\n",
      "  Episode 29/100: reward = -63.00\n",
      "  Episode 30/100: reward = -76.00\n",
      "  Episode 31/100: reward = -71.00\n",
      "  Episode 32/100: reward = -63.00\n",
      "  Episode 33/100: reward = -71.00\n",
      "  Episode 34/100: reward = -70.00\n",
      "  Episode 35/100: reward = -120.00\n",
      "  Episode 36/100: reward = -104.00\n",
      "  Episode 37/100: reward = -71.00\n",
      "  Episode 38/100: reward = -72.00\n",
      "  Episode 39/100: reward = -80.00\n",
      "  Episode 40/100: reward = -77.00\n",
      "  Episode 41/100: reward = -85.00\n",
      "  Episode 42/100: reward = -76.00\n",
      "  Episode 43/100: reward = -97.00\n",
      "  Episode 44/100: reward = -76.00\n",
      "  Episode 45/100: reward = -104.00\n",
      "  Episode 46/100: reward = -75.00\n",
      "  Episode 47/100: reward = -76.00\n",
      "  Episode 48/100: reward = -71.00\n",
      "  Episode 49/100: reward = -63.00\n",
      "  Episode 50/100: reward = -69.00\n",
      "  Episode 51/100: reward = -70.00\n",
      "  Episode 52/100: reward = -82.00\n",
      "  Episode 53/100: reward = -73.00\n",
      "  Episode 54/100: reward = -88.00\n",
      "  Episode 55/100: reward = -104.00\n",
      "  Episode 56/100: reward = -86.00\n",
      "  Episode 57/100: reward = -72.00\n",
      "  Episode 58/100: reward = -86.00\n",
      "  Episode 59/100: reward = -80.00\n",
      "  Episode 60/100: reward = -64.00\n",
      "  Episode 61/100: reward = -91.00\n",
      "  Episode 62/100: reward = -76.00\n",
      "  Episode 63/100: reward = -72.00\n",
      "  Episode 64/100: reward = -96.00\n",
      "  Episode 65/100: reward = -75.00\n",
      "  Episode 66/100: reward = -75.00\n",
      "  Episode 67/100: reward = -76.00\n",
      "  Episode 68/100: reward = -70.00\n",
      "  Episode 69/100: reward = -62.00\n",
      "  Episode 70/100: reward = -89.00\n",
      "  Episode 71/100: reward = -69.00\n",
      "  Episode 72/100: reward = -78.00\n",
      "  Episode 73/100: reward = -71.00\n",
      "  Episode 74/100: reward = -70.00\n",
      "  Episode 75/100: reward = -71.00\n",
      "  Episode 76/100: reward = -63.00\n",
      "  Episode 77/100: reward = -63.00\n",
      "  Episode 78/100: reward = -70.00\n",
      "  Episode 79/100: reward = -76.00\n",
      "  Episode 80/100: reward = -63.00\n",
      "  Episode 81/100: reward = -97.00\n",
      "  Episode 82/100: reward = -80.00\n",
      "  Episode 83/100: reward = -86.00\n",
      "  Episode 84/100: reward = -106.00\n",
      "  Episode 85/100: reward = -71.00\n",
      "  Episode 86/100: reward = -72.00\n",
      "  Episode 87/100: reward = -64.00\n",
      "  Episode 88/100: reward = -76.00\n",
      "  Episode 89/100: reward = -133.00\n",
      "  Episode 90/100: reward = -62.00\n",
      "  Episode 91/100: reward = -82.00\n",
      "  Episode 92/100: reward = -70.00\n",
      "  Episode 93/100: reward = -69.00\n",
      "  Episode 94/100: reward = -129.00\n",
      "  Episode 95/100: reward = -71.00\n",
      "  Episode 96/100: reward = -76.00\n",
      "  Episode 97/100: reward = -80.00\n",
      "  Episode 98/100: reward = -96.00\n",
      "  Episode 99/100: reward = -106.00\n",
      "  Episode 100/100: reward = -86.00\n",
      "✅ Model Acrobot_DQN_mem-50000.pth: avg reward = -82.10\n",
      "🎥 Recording episode 57 for Acrobot_DQN_mem-50000.pth...\n",
      "🎞️ Saved video for Acrobot_DQN_mem-50000.pth at ./videos/Acrobot-v1/Acrobot_DQN_mem-50000/\n",
      "\n",
      "🏁 All models tested and recorded successfully.\n"
     ]
    }
   ],
   "source": [
    "record_models_in_folder(\n",
    "    models_folder=\"./Acrobot models\",\n",
    "    env_name=\"Acrobot-v1\",\n",
    "    num_discrete_actions=5,\n",
    "    num_tests=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64644b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_batch-128.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_batch-128.pth: avg reward = 500.00\n",
      "🎥 Recording episode 1 for CartPole_DQN_batch-128.pth...\n",
      "🎞️ Saved video for CartPole_DQN_batch-128.pth at ./videos/CartPole-v1/CartPole_DQN_batch-128/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_batch-256.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_batch-256.pth: avg reward = 500.00\n",
      "🎥 Recording episode 85 for CartPole_DQN_batch-256.pth...\n",
      "🎞️ Saved video for CartPole_DQN_batch-256.pth at ./videos/CartPole-v1/CartPole_DQN_batch-256/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_batch-64.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 493.00\n",
      "  Episode 3/100: reward = 169.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 316.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 123.00\n",
      "  Episode 10/100: reward = 422.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 166.00\n",
      "  Episode 13/100: reward = 489.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 204.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 336.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 161.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 160.00\n",
      "  Episode 26/100: reward = 376.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 250.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 316.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 479.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 269.00\n",
      "  Episode 45/100: reward = 413.00\n",
      "  Episode 46/100: reward = 392.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 369.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 434.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 246.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 497.00\n",
      "  Episode 57/100: reward = 301.00\n",
      "  Episode 58/100: reward = 130.00\n",
      "  Episode 59/100: reward = 431.00\n",
      "  Episode 60/100: reward = 424.00\n",
      "  Episode 61/100: reward = 269.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 184.00\n",
      "  Episode 64/100: reward = 404.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 229.00\n",
      "  Episode 68/100: reward = 144.00\n",
      "  Episode 69/100: reward = 432.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 282.00\n",
      "  Episode 73/100: reward = 130.00\n",
      "  Episode 74/100: reward = 449.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 482.00\n",
      "  Episode 78/100: reward = 428.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 147.00\n",
      "  Episode 81/100: reward = 266.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 322.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 459.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 307.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 157.00\n",
      "  Episode 91/100: reward = 132.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 140.00\n",
      "  Episode 97/100: reward = 190.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 450.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_batch-64.pth: avg reward = 408.69\n",
      "🎥 Recording episode 62 for CartPole_DQN_batch-64.pth...\n",
      "🎞️ Saved video for CartPole_DQN_batch-64.pth at ./videos/CartPole-v1/CartPole_DQN_batch-64/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_eps-1000.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_eps-1000.pth: avg reward = 500.00\n",
      "🎥 Recording episode 79 for CartPole_DQN_eps-1000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_eps-1000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for CartPole_DQN_eps-1000.pth at ./videos/CartPole-v1/CartPole_DQN_eps-1000/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_eps-5000.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_eps-5000.pth: avg reward = 500.00\n",
      "🎥 Recording episode 70 for CartPole_DQN_eps-5000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_eps-5000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for CartPole_DQN_eps-5000.pth at ./videos/CartPole-v1/CartPole_DQN_eps-5000/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_gamma-0.1.pth\n",
      "  Episode 1/100: reward = 9.00\n",
      "  Episode 2/100: reward = 10.00\n",
      "  Episode 3/100: reward = 9.00\n",
      "  Episode 4/100: reward = 9.00\n",
      "  Episode 5/100: reward = 10.00\n",
      "  Episode 6/100: reward = 9.00\n",
      "  Episode 7/100: reward = 9.00\n",
      "  Episode 8/100: reward = 9.00\n",
      "  Episode 9/100: reward = 11.00\n",
      "  Episode 10/100: reward = 10.00\n",
      "  Episode 11/100: reward = 9.00\n",
      "  Episode 12/100: reward = 9.00\n",
      "  Episode 13/100: reward = 11.00\n",
      "  Episode 14/100: reward = 9.00\n",
      "  Episode 15/100: reward = 10.00\n",
      "  Episode 16/100: reward = 9.00\n",
      "  Episode 17/100: reward = 10.00\n",
      "  Episode 18/100: reward = 8.00\n",
      "  Episode 19/100: reward = 10.00\n",
      "  Episode 20/100: reward = 10.00\n",
      "  Episode 21/100: reward = 10.00\n",
      "  Episode 22/100: reward = 11.00\n",
      "  Episode 23/100: reward = 9.00\n",
      "  Episode 24/100: reward = 9.00\n",
      "  Episode 25/100: reward = 9.00\n",
      "  Episode 26/100: reward = 10.00\n",
      "  Episode 27/100: reward = 8.00\n",
      "  Episode 28/100: reward = 9.00\n",
      "  Episode 29/100: reward = 10.00\n",
      "  Episode 30/100: reward = 10.00\n",
      "  Episode 31/100: reward = 11.00\n",
      "  Episode 32/100: reward = 10.00\n",
      "  Episode 33/100: reward = 9.00\n",
      "  Episode 34/100: reward = 11.00\n",
      "  Episode 35/100: reward = 10.00\n",
      "  Episode 36/100: reward = 10.00\n",
      "  Episode 37/100: reward = 10.00\n",
      "  Episode 38/100: reward = 9.00\n",
      "  Episode 39/100: reward = 11.00\n",
      "  Episode 40/100: reward = 9.00\n",
      "  Episode 41/100: reward = 11.00\n",
      "  Episode 42/100: reward = 10.00\n",
      "  Episode 43/100: reward = 13.00\n",
      "  Episode 44/100: reward = 10.00\n",
      "  Episode 45/100: reward = 9.00\n",
      "  Episode 46/100: reward = 10.00\n",
      "  Episode 47/100: reward = 10.00\n",
      "  Episode 48/100: reward = 9.00\n",
      "  Episode 49/100: reward = 10.00\n",
      "  Episode 50/100: reward = 10.00\n",
      "  Episode 51/100: reward = 9.00\n",
      "  Episode 52/100: reward = 16.00\n",
      "  Episode 53/100: reward = 11.00\n",
      "  Episode 54/100: reward = 10.00\n",
      "  Episode 55/100: reward = 10.00\n",
      "  Episode 56/100: reward = 8.00\n",
      "  Episode 57/100: reward = 10.00\n",
      "  Episode 58/100: reward = 9.00\n",
      "  Episode 59/100: reward = 9.00\n",
      "  Episode 60/100: reward = 10.00\n",
      "  Episode 61/100: reward = 10.00\n",
      "  Episode 62/100: reward = 9.00\n",
      "  Episode 63/100: reward = 9.00\n",
      "  Episode 64/100: reward = 9.00\n",
      "  Episode 65/100: reward = 10.00\n",
      "  Episode 66/100: reward = 10.00\n",
      "  Episode 67/100: reward = 8.00\n",
      "  Episode 68/100: reward = 9.00\n",
      "  Episode 69/100: reward = 9.00\n",
      "  Episode 70/100: reward = 11.00\n",
      "  Episode 71/100: reward = 9.00\n",
      "  Episode 72/100: reward = 12.00\n",
      "  Episode 73/100: reward = 9.00\n",
      "  Episode 74/100: reward = 9.00\n",
      "  Episode 75/100: reward = 9.00\n",
      "  Episode 76/100: reward = 10.00\n",
      "  Episode 77/100: reward = 9.00\n",
      "  Episode 78/100: reward = 11.00\n",
      "  Episode 79/100: reward = 10.00\n",
      "  Episode 80/100: reward = 9.00\n",
      "  Episode 81/100: reward = 9.00\n",
      "  Episode 82/100: reward = 11.00\n",
      "  Episode 83/100: reward = 11.00\n",
      "  Episode 84/100: reward = 12.00\n",
      "  Episode 85/100: reward = 10.00\n",
      "  Episode 86/100: reward = 9.00\n",
      "  Episode 87/100: reward = 10.00\n",
      "  Episode 88/100: reward = 9.00\n",
      "  Episode 89/100: reward = 9.00\n",
      "  Episode 90/100: reward = 10.00\n",
      "  Episode 91/100: reward = 9.00\n",
      "  Episode 92/100: reward = 10.00\n",
      "  Episode 93/100: reward = 13.00\n",
      "  Episode 94/100: reward = 10.00\n",
      "  Episode 95/100: reward = 10.00\n",
      "  Episode 96/100: reward = 9.00\n",
      "  Episode 97/100: reward = 10.00\n",
      "  Episode 98/100: reward = 10.00\n",
      "  Episode 99/100: reward = 10.00\n",
      "  Episode 100/100: reward = 9.00\n",
      "✅ Model CartPole_DQN_gamma-0.1.pth: avg reward = 9.80\n",
      "🎥 Recording episode 53 for CartPole_DQN_gamma-0.1.pth...\n",
      "🎞️ Saved video for CartPole_DQN_gamma-0.1.pth at ./videos/CartPole-v1/CartPole_DQN_gamma-0.1/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_gamma-0.5.pth\n",
      "  Episode 1/100: reward = 304.00\n",
      "  Episode 2/100: reward = 294.00\n",
      "  Episode 3/100: reward = 328.00\n",
      "  Episode 4/100: reward = 341.00\n",
      "  Episode 5/100: reward = 306.00\n",
      "  Episode 6/100: reward = 325.00\n",
      "  Episode 7/100: reward = 367.00\n",
      "  Episode 8/100: reward = 318.00\n",
      "  Episode 9/100: reward = 321.00\n",
      "  Episode 10/100: reward = 352.00\n",
      "  Episode 11/100: reward = 325.00\n",
      "  Episode 12/100: reward = 414.00\n",
      "  Episode 13/100: reward = 331.00\n",
      "  Episode 14/100: reward = 306.00\n",
      "  Episode 15/100: reward = 348.00\n",
      "  Episode 16/100: reward = 281.00\n",
      "  Episode 17/100: reward = 316.00\n",
      "  Episode 18/100: reward = 298.00\n",
      "  Episode 19/100: reward = 326.00\n",
      "  Episode 20/100: reward = 426.00\n",
      "  Episode 21/100: reward = 331.00\n",
      "  Episode 22/100: reward = 304.00\n",
      "  Episode 23/100: reward = 303.00\n",
      "  Episode 24/100: reward = 392.00\n",
      "  Episode 25/100: reward = 324.00\n",
      "  Episode 26/100: reward = 358.00\n",
      "  Episode 27/100: reward = 289.00\n",
      "  Episode 28/100: reward = 292.00\n",
      "  Episode 29/100: reward = 292.00\n",
      "  Episode 30/100: reward = 279.00\n",
      "  Episode 31/100: reward = 294.00\n",
      "  Episode 32/100: reward = 340.00\n",
      "  Episode 33/100: reward = 382.00\n",
      "  Episode 34/100: reward = 310.00\n",
      "  Episode 35/100: reward = 321.00\n",
      "  Episode 36/100: reward = 302.00\n",
      "  Episode 37/100: reward = 358.00\n",
      "  Episode 38/100: reward = 315.00\n",
      "  Episode 39/100: reward = 319.00\n",
      "  Episode 40/100: reward = 314.00\n",
      "  Episode 41/100: reward = 319.00\n",
      "  Episode 42/100: reward = 315.00\n",
      "  Episode 43/100: reward = 311.00\n",
      "  Episode 44/100: reward = 337.00\n",
      "  Episode 45/100: reward = 359.00\n",
      "  Episode 46/100: reward = 340.00\n",
      "  Episode 47/100: reward = 363.00\n",
      "  Episode 48/100: reward = 341.00\n",
      "  Episode 49/100: reward = 327.00\n",
      "  Episode 50/100: reward = 310.00\n",
      "  Episode 51/100: reward = 293.00\n",
      "  Episode 52/100: reward = 303.00\n",
      "  Episode 53/100: reward = 349.00\n",
      "  Episode 54/100: reward = 337.00\n",
      "  Episode 55/100: reward = 306.00\n",
      "  Episode 56/100: reward = 328.00\n",
      "  Episode 57/100: reward = 339.00\n",
      "  Episode 58/100: reward = 330.00\n",
      "  Episode 59/100: reward = 299.00\n",
      "  Episode 60/100: reward = 302.00\n",
      "  Episode 61/100: reward = 392.00\n",
      "  Episode 62/100: reward = 270.00\n",
      "  Episode 63/100: reward = 426.00\n",
      "  Episode 64/100: reward = 341.00\n",
      "  Episode 65/100: reward = 321.00\n",
      "  Episode 66/100: reward = 328.00\n",
      "  Episode 67/100: reward = 285.00\n",
      "  Episode 68/100: reward = 328.00\n",
      "  Episode 69/100: reward = 273.00\n",
      "  Episode 70/100: reward = 286.00\n",
      "  Episode 71/100: reward = 287.00\n",
      "  Episode 72/100: reward = 306.00\n",
      "  Episode 73/100: reward = 294.00\n",
      "  Episode 74/100: reward = 308.00\n",
      "  Episode 75/100: reward = 315.00\n",
      "  Episode 76/100: reward = 314.00\n",
      "  Episode 77/100: reward = 352.00\n",
      "  Episode 78/100: reward = 365.00\n",
      "  Episode 79/100: reward = 305.00\n",
      "  Episode 80/100: reward = 286.00\n",
      "  Episode 81/100: reward = 310.00\n",
      "  Episode 82/100: reward = 305.00\n",
      "  Episode 83/100: reward = 300.00\n",
      "  Episode 84/100: reward = 307.00\n",
      "  Episode 85/100: reward = 452.00\n",
      "  Episode 86/100: reward = 298.00\n",
      "  Episode 87/100: reward = 336.00\n",
      "  Episode 88/100: reward = 292.00\n",
      "  Episode 89/100: reward = 326.00\n",
      "  Episode 90/100: reward = 321.00\n",
      "  Episode 91/100: reward = 300.00\n",
      "  Episode 92/100: reward = 345.00\n",
      "  Episode 93/100: reward = 276.00\n",
      "  Episode 94/100: reward = 290.00\n",
      "  Episode 95/100: reward = 334.00\n",
      "  Episode 96/100: reward = 321.00\n",
      "  Episode 97/100: reward = 329.00\n",
      "  Episode 98/100: reward = 307.00\n",
      "  Episode 99/100: reward = 359.00\n",
      "  Episode 100/100: reward = 308.00\n",
      "✅ Model CartPole_DQN_gamma-0.5.pth: avg reward = 323.52\n",
      "🎥 Recording episode 87 for CartPole_DQN_gamma-0.5.pth...\n",
      "🎞️ Saved video for CartPole_DQN_gamma-0.5.pth at ./videos/CartPole-v1/CartPole_DQN_gamma-0.5/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_gamma-0.9.pth\n",
      "  Episode 1/100: reward = 187.00\n",
      "  Episode 2/100: reward = 151.00\n",
      "  Episode 3/100: reward = 153.00\n",
      "  Episode 4/100: reward = 163.00\n",
      "  Episode 5/100: reward = 226.00\n",
      "  Episode 6/100: reward = 202.00\n",
      "  Episode 7/100: reward = 142.00\n",
      "  Episode 8/100: reward = 231.00\n",
      "  Episode 9/100: reward = 168.00\n",
      "  Episode 10/100: reward = 221.00\n",
      "  Episode 11/100: reward = 220.00\n",
      "  Episode 12/100: reward = 168.00\n",
      "  Episode 13/100: reward = 179.00\n",
      "  Episode 14/100: reward = 205.00\n",
      "  Episode 15/100: reward = 219.00\n",
      "  Episode 16/100: reward = 187.00\n",
      "  Episode 17/100: reward = 159.00\n",
      "  Episode 18/100: reward = 187.00\n",
      "  Episode 19/100: reward = 168.00\n",
      "  Episode 20/100: reward = 216.00\n",
      "  Episode 21/100: reward = 261.00\n",
      "  Episode 22/100: reward = 186.00\n",
      "  Episode 23/100: reward = 179.00\n",
      "  Episode 24/100: reward = 160.00\n",
      "  Episode 25/100: reward = 160.00\n",
      "  Episode 26/100: reward = 248.00\n",
      "  Episode 27/100: reward = 166.00\n",
      "  Episode 28/100: reward = 172.00\n",
      "  Episode 29/100: reward = 194.00\n",
      "  Episode 30/100: reward = 286.00\n",
      "  Episode 31/100: reward = 346.00\n",
      "  Episode 32/100: reward = 236.00\n",
      "  Episode 33/100: reward = 232.00\n",
      "  Episode 34/100: reward = 191.00\n",
      "  Episode 35/100: reward = 233.00\n",
      "  Episode 36/100: reward = 146.00\n",
      "  Episode 37/100: reward = 199.00\n",
      "  Episode 38/100: reward = 188.00\n",
      "  Episode 39/100: reward = 183.00\n",
      "  Episode 40/100: reward = 189.00\n",
      "  Episode 41/100: reward = 188.00\n",
      "  Episode 42/100: reward = 228.00\n",
      "  Episode 43/100: reward = 147.00\n",
      "  Episode 44/100: reward = 208.00\n",
      "  Episode 45/100: reward = 234.00\n",
      "  Episode 46/100: reward = 185.00\n",
      "  Episode 47/100: reward = 176.00\n",
      "  Episode 48/100: reward = 174.00\n",
      "  Episode 49/100: reward = 191.00\n",
      "  Episode 50/100: reward = 291.00\n",
      "  Episode 51/100: reward = 182.00\n",
      "  Episode 52/100: reward = 159.00\n",
      "  Episode 53/100: reward = 202.00\n",
      "  Episode 54/100: reward = 144.00\n",
      "  Episode 55/100: reward = 246.00\n",
      "  Episode 56/100: reward = 150.00\n",
      "  Episode 57/100: reward = 190.00\n",
      "  Episode 58/100: reward = 208.00\n",
      "  Episode 59/100: reward = 216.00\n",
      "  Episode 60/100: reward = 155.00\n",
      "  Episode 61/100: reward = 302.00\n",
      "  Episode 62/100: reward = 143.00\n",
      "  Episode 63/100: reward = 211.00\n",
      "  Episode 64/100: reward = 233.00\n",
      "  Episode 65/100: reward = 188.00\n",
      "  Episode 66/100: reward = 194.00\n",
      "  Episode 67/100: reward = 144.00\n",
      "  Episode 68/100: reward = 154.00\n",
      "  Episode 69/100: reward = 174.00\n",
      "  Episode 70/100: reward = 162.00\n",
      "  Episode 71/100: reward = 205.00\n",
      "  Episode 72/100: reward = 152.00\n",
      "  Episode 73/100: reward = 188.00\n",
      "  Episode 74/100: reward = 203.00\n",
      "  Episode 75/100: reward = 215.00\n",
      "  Episode 76/100: reward = 236.00\n",
      "  Episode 77/100: reward = 189.00\n",
      "  Episode 78/100: reward = 191.00\n",
      "  Episode 79/100: reward = 217.00\n",
      "  Episode 80/100: reward = 162.00\n",
      "  Episode 81/100: reward = 179.00\n",
      "  Episode 82/100: reward = 233.00\n",
      "  Episode 83/100: reward = 210.00\n",
      "  Episode 84/100: reward = 149.00\n",
      "  Episode 85/100: reward = 247.00\n",
      "  Episode 86/100: reward = 210.00\n",
      "  Episode 87/100: reward = 147.00\n",
      "  Episode 88/100: reward = 149.00\n",
      "  Episode 89/100: reward = 176.00\n",
      "  Episode 90/100: reward = 145.00\n",
      "  Episode 91/100: reward = 190.00\n",
      "  Episode 92/100: reward = 188.00\n",
      "  Episode 93/100: reward = 187.00\n",
      "  Episode 94/100: reward = 146.00\n",
      "  Episode 95/100: reward = 208.00\n",
      "  Episode 96/100: reward = 209.00\n",
      "  Episode 97/100: reward = 187.00\n",
      "  Episode 98/100: reward = 165.00\n",
      "  Episode 99/100: reward = 234.00\n",
      "  Episode 100/100: reward = 205.00\n",
      "✅ Model CartPole_DQN_gamma-0.9.pth: avg reward = 194.08\n",
      "🎥 Recording episode 31 for CartPole_DQN_gamma-0.9.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_gamma-0.9 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for CartPole_DQN_gamma-0.9.pth at ./videos/CartPole-v1/CartPole_DQN_gamma-0.9/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_gamma-0.99.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_gamma-0.99.pth: avg reward = 500.00\n",
      "🎥 Recording episode 38 for CartPole_DQN_gamma-0.99.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_gamma-0.99 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for CartPole_DQN_gamma-0.99.pth at ./videos/CartPole-v1/CartPole_DQN_gamma-0.99/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_lr-0.0001.pth\n",
      "  Episode 1/100: reward = 71.00\n",
      "  Episode 2/100: reward = 62.00\n",
      "  Episode 3/100: reward = 68.00\n",
      "  Episode 4/100: reward = 68.00\n",
      "  Episode 5/100: reward = 63.00\n",
      "  Episode 6/100: reward = 60.00\n",
      "  Episode 7/100: reward = 72.00\n",
      "  Episode 8/100: reward = 66.00\n",
      "  Episode 9/100: reward = 62.00\n",
      "  Episode 10/100: reward = 59.00\n",
      "  Episode 11/100: reward = 72.00\n",
      "  Episode 12/100: reward = 68.00\n",
      "  Episode 13/100: reward = 59.00\n",
      "  Episode 14/100: reward = 64.00\n",
      "  Episode 15/100: reward = 68.00\n",
      "  Episode 16/100: reward = 68.00\n",
      "  Episode 17/100: reward = 72.00\n",
      "  Episode 18/100: reward = 64.00\n",
      "  Episode 19/100: reward = 60.00\n",
      "  Episode 20/100: reward = 72.00\n",
      "  Episode 21/100: reward = 53.00\n",
      "  Episode 22/100: reward = 69.00\n",
      "  Episode 23/100: reward = 64.00\n",
      "  Episode 24/100: reward = 68.00\n",
      "  Episode 25/100: reward = 64.00\n",
      "  Episode 26/100: reward = 63.00\n",
      "  Episode 27/100: reward = 60.00\n",
      "  Episode 28/100: reward = 62.00\n",
      "  Episode 29/100: reward = 64.00\n",
      "  Episode 30/100: reward = 70.00\n",
      "  Episode 31/100: reward = 72.00\n",
      "  Episode 32/100: reward = 70.00\n",
      "  Episode 33/100: reward = 59.00\n",
      "  Episode 34/100: reward = 66.00\n",
      "  Episode 35/100: reward = 64.00\n",
      "  Episode 36/100: reward = 69.00\n",
      "  Episode 37/100: reward = 62.00\n",
      "  Episode 38/100: reward = 63.00\n",
      "  Episode 39/100: reward = 65.00\n",
      "  Episode 40/100: reward = 74.00\n",
      "  Episode 41/100: reward = 56.00\n",
      "  Episode 42/100: reward = 63.00\n",
      "  Episode 43/100: reward = 68.00\n",
      "  Episode 44/100: reward = 65.00\n",
      "  Episode 45/100: reward = 70.00\n",
      "  Episode 46/100: reward = 64.00\n",
      "  Episode 47/100: reward = 70.00\n",
      "  Episode 48/100: reward = 70.00\n",
      "  Episode 49/100: reward = 60.00\n",
      "  Episode 50/100: reward = 68.00\n",
      "  Episode 51/100: reward = 60.00\n",
      "  Episode 52/100: reward = 62.00\n",
      "  Episode 53/100: reward = 68.00\n",
      "  Episode 54/100: reward = 66.00\n",
      "  Episode 55/100: reward = 60.00\n",
      "  Episode 56/100: reward = 68.00\n",
      "  Episode 57/100: reward = 64.00\n",
      "  Episode 58/100: reward = 66.00\n",
      "  Episode 59/100: reward = 62.00\n",
      "  Episode 60/100: reward = 66.00\n",
      "  Episode 61/100: reward = 68.00\n",
      "  Episode 62/100: reward = 78.00\n",
      "  Episode 63/100: reward = 68.00\n",
      "  Episode 64/100: reward = 57.00\n",
      "  Episode 65/100: reward = 70.00\n",
      "  Episode 66/100: reward = 66.00\n",
      "  Episode 67/100: reward = 63.00\n",
      "  Episode 68/100: reward = 66.00\n",
      "  Episode 69/100: reward = 67.00\n",
      "  Episode 70/100: reward = 62.00\n",
      "  Episode 71/100: reward = 76.00\n",
      "  Episode 72/100: reward = 66.00\n",
      "  Episode 73/100: reward = 65.00\n",
      "  Episode 74/100: reward = 62.00\n",
      "  Episode 75/100: reward = 58.00\n",
      "  Episode 76/100: reward = 75.00\n",
      "  Episode 77/100: reward = 65.00\n",
      "  Episode 78/100: reward = 71.00\n",
      "  Episode 79/100: reward = 68.00\n",
      "  Episode 80/100: reward = 69.00\n",
      "  Episode 81/100: reward = 61.00\n",
      "  Episode 82/100: reward = 69.00\n",
      "  Episode 83/100: reward = 59.00\n",
      "  Episode 84/100: reward = 67.00\n",
      "  Episode 85/100: reward = 69.00\n",
      "  Episode 86/100: reward = 68.00\n",
      "  Episode 87/100: reward = 76.00\n",
      "  Episode 88/100: reward = 63.00\n",
      "  Episode 89/100: reward = 70.00\n",
      "  Episode 90/100: reward = 67.00\n",
      "  Episode 91/100: reward = 60.00\n",
      "  Episode 92/100: reward = 73.00\n",
      "  Episode 93/100: reward = 67.00\n",
      "  Episode 94/100: reward = 64.00\n",
      "  Episode 95/100: reward = 68.00\n",
      "  Episode 96/100: reward = 71.00\n",
      "  Episode 97/100: reward = 67.00\n",
      "  Episode 98/100: reward = 67.00\n",
      "  Episode 99/100: reward = 70.00\n",
      "  Episode 100/100: reward = 59.00\n",
      "✅ Model CartPole_DQN_lr-0.0001.pth: avg reward = 65.90\n",
      "🎥 Recording episode 70 for CartPole_DQN_lr-0.0001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_lr-0.0001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for CartPole_DQN_lr-0.0001.pth at ./videos/CartPole-v1/CartPole_DQN_lr-0.0001/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_lr-0.0003.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_lr-0.0003.pth: avg reward = 500.00\n",
      "🎥 Recording episode 27 for CartPole_DQN_lr-0.0003.pth...\n",
      "🎞️ Saved video for CartPole_DQN_lr-0.0003.pth at ./videos/CartPole-v1/CartPole_DQN_lr-0.0003/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_lr-0.001.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_lr-0.001.pth: avg reward = 500.00\n",
      "🎥 Recording episode 30 for CartPole_DQN_lr-0.001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_lr-0.001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for CartPole_DQN_lr-0.001.pth at ./videos/CartPole-v1/CartPole_DQN_lr-0.001/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_lr-0.01.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_lr-0.01.pth: avg reward = 500.00\n",
      "🎥 Recording episode 65 for CartPole_DQN_lr-0.01.pth...\n",
      "🎞️ Saved video for CartPole_DQN_lr-0.01.pth at ./videos/CartPole-v1/CartPole_DQN_lr-0.01/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_lr-1e-05.pth\n",
      "  Episode 1/100: reward = 11.00\n",
      "  Episode 2/100: reward = 10.00\n",
      "  Episode 3/100: reward = 9.00\n",
      "  Episode 4/100: reward = 9.00\n",
      "  Episode 5/100: reward = 9.00\n",
      "  Episode 6/100: reward = 10.00\n",
      "  Episode 7/100: reward = 9.00\n",
      "  Episode 8/100: reward = 9.00\n",
      "  Episode 9/100: reward = 9.00\n",
      "  Episode 10/100: reward = 10.00\n",
      "  Episode 11/100: reward = 10.00\n",
      "  Episode 12/100: reward = 9.00\n",
      "  Episode 13/100: reward = 9.00\n",
      "  Episode 14/100: reward = 10.00\n",
      "  Episode 15/100: reward = 9.00\n",
      "  Episode 16/100: reward = 9.00\n",
      "  Episode 17/100: reward = 9.00\n",
      "  Episode 18/100: reward = 8.00\n",
      "  Episode 19/100: reward = 9.00\n",
      "  Episode 20/100: reward = 10.00\n",
      "  Episode 21/100: reward = 9.00\n",
      "  Episode 22/100: reward = 10.00\n",
      "  Episode 23/100: reward = 10.00\n",
      "  Episode 24/100: reward = 9.00\n",
      "  Episode 25/100: reward = 10.00\n",
      "  Episode 26/100: reward = 11.00\n",
      "  Episode 27/100: reward = 10.00\n",
      "  Episode 28/100: reward = 10.00\n",
      "  Episode 29/100: reward = 9.00\n",
      "  Episode 30/100: reward = 10.00\n",
      "  Episode 31/100: reward = 8.00\n",
      "  Episode 32/100: reward = 10.00\n",
      "  Episode 33/100: reward = 9.00\n",
      "  Episode 34/100: reward = 9.00\n",
      "  Episode 35/100: reward = 9.00\n",
      "  Episode 36/100: reward = 10.00\n",
      "  Episode 37/100: reward = 9.00\n",
      "  Episode 38/100: reward = 9.00\n",
      "  Episode 39/100: reward = 9.00\n",
      "  Episode 40/100: reward = 10.00\n",
      "  Episode 41/100: reward = 9.00\n",
      "  Episode 42/100: reward = 8.00\n",
      "  Episode 43/100: reward = 9.00\n",
      "  Episode 44/100: reward = 10.00\n",
      "  Episode 45/100: reward = 9.00\n",
      "  Episode 46/100: reward = 9.00\n",
      "  Episode 47/100: reward = 8.00\n",
      "  Episode 48/100: reward = 10.00\n",
      "  Episode 49/100: reward = 9.00\n",
      "  Episode 50/100: reward = 10.00\n",
      "  Episode 51/100: reward = 10.00\n",
      "  Episode 52/100: reward = 9.00\n",
      "  Episode 53/100: reward = 10.00\n",
      "  Episode 54/100: reward = 10.00\n",
      "  Episode 55/100: reward = 11.00\n",
      "  Episode 56/100: reward = 10.00\n",
      "  Episode 57/100: reward = 8.00\n",
      "  Episode 58/100: reward = 9.00\n",
      "  Episode 59/100: reward = 10.00\n",
      "  Episode 60/100: reward = 10.00\n",
      "  Episode 61/100: reward = 9.00\n",
      "  Episode 62/100: reward = 9.00\n",
      "  Episode 63/100: reward = 10.00\n",
      "  Episode 64/100: reward = 9.00\n",
      "  Episode 65/100: reward = 10.00\n",
      "  Episode 66/100: reward = 9.00\n",
      "  Episode 67/100: reward = 9.00\n",
      "  Episode 68/100: reward = 9.00\n",
      "  Episode 69/100: reward = 10.00\n",
      "  Episode 70/100: reward = 10.00\n",
      "  Episode 71/100: reward = 10.00\n",
      "  Episode 72/100: reward = 10.00\n",
      "  Episode 73/100: reward = 10.00\n",
      "  Episode 74/100: reward = 10.00\n",
      "  Episode 75/100: reward = 10.00\n",
      "  Episode 76/100: reward = 9.00\n",
      "  Episode 77/100: reward = 9.00\n",
      "  Episode 78/100: reward = 9.00\n",
      "  Episode 79/100: reward = 9.00\n",
      "  Episode 80/100: reward = 9.00\n",
      "  Episode 81/100: reward = 10.00\n",
      "  Episode 82/100: reward = 9.00\n",
      "  Episode 83/100: reward = 9.00\n",
      "  Episode 84/100: reward = 10.00\n",
      "  Episode 85/100: reward = 9.00\n",
      "  Episode 86/100: reward = 8.00\n",
      "  Episode 87/100: reward = 9.00\n",
      "  Episode 88/100: reward = 9.00\n",
      "  Episode 89/100: reward = 10.00\n",
      "  Episode 90/100: reward = 10.00\n",
      "  Episode 91/100: reward = 8.00\n",
      "  Episode 92/100: reward = 10.00\n",
      "  Episode 93/100: reward = 10.00\n",
      "  Episode 94/100: reward = 11.00\n",
      "  Episode 95/100: reward = 10.00\n",
      "  Episode 96/100: reward = 10.00\n",
      "  Episode 97/100: reward = 10.00\n",
      "  Episode 98/100: reward = 8.00\n",
      "  Episode 99/100: reward = 9.00\n",
      "  Episode 100/100: reward = 9.00\n",
      "✅ Model CartPole_DQN_lr-1e-05.pth: avg reward = 9.42\n",
      "🎥 Recording episode 56 for CartPole_DQN_lr-1e-05.pth...\n",
      "🎞️ Saved video for CartPole_DQN_lr-1e-05.pth at ./videos/CartPole-v1/CartPole_DQN_lr-1e-05/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_mem-10000.pth\n",
      "  Episode 1/100: reward = 500.00\n",
      "  Episode 2/100: reward = 500.00\n",
      "  Episode 3/100: reward = 500.00\n",
      "  Episode 4/100: reward = 500.00\n",
      "  Episode 5/100: reward = 500.00\n",
      "  Episode 6/100: reward = 500.00\n",
      "  Episode 7/100: reward = 500.00\n",
      "  Episode 8/100: reward = 500.00\n",
      "  Episode 9/100: reward = 500.00\n",
      "  Episode 10/100: reward = 500.00\n",
      "  Episode 11/100: reward = 500.00\n",
      "  Episode 12/100: reward = 500.00\n",
      "  Episode 13/100: reward = 500.00\n",
      "  Episode 14/100: reward = 500.00\n",
      "  Episode 15/100: reward = 500.00\n",
      "  Episode 16/100: reward = 500.00\n",
      "  Episode 17/100: reward = 500.00\n",
      "  Episode 18/100: reward = 500.00\n",
      "  Episode 19/100: reward = 500.00\n",
      "  Episode 20/100: reward = 500.00\n",
      "  Episode 21/100: reward = 500.00\n",
      "  Episode 22/100: reward = 500.00\n",
      "  Episode 23/100: reward = 500.00\n",
      "  Episode 24/100: reward = 500.00\n",
      "  Episode 25/100: reward = 500.00\n",
      "  Episode 26/100: reward = 500.00\n",
      "  Episode 27/100: reward = 500.00\n",
      "  Episode 28/100: reward = 500.00\n",
      "  Episode 29/100: reward = 500.00\n",
      "  Episode 30/100: reward = 500.00\n",
      "  Episode 31/100: reward = 500.00\n",
      "  Episode 32/100: reward = 500.00\n",
      "  Episode 33/100: reward = 500.00\n",
      "  Episode 34/100: reward = 500.00\n",
      "  Episode 35/100: reward = 500.00\n",
      "  Episode 36/100: reward = 500.00\n",
      "  Episode 37/100: reward = 500.00\n",
      "  Episode 38/100: reward = 500.00\n",
      "  Episode 39/100: reward = 500.00\n",
      "  Episode 40/100: reward = 500.00\n",
      "  Episode 41/100: reward = 500.00\n",
      "  Episode 42/100: reward = 500.00\n",
      "  Episode 43/100: reward = 500.00\n",
      "  Episode 44/100: reward = 500.00\n",
      "  Episode 45/100: reward = 500.00\n",
      "  Episode 46/100: reward = 500.00\n",
      "  Episode 47/100: reward = 500.00\n",
      "  Episode 48/100: reward = 500.00\n",
      "  Episode 49/100: reward = 500.00\n",
      "  Episode 50/100: reward = 500.00\n",
      "  Episode 51/100: reward = 500.00\n",
      "  Episode 52/100: reward = 500.00\n",
      "  Episode 53/100: reward = 500.00\n",
      "  Episode 54/100: reward = 500.00\n",
      "  Episode 55/100: reward = 500.00\n",
      "  Episode 56/100: reward = 500.00\n",
      "  Episode 57/100: reward = 500.00\n",
      "  Episode 58/100: reward = 500.00\n",
      "  Episode 59/100: reward = 500.00\n",
      "  Episode 60/100: reward = 500.00\n",
      "  Episode 61/100: reward = 500.00\n",
      "  Episode 62/100: reward = 500.00\n",
      "  Episode 63/100: reward = 500.00\n",
      "  Episode 64/100: reward = 500.00\n",
      "  Episode 65/100: reward = 500.00\n",
      "  Episode 66/100: reward = 500.00\n",
      "  Episode 67/100: reward = 500.00\n",
      "  Episode 68/100: reward = 500.00\n",
      "  Episode 69/100: reward = 500.00\n",
      "  Episode 70/100: reward = 500.00\n",
      "  Episode 71/100: reward = 500.00\n",
      "  Episode 72/100: reward = 500.00\n",
      "  Episode 73/100: reward = 500.00\n",
      "  Episode 74/100: reward = 500.00\n",
      "  Episode 75/100: reward = 500.00\n",
      "  Episode 76/100: reward = 500.00\n",
      "  Episode 77/100: reward = 500.00\n",
      "  Episode 78/100: reward = 500.00\n",
      "  Episode 79/100: reward = 500.00\n",
      "  Episode 80/100: reward = 500.00\n",
      "  Episode 81/100: reward = 500.00\n",
      "  Episode 82/100: reward = 500.00\n",
      "  Episode 83/100: reward = 500.00\n",
      "  Episode 84/100: reward = 500.00\n",
      "  Episode 85/100: reward = 500.00\n",
      "  Episode 86/100: reward = 500.00\n",
      "  Episode 87/100: reward = 500.00\n",
      "  Episode 88/100: reward = 500.00\n",
      "  Episode 89/100: reward = 500.00\n",
      "  Episode 90/100: reward = 500.00\n",
      "  Episode 91/100: reward = 500.00\n",
      "  Episode 92/100: reward = 500.00\n",
      "  Episode 93/100: reward = 500.00\n",
      "  Episode 94/100: reward = 500.00\n",
      "  Episode 95/100: reward = 500.00\n",
      "  Episode 96/100: reward = 500.00\n",
      "  Episode 97/100: reward = 500.00\n",
      "  Episode 98/100: reward = 500.00\n",
      "  Episode 99/100: reward = 500.00\n",
      "  Episode 100/100: reward = 500.00\n",
      "✅ Model CartPole_DQN_mem-10000.pth: avg reward = 500.00\n",
      "🎥 Recording episode 21 for CartPole_DQN_mem-10000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_mem-10000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for CartPole_DQN_mem-10000.pth at ./videos/CartPole-v1/CartPole_DQN_mem-10000/\n",
      "\n",
      "🎬 Testing and recording model: CartPole_DQN_mem-50000.pth\n",
      "  Episode 1/100: reward = 189.00\n",
      "  Episode 2/100: reward = 171.00\n",
      "  Episode 3/100: reward = 159.00\n",
      "  Episode 4/100: reward = 168.00\n",
      "  Episode 5/100: reward = 156.00\n",
      "  Episode 6/100: reward = 171.00\n",
      "  Episode 7/100: reward = 177.00\n",
      "  Episode 8/100: reward = 174.00\n",
      "  Episode 9/100: reward = 155.00\n",
      "  Episode 10/100: reward = 160.00\n",
      "  Episode 11/100: reward = 160.00\n",
      "  Episode 12/100: reward = 175.00\n",
      "  Episode 13/100: reward = 170.00\n",
      "  Episode 14/100: reward = 165.00\n",
      "  Episode 15/100: reward = 154.00\n",
      "  Episode 16/100: reward = 159.00\n",
      "  Episode 17/100: reward = 169.00\n",
      "  Episode 18/100: reward = 174.00\n",
      "  Episode 19/100: reward = 158.00\n",
      "  Episode 20/100: reward = 161.00\n",
      "  Episode 21/100: reward = 162.00\n",
      "  Episode 22/100: reward = 169.00\n",
      "  Episode 23/100: reward = 180.00\n",
      "  Episode 24/100: reward = 174.00\n",
      "  Episode 25/100: reward = 164.00\n",
      "  Episode 26/100: reward = 157.00\n",
      "  Episode 27/100: reward = 182.00\n",
      "  Episode 28/100: reward = 175.00\n",
      "  Episode 29/100: reward = 171.00\n",
      "  Episode 30/100: reward = 162.00\n",
      "  Episode 31/100: reward = 163.00\n",
      "  Episode 32/100: reward = 178.00\n",
      "  Episode 33/100: reward = 160.00\n",
      "  Episode 34/100: reward = 183.00\n",
      "  Episode 35/100: reward = 184.00\n",
      "  Episode 36/100: reward = 168.00\n",
      "  Episode 37/100: reward = 162.00\n",
      "  Episode 38/100: reward = 163.00\n",
      "  Episode 39/100: reward = 156.00\n",
      "  Episode 40/100: reward = 158.00\n",
      "  Episode 41/100: reward = 160.00\n",
      "  Episode 42/100: reward = 167.00\n",
      "  Episode 43/100: reward = 157.00\n",
      "  Episode 44/100: reward = 175.00\n",
      "  Episode 45/100: reward = 164.00\n",
      "  Episode 46/100: reward = 167.00\n",
      "  Episode 47/100: reward = 162.00\n",
      "  Episode 48/100: reward = 166.00\n",
      "  Episode 49/100: reward = 156.00\n",
      "  Episode 50/100: reward = 154.00\n",
      "  Episode 51/100: reward = 164.00\n",
      "  Episode 52/100: reward = 175.00\n",
      "  Episode 53/100: reward = 158.00\n",
      "  Episode 54/100: reward = 170.00\n",
      "  Episode 55/100: reward = 166.00\n",
      "  Episode 56/100: reward = 168.00\n",
      "  Episode 57/100: reward = 162.00\n",
      "  Episode 58/100: reward = 156.00\n",
      "  Episode 59/100: reward = 162.00\n",
      "  Episode 60/100: reward = 173.00\n",
      "  Episode 61/100: reward = 161.00\n",
      "  Episode 62/100: reward = 172.00\n",
      "  Episode 63/100: reward = 171.00\n",
      "  Episode 64/100: reward = 166.00\n",
      "  Episode 65/100: reward = 187.00\n",
      "  Episode 66/100: reward = 162.00\n",
      "  Episode 67/100: reward = 183.00\n",
      "  Episode 68/100: reward = 168.00\n",
      "  Episode 69/100: reward = 158.00\n",
      "  Episode 70/100: reward = 162.00\n",
      "  Episode 71/100: reward = 179.00\n",
      "  Episode 72/100: reward = 162.00\n",
      "  Episode 73/100: reward = 172.00\n",
      "  Episode 74/100: reward = 164.00\n",
      "  Episode 75/100: reward = 157.00\n",
      "  Episode 76/100: reward = 164.00\n",
      "  Episode 77/100: reward = 162.00\n",
      "  Episode 78/100: reward = 174.00\n",
      "  Episode 79/100: reward = 160.00\n",
      "  Episode 80/100: reward = 174.00\n",
      "  Episode 81/100: reward = 164.00\n",
      "  Episode 82/100: reward = 185.00\n",
      "  Episode 83/100: reward = 173.00\n",
      "  Episode 84/100: reward = 162.00\n",
      "  Episode 85/100: reward = 166.00\n",
      "  Episode 86/100: reward = 164.00\n",
      "  Episode 87/100: reward = 185.00\n",
      "  Episode 88/100: reward = 162.00\n",
      "  Episode 89/100: reward = 163.00\n",
      "  Episode 90/100: reward = 158.00\n",
      "  Episode 91/100: reward = 162.00\n",
      "  Episode 92/100: reward = 182.00\n",
      "  Episode 93/100: reward = 166.00\n",
      "  Episode 94/100: reward = 175.00\n",
      "  Episode 95/100: reward = 164.00\n",
      "  Episode 96/100: reward = 164.00\n",
      "  Episode 97/100: reward = 173.00\n",
      "  Episode 98/100: reward = 163.00\n",
      "  Episode 99/100: reward = 176.00\n",
      "  Episode 100/100: reward = 176.00\n",
      "✅ Model CartPole_DQN_mem-50000.pth: avg reward = 167.19\n",
      "🎥 Recording episode 2 for CartPole_DQN_mem-50000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_mem-50000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for CartPole_DQN_mem-50000.pth at ./videos/CartPole-v1/CartPole_DQN_mem-50000/\n",
      "\n",
      "🏁 All models tested and recorded successfully.\n"
     ]
    }
   ],
   "source": [
    "record_models_in_folder(\n",
    "    models_folder=\"./cartPole models\",\n",
    "    env_name=\"CartPole-v1\",\n",
    "    num_discrete_actions=5,\n",
    "    num_tests=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f7895e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_batch-128.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_batch-128.pth: avg reward = -200.00\n",
      "🎥 Recording episode 69 for MountainCar_DQN_batch-128.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_batch-128.pth at ./videos/MountainCar-v0/MountainCar_DQN_batch-128/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_batch-256.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_batch-256.pth: avg reward = -200.00\n",
      "🎥 Recording episode 95 for MountainCar_DQN_batch-256.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_batch-256.pth at ./videos/MountainCar-v0/MountainCar_DQN_batch-256/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_batch-64.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_batch-64.pth: avg reward = -200.00\n",
      "🎥 Recording episode 60 for MountainCar_DQN_batch-64.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_batch-64.pth at ./videos/MountainCar-v0/MountainCar_DQN_batch-64/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_eps-1000.pth\n",
      "  Episode 1/100: reward = -128.00\n",
      "  Episode 2/100: reward = -137.00\n",
      "  Episode 3/100: reward = -136.00\n",
      "  Episode 4/100: reward = -104.00\n",
      "  Episode 5/100: reward = -135.00\n",
      "  Episode 6/100: reward = -85.00\n",
      "  Episode 7/100: reward = -135.00\n",
      "  Episode 8/100: reward = -104.00\n",
      "  Episode 9/100: reward = -85.00\n",
      "  Episode 10/100: reward = -137.00\n",
      "  Episode 11/100: reward = -104.00\n",
      "  Episode 12/100: reward = -104.00\n",
      "  Episode 13/100: reward = -135.00\n",
      "  Episode 14/100: reward = -105.00\n",
      "  Episode 15/100: reward = -104.00\n",
      "  Episode 16/100: reward = -89.00\n",
      "  Episode 17/100: reward = -104.00\n",
      "  Episode 18/100: reward = -88.00\n",
      "  Episode 19/100: reward = -105.00\n",
      "  Episode 20/100: reward = -104.00\n",
      "  Episode 21/100: reward = -136.00\n",
      "  Episode 22/100: reward = -104.00\n",
      "  Episode 23/100: reward = -104.00\n",
      "  Episode 24/100: reward = -135.00\n",
      "  Episode 25/100: reward = -144.00\n",
      "  Episode 26/100: reward = -104.00\n",
      "  Episode 27/100: reward = -162.00\n",
      "  Episode 28/100: reward = -84.00\n",
      "  Episode 29/100: reward = -87.00\n",
      "  Episode 30/100: reward = -105.00\n",
      "  Episode 31/100: reward = -103.00\n",
      "  Episode 32/100: reward = -108.00\n",
      "  Episode 33/100: reward = -104.00\n",
      "  Episode 34/100: reward = -105.00\n",
      "  Episode 35/100: reward = -92.00\n",
      "  Episode 36/100: reward = -104.00\n",
      "  Episode 37/100: reward = -148.00\n",
      "  Episode 38/100: reward = -105.00\n",
      "  Episode 39/100: reward = -119.00\n",
      "  Episode 40/100: reward = -104.00\n",
      "  Episode 41/100: reward = -104.00\n",
      "  Episode 42/100: reward = -137.00\n",
      "  Episode 43/100: reward = -111.00\n",
      "  Episode 44/100: reward = -137.00\n",
      "  Episode 45/100: reward = -137.00\n",
      "  Episode 46/100: reward = -137.00\n",
      "  Episode 47/100: reward = -137.00\n",
      "  Episode 48/100: reward = -135.00\n",
      "  Episode 49/100: reward = -135.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -137.00\n",
      "  Episode 52/100: reward = -145.00\n",
      "  Episode 53/100: reward = -135.00\n",
      "  Episode 54/100: reward = -89.00\n",
      "  Episode 55/100: reward = -137.00\n",
      "  Episode 56/100: reward = -86.00\n",
      "  Episode 57/100: reward = -134.00\n",
      "  Episode 58/100: reward = -104.00\n",
      "  Episode 59/100: reward = -104.00\n",
      "  Episode 60/100: reward = -104.00\n",
      "  Episode 61/100: reward = -104.00\n",
      "  Episode 62/100: reward = -93.00\n",
      "  Episode 63/100: reward = -142.00\n",
      "  Episode 64/100: reward = -104.00\n",
      "  Episode 65/100: reward = -143.00\n",
      "  Episode 66/100: reward = -100.00\n",
      "  Episode 67/100: reward = -104.00\n",
      "  Episode 68/100: reward = -162.00\n",
      "  Episode 69/100: reward = -89.00\n",
      "  Episode 70/100: reward = -104.00\n",
      "  Episode 71/100: reward = -104.00\n",
      "  Episode 72/100: reward = -157.00\n",
      "  Episode 73/100: reward = -97.00\n",
      "  Episode 74/100: reward = -136.00\n",
      "  Episode 75/100: reward = -145.00\n",
      "  Episode 76/100: reward = -104.00\n",
      "  Episode 77/100: reward = -135.00\n",
      "  Episode 78/100: reward = -104.00\n",
      "  Episode 79/100: reward = -105.00\n",
      "  Episode 80/100: reward = -104.00\n",
      "  Episode 81/100: reward = -135.00\n",
      "  Episode 82/100: reward = -135.00\n",
      "  Episode 83/100: reward = -104.00\n",
      "  Episode 84/100: reward = -87.00\n",
      "  Episode 85/100: reward = -162.00\n",
      "  Episode 86/100: reward = -104.00\n",
      "  Episode 87/100: reward = -137.00\n",
      "  Episode 88/100: reward = -106.00\n",
      "  Episode 89/100: reward = -130.00\n",
      "  Episode 90/100: reward = -135.00\n",
      "  Episode 91/100: reward = -136.00\n",
      "  Episode 92/100: reward = -87.00\n",
      "  Episode 93/100: reward = -143.00\n",
      "  Episode 94/100: reward = -94.00\n",
      "  Episode 95/100: reward = -87.00\n",
      "  Episode 96/100: reward = -135.00\n",
      "  Episode 97/100: reward = -90.00\n",
      "  Episode 98/100: reward = -135.00\n",
      "  Episode 99/100: reward = -109.00\n",
      "  Episode 100/100: reward = -87.00\n",
      "✅ Model MountainCar_DQN_eps-1000.pth: avg reward = -117.04\n",
      "🎥 Recording episode 41 for MountainCar_DQN_eps-1000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_eps-1000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for MountainCar_DQN_eps-1000.pth at ./videos/MountainCar-v0/MountainCar_DQN_eps-1000/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_eps-2500.pth\n",
      "  Episode 1/100: reward = -143.00\n",
      "  Episode 2/100: reward = -175.00\n",
      "  Episode 3/100: reward = -144.00\n",
      "  Episode 4/100: reward = -111.00\n",
      "  Episode 5/100: reward = -91.00\n",
      "  Episode 6/100: reward = -96.00\n",
      "  Episode 7/100: reward = -92.00\n",
      "  Episode 8/100: reward = -93.00\n",
      "  Episode 9/100: reward = -87.00\n",
      "  Episode 10/100: reward = -97.00\n",
      "  Episode 11/100: reward = -83.00\n",
      "  Episode 12/100: reward = -98.00\n",
      "  Episode 13/100: reward = -143.00\n",
      "  Episode 14/100: reward = -171.00\n",
      "  Episode 15/100: reward = -143.00\n",
      "  Episode 16/100: reward = -144.00\n",
      "  Episode 17/100: reward = -83.00\n",
      "  Episode 18/100: reward = -143.00\n",
      "  Episode 19/100: reward = -113.00\n",
      "  Episode 20/100: reward = -84.00\n",
      "  Episode 21/100: reward = -167.00\n",
      "  Episode 22/100: reward = -143.00\n",
      "  Episode 23/100: reward = -143.00\n",
      "  Episode 24/100: reward = -87.00\n",
      "  Episode 25/100: reward = -104.00\n",
      "  Episode 26/100: reward = -144.00\n",
      "  Episode 27/100: reward = -111.00\n",
      "  Episode 28/100: reward = -143.00\n",
      "  Episode 29/100: reward = -143.00\n",
      "  Episode 30/100: reward = -96.00\n",
      "  Episode 31/100: reward = -84.00\n",
      "  Episode 32/100: reward = -143.00\n",
      "  Episode 33/100: reward = -83.00\n",
      "  Episode 34/100: reward = -143.00\n",
      "  Episode 35/100: reward = -85.00\n",
      "  Episode 36/100: reward = -143.00\n",
      "  Episode 37/100: reward = -143.00\n",
      "  Episode 38/100: reward = -187.00\n",
      "  Episode 39/100: reward = -165.00\n",
      "  Episode 40/100: reward = -143.00\n",
      "  Episode 41/100: reward = -88.00\n",
      "  Episode 42/100: reward = -143.00\n",
      "  Episode 43/100: reward = -188.00\n",
      "  Episode 44/100: reward = -143.00\n",
      "  Episode 45/100: reward = -143.00\n",
      "  Episode 46/100: reward = -144.00\n",
      "  Episode 47/100: reward = -143.00\n",
      "  Episode 48/100: reward = -142.00\n",
      "  Episode 49/100: reward = -90.00\n",
      "  Episode 50/100: reward = -169.00\n",
      "  Episode 51/100: reward = -102.00\n",
      "  Episode 52/100: reward = -87.00\n",
      "  Episode 53/100: reward = -102.00\n",
      "  Episode 54/100: reward = -143.00\n",
      "  Episode 55/100: reward = -144.00\n",
      "  Episode 56/100: reward = -165.00\n",
      "  Episode 57/100: reward = -143.00\n",
      "  Episode 58/100: reward = -112.00\n",
      "  Episode 59/100: reward = -85.00\n",
      "  Episode 60/100: reward = -144.00\n",
      "  Episode 61/100: reward = -107.00\n",
      "  Episode 62/100: reward = -85.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -143.00\n",
      "  Episode 65/100: reward = -143.00\n",
      "  Episode 66/100: reward = -166.00\n",
      "  Episode 67/100: reward = -167.00\n",
      "  Episode 68/100: reward = -143.00\n",
      "  Episode 69/100: reward = -109.00\n",
      "  Episode 70/100: reward = -86.00\n",
      "  Episode 71/100: reward = -91.00\n",
      "  Episode 72/100: reward = -144.00\n",
      "  Episode 73/100: reward = -143.00\n",
      "  Episode 74/100: reward = -89.00\n",
      "  Episode 75/100: reward = -84.00\n",
      "  Episode 76/100: reward = -144.00\n",
      "  Episode 77/100: reward = -143.00\n",
      "  Episode 78/100: reward = -144.00\n",
      "  Episode 79/100: reward = -144.00\n",
      "  Episode 80/100: reward = -143.00\n",
      "  Episode 81/100: reward = -143.00\n",
      "  Episode 82/100: reward = -93.00\n",
      "  Episode 83/100: reward = -95.00\n",
      "  Episode 84/100: reward = -143.00\n",
      "  Episode 85/100: reward = -144.00\n",
      "  Episode 86/100: reward = -143.00\n",
      "  Episode 87/100: reward = -92.00\n",
      "  Episode 88/100: reward = -87.00\n",
      "  Episode 89/100: reward = -169.00\n",
      "  Episode 90/100: reward = -175.00\n",
      "  Episode 91/100: reward = -144.00\n",
      "  Episode 92/100: reward = -143.00\n",
      "  Episode 93/100: reward = -167.00\n",
      "  Episode 94/100: reward = -85.00\n",
      "  Episode 95/100: reward = -144.00\n",
      "  Episode 96/100: reward = -93.00\n",
      "  Episode 97/100: reward = -166.00\n",
      "  Episode 98/100: reward = -143.00\n",
      "  Episode 99/100: reward = -144.00\n",
      "  Episode 100/100: reward = -94.00\n",
      "✅ Model MountainCar_DQN_eps-2500.pth: avg reward = -127.79\n",
      "🎥 Recording episode 67 for MountainCar_DQN_eps-2500.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_eps-2500.pth at ./videos/MountainCar-v0/MountainCar_DQN_eps-2500/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_eps-500.pth\n",
      "  Episode 1/100: reward = -138.00\n",
      "  Episode 2/100: reward = -137.00\n",
      "  Episode 3/100: reward = -141.00\n",
      "  Episode 4/100: reward = -174.00\n",
      "  Episode 5/100: reward = -153.00\n",
      "  Episode 6/100: reward = -140.00\n",
      "  Episode 7/100: reward = -137.00\n",
      "  Episode 8/100: reward = -138.00\n",
      "  Episode 9/100: reward = -149.00\n",
      "  Episode 10/100: reward = -138.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -144.00\n",
      "  Episode 13/100: reward = -108.00\n",
      "  Episode 14/100: reward = -138.00\n",
      "  Episode 15/100: reward = -101.00\n",
      "  Episode 16/100: reward = -144.00\n",
      "  Episode 17/100: reward = -141.00\n",
      "  Episode 18/100: reward = -156.00\n",
      "  Episode 19/100: reward = -148.00\n",
      "  Episode 20/100: reward = -137.00\n",
      "  Episode 21/100: reward = -139.00\n",
      "  Episode 22/100: reward = -138.00\n",
      "  Episode 23/100: reward = -142.00\n",
      "  Episode 24/100: reward = -137.00\n",
      "  Episode 25/100: reward = -109.00\n",
      "  Episode 26/100: reward = -149.00\n",
      "  Episode 27/100: reward = -144.00\n",
      "  Episode 28/100: reward = -138.00\n",
      "  Episode 29/100: reward = -96.00\n",
      "  Episode 30/100: reward = -146.00\n",
      "  Episode 31/100: reward = -148.00\n",
      "  Episode 32/100: reward = -140.00\n",
      "  Episode 33/100: reward = -148.00\n",
      "  Episode 34/100: reward = -138.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -140.00\n",
      "  Episode 37/100: reward = -139.00\n",
      "  Episode 38/100: reward = -144.00\n",
      "  Episode 39/100: reward = -138.00\n",
      "  Episode 40/100: reward = -137.00\n",
      "  Episode 41/100: reward = -159.00\n",
      "  Episode 42/100: reward = -140.00\n",
      "  Episode 43/100: reward = -113.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -139.00\n",
      "  Episode 46/100: reward = -170.00\n",
      "  Episode 47/100: reward = -149.00\n",
      "  Episode 48/100: reward = -141.00\n",
      "  Episode 49/100: reward = -141.00\n",
      "  Episode 50/100: reward = -138.00\n",
      "  Episode 51/100: reward = -150.00\n",
      "  Episode 52/100: reward = -158.00\n",
      "  Episode 53/100: reward = -137.00\n",
      "  Episode 54/100: reward = -137.00\n",
      "  Episode 55/100: reward = -148.00\n",
      "  Episode 56/100: reward = -144.00\n",
      "  Episode 57/100: reward = -142.00\n",
      "  Episode 58/100: reward = -137.00\n",
      "  Episode 59/100: reward = -138.00\n",
      "  Episode 60/100: reward = -141.00\n",
      "  Episode 61/100: reward = -146.00\n",
      "  Episode 62/100: reward = -144.00\n",
      "  Episode 63/100: reward = -156.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -110.00\n",
      "  Episode 66/100: reward = -144.00\n",
      "  Episode 67/100: reward = -140.00\n",
      "  Episode 68/100: reward = -148.00\n",
      "  Episode 69/100: reward = -144.00\n",
      "  Episode 70/100: reward = -141.00\n",
      "  Episode 71/100: reward = -102.00\n",
      "  Episode 72/100: reward = -139.00\n",
      "  Episode 73/100: reward = -151.00\n",
      "  Episode 74/100: reward = -139.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -139.00\n",
      "  Episode 77/100: reward = -140.00\n",
      "  Episode 78/100: reward = -108.00\n",
      "  Episode 79/100: reward = -148.00\n",
      "  Episode 80/100: reward = -148.00\n",
      "  Episode 81/100: reward = -140.00\n",
      "  Episode 82/100: reward = -144.00\n",
      "  Episode 83/100: reward = -147.00\n",
      "  Episode 84/100: reward = -140.00\n",
      "  Episode 85/100: reward = -141.00\n",
      "  Episode 86/100: reward = -99.00\n",
      "  Episode 87/100: reward = -147.00\n",
      "  Episode 88/100: reward = -144.00\n",
      "  Episode 89/100: reward = -138.00\n",
      "  Episode 90/100: reward = -137.00\n",
      "  Episode 91/100: reward = -139.00\n",
      "  Episode 92/100: reward = -125.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -144.00\n",
      "  Episode 95/100: reward = -144.00\n",
      "  Episode 96/100: reward = -148.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -153.00\n",
      "  Episode 99/100: reward = -139.00\n",
      "  Episode 100/100: reward = -140.00\n",
      "✅ Model MountainCar_DQN_eps-500.pth: avg reward = -143.90\n",
      "🎥 Recording episode 14 for MountainCar_DQN_eps-500.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_eps-500.pth at ./videos/MountainCar-v0/MountainCar_DQN_eps-500/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_eps-5000.pth\n",
      "  Episode 1/100: reward = -92.00\n",
      "  Episode 2/100: reward = -154.00\n",
      "  Episode 3/100: reward = -98.00\n",
      "  Episode 4/100: reward = -160.00\n",
      "  Episode 5/100: reward = -138.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -148.00\n",
      "  Episode 8/100: reward = -94.00\n",
      "  Episode 9/100: reward = -143.00\n",
      "  Episode 10/100: reward = -146.00\n",
      "  Episode 11/100: reward = -96.00\n",
      "  Episode 12/100: reward = -147.00\n",
      "  Episode 13/100: reward = -145.00\n",
      "  Episode 14/100: reward = -93.00\n",
      "  Episode 15/100: reward = -141.00\n",
      "  Episode 16/100: reward = -93.00\n",
      "  Episode 17/100: reward = -163.00\n",
      "  Episode 18/100: reward = -159.00\n",
      "  Episode 19/100: reward = -140.00\n",
      "  Episode 20/100: reward = -98.00\n",
      "  Episode 21/100: reward = -146.00\n",
      "  Episode 22/100: reward = -142.00\n",
      "  Episode 23/100: reward = -142.00\n",
      "  Episode 24/100: reward = -149.00\n",
      "  Episode 25/100: reward = -137.00\n",
      "  Episode 26/100: reward = -148.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -122.00\n",
      "  Episode 29/100: reward = -137.00\n",
      "  Episode 30/100: reward = -147.00\n",
      "  Episode 31/100: reward = -142.00\n",
      "  Episode 32/100: reward = -141.00\n",
      "  Episode 33/100: reward = -147.00\n",
      "  Episode 34/100: reward = -129.00\n",
      "  Episode 35/100: reward = -146.00\n",
      "  Episode 36/100: reward = -90.00\n",
      "  Episode 37/100: reward = -141.00\n",
      "  Episode 38/100: reward = -147.00\n",
      "  Episode 39/100: reward = -145.00\n",
      "  Episode 40/100: reward = -147.00\n",
      "  Episode 41/100: reward = -135.00\n",
      "  Episode 42/100: reward = -141.00\n",
      "  Episode 43/100: reward = -102.00\n",
      "  Episode 44/100: reward = -151.00\n",
      "  Episode 45/100: reward = -137.00\n",
      "  Episode 46/100: reward = -160.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -144.00\n",
      "  Episode 49/100: reward = -146.00\n",
      "  Episode 50/100: reward = -99.00\n",
      "  Episode 51/100: reward = -149.00\n",
      "  Episode 52/100: reward = -137.00\n",
      "  Episode 53/100: reward = -142.00\n",
      "  Episode 54/100: reward = -135.00\n",
      "  Episode 55/100: reward = -105.00\n",
      "  Episode 56/100: reward = -149.00\n",
      "  Episode 57/100: reward = -143.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -148.00\n",
      "  Episode 60/100: reward = -141.00\n",
      "  Episode 61/100: reward = -154.00\n",
      "  Episode 62/100: reward = -169.00\n",
      "  Episode 63/100: reward = -143.00\n",
      "  Episode 64/100: reward = -143.00\n",
      "  Episode 65/100: reward = -150.00\n",
      "  Episode 66/100: reward = -143.00\n",
      "  Episode 67/100: reward = -141.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -96.00\n",
      "  Episode 70/100: reward = -142.00\n",
      "  Episode 71/100: reward = -156.00\n",
      "  Episode 72/100: reward = -152.00\n",
      "  Episode 73/100: reward = -168.00\n",
      "  Episode 74/100: reward = -143.00\n",
      "  Episode 75/100: reward = -94.00\n",
      "  Episode 76/100: reward = -140.00\n",
      "  Episode 77/100: reward = -157.00\n",
      "  Episode 78/100: reward = -140.00\n",
      "  Episode 79/100: reward = -143.00\n",
      "  Episode 80/100: reward = -140.00\n",
      "  Episode 81/100: reward = -147.00\n",
      "  Episode 82/100: reward = -157.00\n",
      "  Episode 83/100: reward = -137.00\n",
      "  Episode 84/100: reward = -135.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -141.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -143.00\n",
      "  Episode 89/100: reward = -146.00\n",
      "  Episode 90/100: reward = -143.00\n",
      "  Episode 91/100: reward = -143.00\n",
      "  Episode 92/100: reward = -104.00\n",
      "  Episode 93/100: reward = -141.00\n",
      "  Episode 94/100: reward = -143.00\n",
      "  Episode 95/100: reward = -137.00\n",
      "  Episode 96/100: reward = -148.00\n",
      "  Episode 97/100: reward = -140.00\n",
      "  Episode 98/100: reward = -138.00\n",
      "  Episode 99/100: reward = -90.00\n",
      "  Episode 100/100: reward = -143.00\n",
      "✅ Model MountainCar_DQN_eps-5000.pth: avg reward = -141.47\n",
      "🎥 Recording episode 30 for MountainCar_DQN_eps-5000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_eps-5000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for MountainCar_DQN_eps-5000.pth at ./videos/MountainCar-v0/MountainCar_DQN_eps-5000/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_gamma-0.1.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_gamma-0.1.pth: avg reward = -200.00\n",
      "🎥 Recording episode 14 for MountainCar_DQN_gamma-0.1.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_gamma-0.1.pth at ./videos/MountainCar-v0/MountainCar_DQN_gamma-0.1/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_gamma-0.5.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_gamma-0.5.pth: avg reward = -200.00\n",
      "🎥 Recording episode 22 for MountainCar_DQN_gamma-0.5.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_gamma-0.5.pth at ./videos/MountainCar-v0/MountainCar_DQN_gamma-0.5/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_gamma-0.9.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_gamma-0.9.pth: avg reward = -200.00\n",
      "🎥 Recording episode 84 for MountainCar_DQN_gamma-0.9.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_gamma-0.9 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for MountainCar_DQN_gamma-0.9.pth at ./videos/MountainCar-v0/MountainCar_DQN_gamma-0.9/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_gamma-0.99.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_gamma-0.99.pth: avg reward = -200.00\n",
      "🎥 Recording episode 70 for MountainCar_DQN_gamma-0.99.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_gamma-0.99 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for MountainCar_DQN_gamma-0.99.pth at ./videos/MountainCar-v0/MountainCar_DQN_gamma-0.99/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_lr-0.0003.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_lr-0.0003.pth: avg reward = -200.00\n",
      "🎥 Recording episode 87 for MountainCar_DQN_lr-0.0003.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_lr-0.0003.pth at ./videos/MountainCar-v0/MountainCar_DQN_lr-0.0003/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_lr-0.001.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_lr-0.001.pth: avg reward = -200.00\n",
      "🎥 Recording episode 41 for MountainCar_DQN_lr-0.001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_lr-0.001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for MountainCar_DQN_lr-0.001.pth at ./videos/MountainCar-v0/MountainCar_DQN_lr-0.001/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_lr-0.01.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_lr-0.01.pth: avg reward = -200.00\n",
      "🎥 Recording episode 100 for MountainCar_DQN_lr-0.01.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_lr-0.01.pth at ./videos/MountainCar-v0/MountainCar_DQN_lr-0.01/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_lr-1e-05.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -200.00\n",
      "  Episode 4/100: reward = -200.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -200.00\n",
      "  Episode 7/100: reward = -200.00\n",
      "  Episode 8/100: reward = -200.00\n",
      "  Episode 9/100: reward = -200.00\n",
      "  Episode 10/100: reward = -200.00\n",
      "  Episode 11/100: reward = -200.00\n",
      "  Episode 12/100: reward = -200.00\n",
      "  Episode 13/100: reward = -200.00\n",
      "  Episode 14/100: reward = -200.00\n",
      "  Episode 15/100: reward = -200.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -200.00\n",
      "  Episode 18/100: reward = -200.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -200.00\n",
      "  Episode 21/100: reward = -200.00\n",
      "  Episode 22/100: reward = -200.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -200.00\n",
      "  Episode 25/100: reward = -200.00\n",
      "  Episode 26/100: reward = -200.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -200.00\n",
      "  Episode 31/100: reward = -200.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -200.00\n",
      "  Episode 34/100: reward = -200.00\n",
      "  Episode 35/100: reward = -200.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -200.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -200.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -200.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -200.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -200.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -200.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -200.00\n",
      "  Episode 60/100: reward = -200.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -200.00\n",
      "  Episode 63/100: reward = -200.00\n",
      "  Episode 64/100: reward = -200.00\n",
      "  Episode 65/100: reward = -200.00\n",
      "  Episode 66/100: reward = -200.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -200.00\n",
      "  Episode 69/100: reward = -200.00\n",
      "  Episode 70/100: reward = -200.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -200.00\n",
      "  Episode 74/100: reward = -200.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -200.00\n",
      "  Episode 77/100: reward = -200.00\n",
      "  Episode 78/100: reward = -200.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -200.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -200.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -200.00\n",
      "  Episode 95/100: reward = -200.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -200.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_lr-1e-05.pth: avg reward = -200.00\n",
      "🎥 Recording episode 13 for MountainCar_DQN_lr-1e-05.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_lr-1e-05.pth at ./videos/MountainCar-v0/MountainCar_DQN_lr-1e-05/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_mem-10000 .pth\n",
      "  Episode 1/100: reward = -140.00\n",
      "  Episode 2/100: reward = -141.00\n",
      "  Episode 3/100: reward = -140.00\n",
      "  Episode 4/100: reward = -143.00\n",
      "  Episode 5/100: reward = -200.00\n",
      "  Episode 6/100: reward = -143.00\n",
      "  Episode 7/100: reward = -141.00\n",
      "  Episode 8/100: reward = -142.00\n",
      "  Episode 9/100: reward = -140.00\n",
      "  Episode 10/100: reward = -142.00\n",
      "  Episode 11/100: reward = -140.00\n",
      "  Episode 12/100: reward = -140.00\n",
      "  Episode 13/100: reward = -140.00\n",
      "  Episode 14/100: reward = -148.00\n",
      "  Episode 15/100: reward = -153.00\n",
      "  Episode 16/100: reward = -142.00\n",
      "  Episode 17/100: reward = -199.00\n",
      "  Episode 18/100: reward = -144.00\n",
      "  Episode 19/100: reward = -140.00\n",
      "  Episode 20/100: reward = -141.00\n",
      "  Episode 21/100: reward = -141.00\n",
      "  Episode 22/100: reward = -150.00\n",
      "  Episode 23/100: reward = -200.00\n",
      "  Episode 24/100: reward = -151.00\n",
      "  Episode 25/100: reward = -141.00\n",
      "  Episode 26/100: reward = -142.00\n",
      "  Episode 27/100: reward = -144.00\n",
      "  Episode 28/100: reward = -140.00\n",
      "  Episode 29/100: reward = -146.00\n",
      "  Episode 30/100: reward = -150.00\n",
      "  Episode 31/100: reward = -142.00\n",
      "  Episode 32/100: reward = -200.00\n",
      "  Episode 33/100: reward = -143.00\n",
      "  Episode 34/100: reward = -141.00\n",
      "  Episode 35/100: reward = -149.00\n",
      "  Episode 36/100: reward = -200.00\n",
      "  Episode 37/100: reward = -200.00\n",
      "  Episode 38/100: reward = -140.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -144.00\n",
      "  Episode 41/100: reward = -140.00\n",
      "  Episode 42/100: reward = -140.00\n",
      "  Episode 43/100: reward = -146.00\n",
      "  Episode 44/100: reward = -200.00\n",
      "  Episode 45/100: reward = -142.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -149.00\n",
      "  Episode 48/100: reward = -143.00\n",
      "  Episode 49/100: reward = -150.00\n",
      "  Episode 50/100: reward = -140.00\n",
      "  Episode 51/100: reward = -141.00\n",
      "  Episode 52/100: reward = -200.00\n",
      "  Episode 53/100: reward = -141.00\n",
      "  Episode 54/100: reward = -147.00\n",
      "  Episode 55/100: reward = -148.00\n",
      "  Episode 56/100: reward = -145.00\n",
      "  Episode 57/100: reward = -144.00\n",
      "  Episode 58/100: reward = -145.00\n",
      "  Episode 59/100: reward = -141.00\n",
      "  Episode 60/100: reward = -145.00\n",
      "  Episode 61/100: reward = -141.00\n",
      "  Episode 62/100: reward = -142.00\n",
      "  Episode 63/100: reward = -141.00\n",
      "  Episode 64/100: reward = -140.00\n",
      "  Episode 65/100: reward = -148.00\n",
      "  Episode 66/100: reward = -141.00\n",
      "  Episode 67/100: reward = -141.00\n",
      "  Episode 68/100: reward = -142.00\n",
      "  Episode 69/100: reward = -141.00\n",
      "  Episode 70/100: reward = -145.00\n",
      "  Episode 71/100: reward = -149.00\n",
      "  Episode 72/100: reward = -200.00\n",
      "  Episode 73/100: reward = -142.00\n",
      "  Episode 74/100: reward = -150.00\n",
      "  Episode 75/100: reward = -142.00\n",
      "  Episode 76/100: reward = -147.00\n",
      "  Episode 77/100: reward = -146.00\n",
      "  Episode 78/100: reward = -141.00\n",
      "  Episode 79/100: reward = -142.00\n",
      "  Episode 80/100: reward = -144.00\n",
      "  Episode 81/100: reward = -200.00\n",
      "  Episode 82/100: reward = -140.00\n",
      "  Episode 83/100: reward = -145.00\n",
      "  Episode 84/100: reward = -200.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -200.00\n",
      "  Episode 88/100: reward = -142.00\n",
      "  Episode 89/100: reward = -200.00\n",
      "  Episode 90/100: reward = -147.00\n",
      "  Episode 91/100: reward = -141.00\n",
      "  Episode 92/100: reward = -150.00\n",
      "  Episode 93/100: reward = -200.00\n",
      "  Episode 94/100: reward = -142.00\n",
      "  Episode 95/100: reward = -199.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -143.00\n",
      "  Episode 98/100: reward = -200.00\n",
      "  Episode 99/100: reward = -199.00\n",
      "  Episode 100/100: reward = -140.00\n",
      "✅ Model MountainCar_DQN_mem-10000 .pth: avg reward = -155.83\n",
      "🎥 Recording episode 62 for MountainCar_DQN_mem-10000 .pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_mem-10000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for MountainCar_DQN_mem-10000 .pth at ./videos/MountainCar-v0/MountainCar_DQN_mem-10000 /\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_mem-500.pth\n",
      "  Episode 1/100: reward = -200.00\n",
      "  Episode 2/100: reward = -200.00\n",
      "  Episode 3/100: reward = -174.00\n",
      "  Episode 4/100: reward = -182.00\n",
      "  Episode 5/100: reward = -166.00\n",
      "  Episode 6/100: reward = -92.00\n",
      "  Episode 7/100: reward = -90.00\n",
      "  Episode 8/100: reward = -180.00\n",
      "  Episode 9/100: reward = -89.00\n",
      "  Episode 10/100: reward = -90.00\n",
      "  Episode 11/100: reward = -135.00\n",
      "  Episode 12/100: reward = -91.00\n",
      "  Episode 13/100: reward = -185.00\n",
      "  Episode 14/100: reward = -110.00\n",
      "  Episode 15/100: reward = -166.00\n",
      "  Episode 16/100: reward = -200.00\n",
      "  Episode 17/100: reward = -197.00\n",
      "  Episode 18/100: reward = -170.00\n",
      "  Episode 19/100: reward = -200.00\n",
      "  Episode 20/100: reward = -103.00\n",
      "  Episode 21/100: reward = -167.00\n",
      "  Episode 22/100: reward = -136.00\n",
      "  Episode 23/100: reward = -90.00\n",
      "  Episode 24/100: reward = -103.00\n",
      "  Episode 25/100: reward = -119.00\n",
      "  Episode 26/100: reward = -101.00\n",
      "  Episode 27/100: reward = -200.00\n",
      "  Episode 28/100: reward = -200.00\n",
      "  Episode 29/100: reward = -200.00\n",
      "  Episode 30/100: reward = -101.00\n",
      "  Episode 31/100: reward = -168.00\n",
      "  Episode 32/100: reward = -88.00\n",
      "  Episode 33/100: reward = -184.00\n",
      "  Episode 34/100: reward = -99.00\n",
      "  Episode 35/100: reward = -91.00\n",
      "  Episode 36/100: reward = -166.00\n",
      "  Episode 37/100: reward = -187.00\n",
      "  Episode 38/100: reward = -89.00\n",
      "  Episode 39/100: reward = -200.00\n",
      "  Episode 40/100: reward = -188.00\n",
      "  Episode 41/100: reward = -200.00\n",
      "  Episode 42/100: reward = -91.00\n",
      "  Episode 43/100: reward = -200.00\n",
      "  Episode 44/100: reward = -176.00\n",
      "  Episode 45/100: reward = -200.00\n",
      "  Episode 46/100: reward = -200.00\n",
      "  Episode 47/100: reward = -168.00\n",
      "  Episode 48/100: reward = -200.00\n",
      "  Episode 49/100: reward = -192.00\n",
      "  Episode 50/100: reward = -200.00\n",
      "  Episode 51/100: reward = -200.00\n",
      "  Episode 52/100: reward = -93.00\n",
      "  Episode 53/100: reward = -200.00\n",
      "  Episode 54/100: reward = -200.00\n",
      "  Episode 55/100: reward = -200.00\n",
      "  Episode 56/100: reward = -93.00\n",
      "  Episode 57/100: reward = -200.00\n",
      "  Episode 58/100: reward = -200.00\n",
      "  Episode 59/100: reward = -107.00\n",
      "  Episode 60/100: reward = -118.00\n",
      "  Episode 61/100: reward = -200.00\n",
      "  Episode 62/100: reward = -94.00\n",
      "  Episode 63/100: reward = -89.00\n",
      "  Episode 64/100: reward = -192.00\n",
      "  Episode 65/100: reward = -89.00\n",
      "  Episode 66/100: reward = -101.00\n",
      "  Episode 67/100: reward = -200.00\n",
      "  Episode 68/100: reward = -165.00\n",
      "  Episode 69/100: reward = -114.00\n",
      "  Episode 70/100: reward = -88.00\n",
      "  Episode 71/100: reward = -200.00\n",
      "  Episode 72/100: reward = -101.00\n",
      "  Episode 73/100: reward = -95.00\n",
      "  Episode 74/100: reward = -181.00\n",
      "  Episode 75/100: reward = -200.00\n",
      "  Episode 76/100: reward = -92.00\n",
      "  Episode 77/100: reward = -90.00\n",
      "  Episode 78/100: reward = -93.00\n",
      "  Episode 79/100: reward = -200.00\n",
      "  Episode 80/100: reward = -197.00\n",
      "  Episode 81/100: reward = -89.00\n",
      "  Episode 82/100: reward = -200.00\n",
      "  Episode 83/100: reward = -200.00\n",
      "  Episode 84/100: reward = -89.00\n",
      "  Episode 85/100: reward = -200.00\n",
      "  Episode 86/100: reward = -200.00\n",
      "  Episode 87/100: reward = -179.00\n",
      "  Episode 88/100: reward = -200.00\n",
      "  Episode 89/100: reward = -187.00\n",
      "  Episode 90/100: reward = -200.00\n",
      "  Episode 91/100: reward = -200.00\n",
      "  Episode 92/100: reward = -188.00\n",
      "  Episode 93/100: reward = -168.00\n",
      "  Episode 94/100: reward = -185.00\n",
      "  Episode 95/100: reward = -90.00\n",
      "  Episode 96/100: reward = -200.00\n",
      "  Episode 97/100: reward = -200.00\n",
      "  Episode 98/100: reward = -160.00\n",
      "  Episode 99/100: reward = -90.00\n",
      "  Episode 100/100: reward = -200.00\n",
      "✅ Model MountainCar_DQN_mem-500.pth: avg reward = -155.41\n",
      "🎥 Recording episode 99 for MountainCar_DQN_mem-500.pth...\n",
      "🎞️ Saved video for MountainCar_DQN_mem-500.pth at ./videos/MountainCar-v0/MountainCar_DQN_mem-500/\n",
      "\n",
      "🎬 Testing and recording model: MountainCar_DQN_mem-50000.pth\n",
      "  Episode 1/100: reward = -95.00\n",
      "  Episode 2/100: reward = -100.00\n",
      "  Episode 3/100: reward = -147.00\n",
      "  Episode 4/100: reward = -144.00\n",
      "  Episode 5/100: reward = -142.00\n",
      "  Episode 6/100: reward = -139.00\n",
      "  Episode 7/100: reward = -143.00\n",
      "  Episode 8/100: reward = -141.00\n",
      "  Episode 9/100: reward = -97.00\n",
      "  Episode 10/100: reward = -138.00\n",
      "  Episode 11/100: reward = -140.00\n",
      "  Episode 12/100: reward = -147.00\n",
      "  Episode 13/100: reward = -103.00\n",
      "  Episode 14/100: reward = -154.00\n",
      "  Episode 15/100: reward = -143.00\n",
      "  Episode 16/100: reward = -138.00\n",
      "  Episode 17/100: reward = -108.00\n",
      "  Episode 18/100: reward = -85.00\n",
      "  Episode 19/100: reward = -148.00\n",
      "  Episode 20/100: reward = -139.00\n",
      "  Episode 21/100: reward = -90.00\n",
      "  Episode 22/100: reward = -139.00\n",
      "  Episode 23/100: reward = -147.00\n",
      "  Episode 24/100: reward = -84.00\n",
      "  Episode 25/100: reward = -87.00\n",
      "  Episode 26/100: reward = -117.00\n",
      "  Episode 27/100: reward = -85.00\n",
      "  Episode 28/100: reward = -149.00\n",
      "  Episode 29/100: reward = -153.00\n",
      "  Episode 30/100: reward = -140.00\n",
      "  Episode 31/100: reward = -92.00\n",
      "  Episode 32/100: reward = -140.00\n",
      "  Episode 33/100: reward = -84.00\n",
      "  Episode 34/100: reward = -85.00\n",
      "  Episode 35/100: reward = -139.00\n",
      "  Episode 36/100: reward = -85.00\n",
      "  Episode 37/100: reward = -138.00\n",
      "  Episode 38/100: reward = -139.00\n",
      "  Episode 39/100: reward = -90.00\n",
      "  Episode 40/100: reward = -91.00\n",
      "  Episode 41/100: reward = -138.00\n",
      "  Episode 42/100: reward = -141.00\n",
      "  Episode 43/100: reward = -140.00\n",
      "  Episode 44/100: reward = -139.00\n",
      "  Episode 45/100: reward = -86.00\n",
      "  Episode 46/100: reward = -141.00\n",
      "  Episode 47/100: reward = -138.00\n",
      "  Episode 48/100: reward = -140.00\n",
      "  Episode 49/100: reward = -140.00\n",
      "  Episode 50/100: reward = -92.00\n",
      "  Episode 51/100: reward = -143.00\n",
      "  Episode 52/100: reward = -86.00\n",
      "  Episode 53/100: reward = -92.00\n",
      "  Episode 54/100: reward = -146.00\n",
      "  Episode 55/100: reward = -89.00\n",
      "  Episode 56/100: reward = -89.00\n",
      "  Episode 57/100: reward = -89.00\n",
      "  Episode 58/100: reward = -143.00\n",
      "  Episode 59/100: reward = -145.00\n",
      "  Episode 60/100: reward = -86.00\n",
      "  Episode 61/100: reward = -87.00\n",
      "  Episode 62/100: reward = -143.00\n",
      "  Episode 63/100: reward = -93.00\n",
      "  Episode 64/100: reward = -111.00\n",
      "  Episode 65/100: reward = -152.00\n",
      "  Episode 66/100: reward = -93.00\n",
      "  Episode 67/100: reward = -138.00\n",
      "  Episode 68/100: reward = -143.00\n",
      "  Episode 69/100: reward = -139.00\n",
      "  Episode 70/100: reward = -140.00\n",
      "  Episode 71/100: reward = -104.00\n",
      "  Episode 72/100: reward = -84.00\n",
      "  Episode 73/100: reward = -85.00\n",
      "  Episode 74/100: reward = -85.00\n",
      "  Episode 75/100: reward = -88.00\n",
      "  Episode 76/100: reward = -138.00\n",
      "  Episode 77/100: reward = -142.00\n",
      "  Episode 78/100: reward = -94.00\n",
      "  Episode 79/100: reward = -140.00\n",
      "  Episode 80/100: reward = -97.00\n",
      "  Episode 81/100: reward = -99.00\n",
      "  Episode 82/100: reward = -141.00\n",
      "  Episode 83/100: reward = -85.00\n",
      "  Episode 84/100: reward = -147.00\n",
      "  Episode 85/100: reward = -142.00\n",
      "  Episode 86/100: reward = -93.00\n",
      "  Episode 87/100: reward = -145.00\n",
      "  Episode 88/100: reward = -87.00\n",
      "  Episode 89/100: reward = -85.00\n",
      "  Episode 90/100: reward = -92.00\n",
      "  Episode 91/100: reward = -98.00\n",
      "  Episode 92/100: reward = -139.00\n",
      "  Episode 93/100: reward = -140.00\n",
      "  Episode 94/100: reward = -141.00\n",
      "  Episode 95/100: reward = -142.00\n",
      "  Episode 96/100: reward = -140.00\n",
      "  Episode 97/100: reward = -106.00\n",
      "  Episode 98/100: reward = -140.00\n",
      "  Episode 99/100: reward = -139.00\n",
      "  Episode 100/100: reward = -89.00\n",
      "✅ Model MountainCar_DQN_mem-50000.pth: avg reward = -119.54\n",
      "🎥 Recording episode 91 for MountainCar_DQN_mem-50000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_mem-50000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for MountainCar_DQN_mem-50000.pth at ./videos/MountainCar-v0/MountainCar_DQN_mem-50000/\n",
      "\n",
      "🏁 All models tested and recorded successfully.\n"
     ]
    }
   ],
   "source": [
    "record_models_in_folder(\n",
    "    models_folder=\"./MountainCar models\",\n",
    "    env_name=\"MountainCar-v0\",\n",
    "    num_discrete_actions=5,\n",
    "    num_tests=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc6c07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_eps-1000.pth\n",
      "  Episode 1/100: reward = -1.05\n",
      "  Episode 2/100: reward = -122.19\n",
      "  Episode 3/100: reward = -116.88\n",
      "  Episode 4/100: reward = -118.88\n",
      "  Episode 5/100: reward = -225.83\n",
      "  Episode 6/100: reward = -1.04\n",
      "  Episode 7/100: reward = -236.11\n",
      "  Episode 8/100: reward = -119.33\n",
      "  Episode 9/100: reward = -233.71\n",
      "  Episode 10/100: reward = -238.68\n",
      "  Episode 11/100: reward = -228.69\n",
      "  Episode 12/100: reward = -116.75\n",
      "  Episode 13/100: reward = -125.85\n",
      "  Episode 14/100: reward = -120.37\n",
      "  Episode 15/100: reward = -119.82\n",
      "  Episode 16/100: reward = -233.81\n",
      "  Episode 17/100: reward = -128.60\n",
      "  Episode 18/100: reward = -128.07\n",
      "  Episode 19/100: reward = -125.38\n",
      "  Episode 20/100: reward = -2.37\n",
      "  Episode 21/100: reward = -115.35\n",
      "  Episode 22/100: reward = -1.21\n",
      "  Episode 23/100: reward = -114.12\n",
      "  Episode 24/100: reward = -120.35\n",
      "  Episode 25/100: reward = -116.36\n",
      "  Episode 26/100: reward = -2.27\n",
      "  Episode 27/100: reward = -311.96\n",
      "  Episode 28/100: reward = -122.27\n",
      "  Episode 29/100: reward = -123.13\n",
      "  Episode 30/100: reward = -232.87\n",
      "  Episode 31/100: reward = -122.02\n",
      "  Episode 32/100: reward = -224.94\n",
      "  Episode 33/100: reward = -114.48\n",
      "  Episode 34/100: reward = -219.19\n",
      "  Episode 35/100: reward = -236.62\n",
      "  Episode 36/100: reward = -121.99\n",
      "  Episode 37/100: reward = -125.38\n",
      "  Episode 38/100: reward = -116.99\n",
      "  Episode 39/100: reward = -123.28\n",
      "  Episode 40/100: reward = -122.64\n",
      "  Episode 41/100: reward = -128.18\n",
      "  Episode 42/100: reward = -119.16\n",
      "  Episode 43/100: reward = -237.41\n",
      "  Episode 44/100: reward = -120.20\n",
      "  Episode 45/100: reward = -114.49\n",
      "  Episode 46/100: reward = -307.90\n",
      "  Episode 47/100: reward = -125.05\n",
      "  Episode 48/100: reward = -121.02\n",
      "  Episode 49/100: reward = -118.82\n",
      "  Episode 50/100: reward = -125.11\n",
      "  Episode 51/100: reward = -1.91\n",
      "  Episode 52/100: reward = -125.92\n",
      "  Episode 53/100: reward = -3.31\n",
      "  Episode 54/100: reward = -121.75\n",
      "  Episode 55/100: reward = -246.82\n",
      "  Episode 56/100: reward = -117.42\n",
      "  Episode 57/100: reward = -117.45\n",
      "  Episode 58/100: reward = -123.61\n",
      "  Episode 59/100: reward = -121.52\n",
      "  Episode 60/100: reward = -1.12\n",
      "  Episode 61/100: reward = -320.84\n",
      "  Episode 62/100: reward = -122.48\n",
      "  Episode 63/100: reward = -126.93\n",
      "  Episode 64/100: reward = -232.61\n",
      "  Episode 65/100: reward = -125.07\n",
      "  Episode 66/100: reward = -346.09\n",
      "  Episode 67/100: reward = -116.45\n",
      "  Episode 68/100: reward = -5.01\n",
      "  Episode 69/100: reward = -3.15\n",
      "  Episode 70/100: reward = -125.61\n",
      "  Episode 71/100: reward = -121.56\n",
      "  Episode 72/100: reward = -119.57\n",
      "  Episode 73/100: reward = -117.67\n",
      "  Episode 74/100: reward = -238.24\n",
      "  Episode 75/100: reward = -122.95\n",
      "  Episode 76/100: reward = -116.51\n",
      "  Episode 77/100: reward = -121.84\n",
      "  Episode 78/100: reward = -129.01\n",
      "  Episode 79/100: reward = -119.81\n",
      "  Episode 80/100: reward = -121.53\n",
      "  Episode 81/100: reward = -125.98\n",
      "  Episode 82/100: reward = -116.83\n",
      "  Episode 83/100: reward = -222.72\n",
      "  Episode 84/100: reward = -1.78\n",
      "  Episode 85/100: reward = -125.22\n",
      "  Episode 86/100: reward = -355.25\n",
      "  Episode 87/100: reward = -119.33\n",
      "  Episode 88/100: reward = -0.84\n",
      "  Episode 89/100: reward = -125.92\n",
      "  Episode 90/100: reward = -358.59\n",
      "  Episode 91/100: reward = -126.90\n",
      "  Episode 92/100: reward = -228.95\n",
      "  Episode 93/100: reward = -1.10\n",
      "  Episode 94/100: reward = -237.72\n",
      "  Episode 95/100: reward = -123.88\n",
      "  Episode 96/100: reward = -234.30\n",
      "  Episode 97/100: reward = -124.89\n",
      "  Episode 98/100: reward = -227.09\n",
      "  Episode 99/100: reward = -117.28\n",
      "  Episode 100/100: reward = -229.18\n",
      "✅ Model Pendulum_DQN_eps-1000.pth: avg reward = -140.86\n",
      "🎥 Recording episode 53 for Pendulum_DQN_eps-1000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_eps-1000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Pendulum_DQN_eps-1000.pth at ./videos/Pendulum-v1/Pendulum_DQN_eps-1000/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_eps-5000.pth\n",
      "  Episode 1/100: reward = -123.74\n",
      "  Episode 2/100: reward = -232.52\n",
      "  Episode 3/100: reward = -118.73\n",
      "  Episode 4/100: reward = -230.59\n",
      "  Episode 5/100: reward = -306.82\n",
      "  Episode 6/100: reward = -124.39\n",
      "  Episode 7/100: reward = -231.81\n",
      "  Episode 8/100: reward = -124.10\n",
      "  Episode 9/100: reward = -126.65\n",
      "  Episode 10/100: reward = -119.60\n",
      "  Episode 11/100: reward = -1.35\n",
      "  Episode 12/100: reward = -231.99\n",
      "  Episode 13/100: reward = -232.13\n",
      "  Episode 14/100: reward = -122.06\n",
      "  Episode 15/100: reward = -120.44\n",
      "  Episode 16/100: reward = -0.91\n",
      "  Episode 17/100: reward = -126.68\n",
      "  Episode 18/100: reward = -123.03\n",
      "  Episode 19/100: reward = -115.99\n",
      "  Episode 20/100: reward = -121.69\n",
      "  Episode 21/100: reward = -123.99\n",
      "  Episode 22/100: reward = -228.88\n",
      "  Episode 23/100: reward = -232.73\n",
      "  Episode 24/100: reward = -114.20\n",
      "  Episode 25/100: reward = -119.53\n",
      "  Episode 26/100: reward = -123.97\n",
      "  Episode 27/100: reward = -123.34\n",
      "  Episode 28/100: reward = -241.59\n",
      "  Episode 29/100: reward = -119.28\n",
      "  Episode 30/100: reward = -119.72\n",
      "  Episode 31/100: reward = -124.36\n",
      "  Episode 32/100: reward = -226.93\n",
      "  Episode 33/100: reward = -122.25\n",
      "  Episode 34/100: reward = -118.10\n",
      "  Episode 35/100: reward = -1.41\n",
      "  Episode 36/100: reward = -119.31\n",
      "  Episode 37/100: reward = -120.85\n",
      "  Episode 38/100: reward = -124.19\n",
      "  Episode 39/100: reward = -118.94\n",
      "  Episode 40/100: reward = -123.50\n",
      "  Episode 41/100: reward = -233.56\n",
      "  Episode 42/100: reward = -244.12\n",
      "  Episode 43/100: reward = -118.25\n",
      "  Episode 44/100: reward = -121.10\n",
      "  Episode 45/100: reward = -127.51\n",
      "  Episode 46/100: reward = -115.16\n",
      "  Episode 47/100: reward = -223.18\n",
      "  Episode 48/100: reward = -326.24\n",
      "  Episode 49/100: reward = -120.55\n",
      "  Episode 50/100: reward = -123.53\n",
      "  Episode 51/100: reward = -125.54\n",
      "  Episode 52/100: reward = -221.34\n",
      "  Episode 53/100: reward = -119.69\n",
      "  Episode 54/100: reward = -125.40\n",
      "  Episode 55/100: reward = -123.04\n",
      "  Episode 56/100: reward = -122.98\n",
      "  Episode 57/100: reward = -223.19\n",
      "  Episode 58/100: reward = -115.50\n",
      "  Episode 59/100: reward = -125.16\n",
      "  Episode 60/100: reward = -227.23\n",
      "  Episode 61/100: reward = -123.51\n",
      "  Episode 62/100: reward = -125.42\n",
      "  Episode 63/100: reward = -121.68\n",
      "  Episode 64/100: reward = -225.35\n",
      "  Episode 65/100: reward = -119.56\n",
      "  Episode 66/100: reward = -318.35\n",
      "  Episode 67/100: reward = -117.57\n",
      "  Episode 68/100: reward = -344.00\n",
      "  Episode 69/100: reward = -3.34\n",
      "  Episode 70/100: reward = -120.87\n",
      "  Episode 71/100: reward = -119.70\n",
      "  Episode 72/100: reward = -120.78\n",
      "  Episode 73/100: reward = -124.92\n",
      "  Episode 74/100: reward = -118.07\n",
      "  Episode 75/100: reward = -125.26\n",
      "  Episode 76/100: reward = -229.20\n",
      "  Episode 77/100: reward = -312.45\n",
      "  Episode 78/100: reward = -119.61\n",
      "  Episode 79/100: reward = -342.14\n",
      "  Episode 80/100: reward = -352.82\n",
      "  Episode 81/100: reward = -5.96\n",
      "  Episode 82/100: reward = -114.22\n",
      "  Episode 83/100: reward = -123.90\n",
      "  Episode 84/100: reward = -224.64\n",
      "  Episode 85/100: reward = -234.13\n",
      "  Episode 86/100: reward = -226.81\n",
      "  Episode 87/100: reward = -116.24\n",
      "  Episode 88/100: reward = -3.37\n",
      "  Episode 89/100: reward = -237.33\n",
      "  Episode 90/100: reward = -114.97\n",
      "  Episode 91/100: reward = -126.45\n",
      "  Episode 92/100: reward = -221.27\n",
      "  Episode 93/100: reward = -0.87\n",
      "  Episode 94/100: reward = -224.35\n",
      "  Episode 95/100: reward = -0.91\n",
      "  Episode 96/100: reward = -117.51\n",
      "  Episode 97/100: reward = -118.86\n",
      "  Episode 98/100: reward = -122.09\n",
      "  Episode 99/100: reward = -235.68\n",
      "  Episode 100/100: reward = -117.50\n",
      "✅ Model Pendulum_DQN_eps-5000.pth: avg reward = -152.36\n",
      "🎥 Recording episode 83 for Pendulum_DQN_eps-5000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_eps-5000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Pendulum_DQN_eps-5000.pth at ./videos/Pendulum-v1/Pendulum_DQN_eps-5000/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_gamma-0.1.pth\n",
      "  Episode 1/100: reward = -1512.11\n",
      "  Episode 2/100: reward = -1511.23\n",
      "  Episode 3/100: reward = -1518.56\n",
      "  Episode 4/100: reward = -1357.19\n",
      "  Episode 5/100: reward = -1485.71\n",
      "  Episode 6/100: reward = -1494.52\n",
      "  Episode 7/100: reward = -1437.19\n",
      "  Episode 8/100: reward = -1467.10\n",
      "  Episode 9/100: reward = -1456.63\n",
      "  Episode 10/100: reward = -1385.27\n",
      "  Episode 11/100: reward = -1514.94\n",
      "  Episode 12/100: reward = -1509.34\n",
      "  Episode 13/100: reward = -1455.04\n",
      "  Episode 14/100: reward = -1369.22\n",
      "  Episode 15/100: reward = -1494.89\n",
      "  Episode 16/100: reward = -1476.26\n",
      "  Episode 17/100: reward = -1469.93\n",
      "  Episode 18/100: reward = -1505.67\n",
      "  Episode 19/100: reward = -1459.79\n",
      "  Episode 20/100: reward = -1505.03\n",
      "  Episode 21/100: reward = -1468.80\n",
      "  Episode 22/100: reward = -1519.26\n",
      "  Episode 23/100: reward = -1504.73\n",
      "  Episode 24/100: reward = -1472.33\n",
      "  Episode 25/100: reward = -1473.56\n",
      "  Episode 26/100: reward = -1513.84\n",
      "  Episode 27/100: reward = -1497.94\n",
      "  Episode 28/100: reward = -1430.30\n",
      "  Episode 29/100: reward = -1407.02\n",
      "  Episode 30/100: reward = -1501.33\n",
      "  Episode 31/100: reward = -1491.69\n",
      "  Episode 32/100: reward = -1497.50\n",
      "  Episode 33/100: reward = -1510.13\n",
      "  Episode 34/100: reward = -1476.64\n",
      "  Episode 35/100: reward = -1288.35\n",
      "  Episode 36/100: reward = -1518.78\n",
      "  Episode 37/100: reward = -1454.50\n",
      "  Episode 38/100: reward = -1520.93\n",
      "  Episode 39/100: reward = -1514.56\n",
      "  Episode 40/100: reward = -1494.34\n",
      "  Episode 41/100: reward = -1494.70\n",
      "  Episode 42/100: reward = -1518.73\n",
      "  Episode 43/100: reward = -1465.50\n",
      "  Episode 44/100: reward = -1438.52\n",
      "  Episode 45/100: reward = -1489.95\n",
      "  Episode 46/100: reward = -1426.63\n",
      "  Episode 47/100: reward = -1509.10\n",
      "  Episode 48/100: reward = -1510.45\n",
      "  Episode 49/100: reward = -1520.23\n",
      "  Episode 50/100: reward = -1513.41\n",
      "  Episode 51/100: reward = -1487.87\n",
      "  Episode 52/100: reward = -1382.74\n",
      "  Episode 53/100: reward = -1544.02\n",
      "  Episode 54/100: reward = -1446.59\n",
      "  Episode 55/100: reward = -1472.16\n",
      "  Episode 56/100: reward = -1503.02\n",
      "  Episode 57/100: reward = -1550.88\n",
      "  Episode 58/100: reward = -1473.60\n",
      "  Episode 59/100: reward = -1506.75\n",
      "  Episode 60/100: reward = -1416.52\n",
      "  Episode 61/100: reward = -1424.47\n",
      "  Episode 62/100: reward = -1431.96\n",
      "  Episode 63/100: reward = -1546.06\n",
      "  Episode 64/100: reward = -1522.94\n",
      "  Episode 65/100: reward = -1510.94\n",
      "  Episode 66/100: reward = -1497.33\n",
      "  Episode 67/100: reward = -1491.85\n",
      "  Episode 68/100: reward = -1463.64\n",
      "  Episode 69/100: reward = -1477.54\n",
      "  Episode 70/100: reward = -1509.03\n",
      "  Episode 71/100: reward = -1487.50\n",
      "  Episode 72/100: reward = -1499.31\n",
      "  Episode 73/100: reward = -1491.70\n",
      "  Episode 74/100: reward = -1451.11\n",
      "  Episode 75/100: reward = -1438.19\n",
      "  Episode 76/100: reward = -1487.97\n",
      "  Episode 77/100: reward = -1508.81\n",
      "  Episode 78/100: reward = -1513.08\n",
      "  Episode 79/100: reward = -1511.52\n",
      "  Episode 80/100: reward = -1495.74\n",
      "  Episode 81/100: reward = -1487.50\n",
      "  Episode 82/100: reward = -1447.24\n",
      "  Episode 83/100: reward = -1524.02\n",
      "  Episode 84/100: reward = -1451.45\n",
      "  Episode 85/100: reward = -1519.47\n",
      "  Episode 86/100: reward = -1378.18\n",
      "  Episode 87/100: reward = -1504.41\n",
      "  Episode 88/100: reward = -1507.40\n",
      "  Episode 89/100: reward = -1448.20\n",
      "  Episode 90/100: reward = -1498.14\n",
      "  Episode 91/100: reward = -1547.36\n",
      "  Episode 92/100: reward = -1479.10\n",
      "  Episode 93/100: reward = -1502.58\n",
      "  Episode 94/100: reward = -1433.06\n",
      "  Episode 95/100: reward = -1509.83\n",
      "  Episode 96/100: reward = -1467.43\n",
      "  Episode 97/100: reward = -1384.30\n",
      "  Episode 98/100: reward = -1323.08\n",
      "  Episode 99/100: reward = -1509.26\n",
      "  Episode 100/100: reward = -1445.67\n",
      "✅ Model Pendulum_DQN_gamma-0.1.pth: avg reward = -1477.42\n",
      "🎥 Recording episode 64 for Pendulum_DQN_gamma-0.1.pth...\n",
      "🎞️ Saved video for Pendulum_DQN_gamma-0.1.pth at ./videos/Pendulum-v1/Pendulum_DQN_gamma-0.1/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_gamma-0.5.pth\n",
      "  Episode 1/100: reward = -1305.87\n",
      "  Episode 2/100: reward = -1244.78\n",
      "  Episode 3/100: reward = -714.64\n",
      "  Episode 4/100: reward = -1318.16\n",
      "  Episode 5/100: reward = -1300.46\n",
      "  Episode 6/100: reward = -1305.89\n",
      "  Episode 7/100: reward = -1500.18\n",
      "  Episode 8/100: reward = -1179.75\n",
      "  Episode 9/100: reward = -1281.36\n",
      "  Episode 10/100: reward = -3.77\n",
      "  Episode 11/100: reward = -1386.28\n",
      "  Episode 12/100: reward = -1301.85\n",
      "  Episode 13/100: reward = -1337.79\n",
      "  Episode 14/100: reward = -1518.71\n",
      "  Episode 15/100: reward = -1198.16\n",
      "  Episode 16/100: reward = -1283.65\n",
      "  Episode 17/100: reward = -1.24\n",
      "  Episode 18/100: reward = -1308.32\n",
      "  Episode 19/100: reward = -1519.96\n",
      "  Episode 20/100: reward = -1499.65\n",
      "  Episode 21/100: reward = -1313.40\n",
      "  Episode 22/100: reward = -1312.98\n",
      "  Episode 23/100: reward = -1324.80\n",
      "  Episode 24/100: reward = -1276.56\n",
      "  Episode 25/100: reward = -1496.05\n",
      "  Episode 26/100: reward = -1235.73\n",
      "  Episode 27/100: reward = -1224.94\n",
      "  Episode 28/100: reward = -1497.90\n",
      "  Episode 29/100: reward = -1503.29\n",
      "  Episode 30/100: reward = -2.70\n",
      "  Episode 31/100: reward = -1514.32\n",
      "  Episode 32/100: reward = -1332.67\n",
      "  Episode 33/100: reward = -2.39\n",
      "  Episode 34/100: reward = -1.48\n",
      "  Episode 35/100: reward = -1382.77\n",
      "  Episode 36/100: reward = -1297.59\n",
      "  Episode 37/100: reward = -1494.92\n",
      "  Episode 38/100: reward = -1227.48\n",
      "  Episode 39/100: reward = -1305.02\n",
      "  Episode 40/100: reward = -1278.56\n",
      "  Episode 41/100: reward = -1205.03\n",
      "  Episode 42/100: reward = -1221.74\n",
      "  Episode 43/100: reward = -1376.21\n",
      "  Episode 44/100: reward = -1309.09\n",
      "  Episode 45/100: reward = -2.04\n",
      "  Episode 46/100: reward = -1234.93\n",
      "  Episode 47/100: reward = -1305.90\n",
      "  Episode 48/100: reward = -1292.76\n",
      "  Episode 49/100: reward = -1.13\n",
      "  Episode 50/100: reward = -1293.50\n",
      "  Episode 51/100: reward = -1174.41\n",
      "  Episode 52/100: reward = -1320.16\n",
      "  Episode 53/100: reward = -1362.34\n",
      "  Episode 54/100: reward = -1310.19\n",
      "  Episode 55/100: reward = -1326.91\n",
      "  Episode 56/100: reward = -1170.23\n",
      "  Episode 57/100: reward = -1310.43\n",
      "  Episode 58/100: reward = -1504.36\n",
      "  Episode 59/100: reward = -1336.53\n",
      "  Episode 60/100: reward = -1332.73\n",
      "  Episode 61/100: reward = -1493.39\n",
      "  Episode 62/100: reward = -1493.70\n",
      "  Episode 63/100: reward = -1339.58\n",
      "  Episode 64/100: reward = -1287.85\n",
      "  Episode 65/100: reward = -3.36\n",
      "  Episode 66/100: reward = -1291.92\n",
      "  Episode 67/100: reward = -1.60\n",
      "  Episode 68/100: reward = -1344.53\n",
      "  Episode 69/100: reward = -1319.40\n",
      "  Episode 70/100: reward = -1289.18\n",
      "  Episode 71/100: reward = -1354.78\n",
      "  Episode 72/100: reward = -1272.96\n",
      "  Episode 73/100: reward = -1283.54\n",
      "  Episode 74/100: reward = -1502.57\n",
      "  Episode 75/100: reward = -1267.01\n",
      "  Episode 76/100: reward = -1277.12\n",
      "  Episode 77/100: reward = -1282.07\n",
      "  Episode 78/100: reward = -1.90\n",
      "  Episode 79/100: reward = -1280.60\n",
      "  Episode 80/100: reward = -1330.50\n",
      "  Episode 81/100: reward = -1495.82\n",
      "  Episode 82/100: reward = -1295.03\n",
      "  Episode 83/100: reward = -1501.34\n",
      "  Episode 84/100: reward = -1495.61\n",
      "  Episode 85/100: reward = -1326.97\n",
      "  Episode 86/100: reward = -1313.53\n",
      "  Episode 87/100: reward = -1496.63\n",
      "  Episode 88/100: reward = -1307.77\n",
      "  Episode 89/100: reward = -1499.39\n",
      "  Episode 90/100: reward = -1326.42\n",
      "  Episode 91/100: reward = -1254.95\n",
      "  Episode 92/100: reward = -1299.66\n",
      "  Episode 93/100: reward = -1495.05\n",
      "  Episode 94/100: reward = -1501.21\n",
      "  Episode 95/100: reward = -1266.67\n",
      "  Episode 96/100: reward = -2.03\n",
      "  Episode 97/100: reward = -1.17\n",
      "  Episode 98/100: reward = -1.20\n",
      "  Episode 99/100: reward = -1320.72\n",
      "  Episode 100/100: reward = -1493.63\n",
      "✅ Model Pendulum_DQN_gamma-0.5.pth: avg reward = -1163.39\n",
      "🎥 Recording episode 72 for Pendulum_DQN_gamma-0.5.pth...\n",
      "🎞️ Saved video for Pendulum_DQN_gamma-0.5.pth at ./videos/Pendulum-v1/Pendulum_DQN_gamma-0.5/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_gamma-0.9.pth\n",
      "  Episode 1/100: reward = -121.74\n",
      "  Episode 2/100: reward = -261.18\n",
      "  Episode 3/100: reward = -127.94\n",
      "  Episode 4/100: reward = -132.45\n",
      "  Episode 5/100: reward = -128.15\n",
      "  Episode 6/100: reward = -238.69\n",
      "  Episode 7/100: reward = -133.98\n",
      "  Episode 8/100: reward = -125.64\n",
      "  Episode 9/100: reward = -257.47\n",
      "  Episode 10/100: reward = -131.71\n",
      "  Episode 11/100: reward = -121.53\n",
      "  Episode 12/100: reward = -370.91\n",
      "  Episode 13/100: reward = -120.16\n",
      "  Episode 14/100: reward = -264.73\n",
      "  Episode 15/100: reward = -1.63\n",
      "  Episode 16/100: reward = -126.48\n",
      "  Episode 17/100: reward = -132.17\n",
      "  Episode 18/100: reward = -124.18\n",
      "  Episode 19/100: reward = -358.94\n",
      "  Episode 20/100: reward = -129.10\n",
      "  Episode 21/100: reward = -126.51\n",
      "  Episode 22/100: reward = -348.06\n",
      "  Episode 23/100: reward = -127.11\n",
      "  Episode 24/100: reward = -0.85\n",
      "  Episode 25/100: reward = -268.83\n",
      "  Episode 26/100: reward = -128.86\n",
      "  Episode 27/100: reward = -129.61\n",
      "  Episode 28/100: reward = -364.88\n",
      "  Episode 29/100: reward = -131.98\n",
      "  Episode 30/100: reward = -270.33\n",
      "  Episode 31/100: reward = -131.60\n",
      "  Episode 32/100: reward = -140.75\n",
      "  Episode 33/100: reward = -129.65\n",
      "  Episode 34/100: reward = -261.98\n",
      "  Episode 35/100: reward = -123.01\n",
      "  Episode 36/100: reward = -131.74\n",
      "  Episode 37/100: reward = -135.79\n",
      "  Episode 38/100: reward = -133.18\n",
      "  Episode 39/100: reward = -2.32\n",
      "  Episode 40/100: reward = -127.57\n",
      "  Episode 41/100: reward = -128.05\n",
      "  Episode 42/100: reward = -129.38\n",
      "  Episode 43/100: reward = -130.09\n",
      "  Episode 44/100: reward = -131.30\n",
      "  Episode 45/100: reward = -242.36\n",
      "  Episode 46/100: reward = -140.47\n",
      "  Episode 47/100: reward = -127.68\n",
      "  Episode 48/100: reward = -131.01\n",
      "  Episode 49/100: reward = -2.03\n",
      "  Episode 50/100: reward = -129.23\n",
      "  Episode 51/100: reward = -129.63\n",
      "  Episode 52/100: reward = -123.84\n",
      "  Episode 53/100: reward = -135.39\n",
      "  Episode 54/100: reward = -237.95\n",
      "  Episode 55/100: reward = -126.49\n",
      "  Episode 56/100: reward = -132.73\n",
      "  Episode 57/100: reward = -244.16\n",
      "  Episode 58/100: reward = -0.84\n",
      "  Episode 59/100: reward = -131.16\n",
      "  Episode 60/100: reward = -128.24\n",
      "  Episode 61/100: reward = -133.69\n",
      "  Episode 62/100: reward = -136.26\n",
      "  Episode 63/100: reward = -128.01\n",
      "  Episode 64/100: reward = -132.17\n",
      "  Episode 65/100: reward = -127.44\n",
      "  Episode 66/100: reward = -131.50\n",
      "  Episode 67/100: reward = -126.00\n",
      "  Episode 68/100: reward = -1.78\n",
      "  Episode 69/100: reward = -260.96\n",
      "  Episode 70/100: reward = -289.30\n",
      "  Episode 71/100: reward = -1.25\n",
      "  Episode 72/100: reward = -305.65\n",
      "  Episode 73/100: reward = -129.88\n",
      "  Episode 74/100: reward = -133.18\n",
      "  Episode 75/100: reward = -129.65\n",
      "  Episode 76/100: reward = -240.10\n",
      "  Episode 77/100: reward = -130.79\n",
      "  Episode 78/100: reward = -251.02\n",
      "  Episode 79/100: reward = -127.46\n",
      "  Episode 80/100: reward = -129.32\n",
      "  Episode 81/100: reward = -1.24\n",
      "  Episode 82/100: reward = -131.11\n",
      "  Episode 83/100: reward = -130.68\n",
      "  Episode 84/100: reward = -125.14\n",
      "  Episode 85/100: reward = -133.68\n",
      "  Episode 86/100: reward = -131.83\n",
      "  Episode 87/100: reward = -114.49\n",
      "  Episode 88/100: reward = -253.05\n",
      "  Episode 89/100: reward = -370.73\n",
      "  Episode 90/100: reward = -2.37\n",
      "  Episode 91/100: reward = -1.30\n",
      "  Episode 92/100: reward = -132.29\n",
      "  Episode 93/100: reward = -131.12\n",
      "  Episode 94/100: reward = -132.69\n",
      "  Episode 95/100: reward = -134.02\n",
      "  Episode 96/100: reward = -253.21\n",
      "  Episode 97/100: reward = -129.73\n",
      "  Episode 98/100: reward = -253.89\n",
      "  Episode 99/100: reward = -255.44\n",
      "  Episode 100/100: reward = -131.62\n",
      "✅ Model Pendulum_DQN_gamma-0.9.pth: avg reward = -153.00\n",
      "🎥 Recording episode 77 for Pendulum_DQN_gamma-0.9.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_gamma-0.9 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Pendulum_DQN_gamma-0.9.pth at ./videos/Pendulum-v1/Pendulum_DQN_gamma-0.9/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_gamma-0.99.pth\n",
      "  Episode 1/100: reward = -232.78\n",
      "  Episode 2/100: reward = -346.65\n",
      "  Episode 3/100: reward = -114.82\n",
      "  Episode 4/100: reward = -2.58\n",
      "  Episode 5/100: reward = -116.63\n",
      "  Episode 6/100: reward = -1.53\n",
      "  Episode 7/100: reward = -1.32\n",
      "  Episode 8/100: reward = -119.37\n",
      "  Episode 9/100: reward = -118.95\n",
      "  Episode 10/100: reward = -238.30\n",
      "  Episode 11/100: reward = -121.33\n",
      "  Episode 12/100: reward = -125.66\n",
      "  Episode 13/100: reward = -221.01\n",
      "  Episode 14/100: reward = -119.29\n",
      "  Episode 15/100: reward = -240.15\n",
      "  Episode 16/100: reward = -117.13\n",
      "  Episode 17/100: reward = -121.08\n",
      "  Episode 18/100: reward = -127.27\n",
      "  Episode 19/100: reward = -228.09\n",
      "  Episode 20/100: reward = -125.20\n",
      "  Episode 21/100: reward = -216.13\n",
      "  Episode 22/100: reward = -227.59\n",
      "  Episode 23/100: reward = -126.38\n",
      "  Episode 24/100: reward = -233.74\n",
      "  Episode 25/100: reward = -114.99\n",
      "  Episode 26/100: reward = -235.02\n",
      "  Episode 27/100: reward = -1.68\n",
      "  Episode 28/100: reward = -1.95\n",
      "  Episode 29/100: reward = -117.33\n",
      "  Episode 30/100: reward = -301.00\n",
      "  Episode 31/100: reward = -231.54\n",
      "  Episode 32/100: reward = -222.15\n",
      "  Episode 33/100: reward = -2.64\n",
      "  Episode 34/100: reward = -116.42\n",
      "  Episode 35/100: reward = -115.21\n",
      "  Episode 36/100: reward = -227.86\n",
      "  Episode 37/100: reward = -114.93\n",
      "  Episode 38/100: reward = -343.98\n",
      "  Episode 39/100: reward = -119.62\n",
      "  Episode 40/100: reward = -1.99\n",
      "  Episode 41/100: reward = -120.51\n",
      "  Episode 42/100: reward = -115.00\n",
      "  Episode 43/100: reward = -2.85\n",
      "  Episode 44/100: reward = -242.71\n",
      "  Episode 45/100: reward = -128.25\n",
      "  Episode 46/100: reward = -118.26\n",
      "  Episode 47/100: reward = -1.24\n",
      "  Episode 48/100: reward = -119.50\n",
      "  Episode 49/100: reward = -120.20\n",
      "  Episode 50/100: reward = -120.45\n",
      "  Episode 51/100: reward = -113.83\n",
      "  Episode 52/100: reward = -228.24\n",
      "  Episode 53/100: reward = -368.23\n",
      "  Episode 54/100: reward = -114.20\n",
      "  Episode 55/100: reward = -336.74\n",
      "  Episode 56/100: reward = -119.30\n",
      "  Episode 57/100: reward = -1.42\n",
      "  Episode 58/100: reward = -227.31\n",
      "  Episode 59/100: reward = -1.50\n",
      "  Episode 60/100: reward = -2.30\n",
      "  Episode 61/100: reward = -234.63\n",
      "  Episode 62/100: reward = -233.63\n",
      "  Episode 63/100: reward = -118.35\n",
      "  Episode 64/100: reward = -124.56\n",
      "  Episode 65/100: reward = -117.93\n",
      "  Episode 66/100: reward = -230.13\n",
      "  Episode 67/100: reward = -362.83\n",
      "  Episode 68/100: reward = -2.78\n",
      "  Episode 69/100: reward = -222.44\n",
      "  Episode 70/100: reward = -222.06\n",
      "  Episode 71/100: reward = -345.13\n",
      "  Episode 72/100: reward = -122.14\n",
      "  Episode 73/100: reward = -125.54\n",
      "  Episode 74/100: reward = -237.87\n",
      "  Episode 75/100: reward = -115.75\n",
      "  Episode 76/100: reward = -232.21\n",
      "  Episode 77/100: reward = -226.34\n",
      "  Episode 78/100: reward = -121.51\n",
      "  Episode 79/100: reward = -118.96\n",
      "  Episode 80/100: reward = -119.79\n",
      "  Episode 81/100: reward = -117.28\n",
      "  Episode 82/100: reward = -117.98\n",
      "  Episode 83/100: reward = -231.52\n",
      "  Episode 84/100: reward = -117.06\n",
      "  Episode 85/100: reward = -120.09\n",
      "  Episode 86/100: reward = -117.90\n",
      "  Episode 87/100: reward = -1.05\n",
      "  Episode 88/100: reward = -235.78\n",
      "  Episode 89/100: reward = -119.95\n",
      "  Episode 90/100: reward = -124.35\n",
      "  Episode 91/100: reward = -231.30\n",
      "  Episode 92/100: reward = -118.04\n",
      "  Episode 93/100: reward = -326.12\n",
      "  Episode 94/100: reward = -121.18\n",
      "  Episode 95/100: reward = -119.62\n",
      "  Episode 96/100: reward = -230.59\n",
      "  Episode 97/100: reward = -125.82\n",
      "  Episode 98/100: reward = -117.96\n",
      "  Episode 99/100: reward = -127.44\n",
      "  Episode 100/100: reward = -121.02\n",
      "✅ Model Pendulum_DQN_gamma-0.99.pth: avg reward = -150.90\n",
      "🎥 Recording episode 75 for Pendulum_DQN_gamma-0.99.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_gamma-0.99 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Pendulum_DQN_gamma-0.99.pth at ./videos/Pendulum-v1/Pendulum_DQN_gamma-0.99/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_lr-0.0001.pth\n",
      "  Episode 1/100: reward = -2.54\n",
      "  Episode 2/100: reward = -1.75\n",
      "  Episode 3/100: reward = -127.72\n",
      "  Episode 4/100: reward = -242.64\n",
      "  Episode 5/100: reward = -115.59\n",
      "  Episode 6/100: reward = -118.30\n",
      "  Episode 7/100: reward = -4.94\n",
      "  Episode 8/100: reward = -1.99\n",
      "  Episode 9/100: reward = -127.03\n",
      "  Episode 10/100: reward = -1.57\n",
      "  Episode 11/100: reward = -119.44\n",
      "  Episode 12/100: reward = -240.54\n",
      "  Episode 13/100: reward = -122.53\n",
      "  Episode 14/100: reward = -120.78\n",
      "  Episode 15/100: reward = -114.86\n",
      "  Episode 16/100: reward = -1.66\n",
      "  Episode 17/100: reward = -116.43\n",
      "  Episode 18/100: reward = -119.03\n",
      "  Episode 19/100: reward = -124.69\n",
      "  Episode 20/100: reward = -250.33\n",
      "  Episode 21/100: reward = -122.14\n",
      "  Episode 22/100: reward = -126.14\n",
      "  Episode 23/100: reward = -235.05\n",
      "  Episode 24/100: reward = -224.21\n",
      "  Episode 25/100: reward = -295.54\n",
      "  Episode 26/100: reward = -1.52\n",
      "  Episode 27/100: reward = -121.79\n",
      "  Episode 28/100: reward = -263.38\n",
      "  Episode 29/100: reward = -125.93\n",
      "  Episode 30/100: reward = -1.45\n",
      "  Episode 31/100: reward = -118.15\n",
      "  Episode 32/100: reward = -115.87\n",
      "  Episode 33/100: reward = -318.14\n",
      "  Episode 34/100: reward = -1.52\n",
      "  Episode 35/100: reward = -120.80\n",
      "  Episode 36/100: reward = -326.39\n",
      "  Episode 37/100: reward = -255.10\n",
      "  Episode 38/100: reward = -354.61\n",
      "  Episode 39/100: reward = -118.26\n",
      "  Episode 40/100: reward = -270.99\n",
      "  Episode 41/100: reward = -233.26\n",
      "  Episode 42/100: reward = -240.06\n",
      "  Episode 43/100: reward = -121.42\n",
      "  Episode 44/100: reward = -121.27\n",
      "  Episode 45/100: reward = -123.95\n",
      "  Episode 46/100: reward = -120.46\n",
      "  Episode 47/100: reward = -125.55\n",
      "  Episode 48/100: reward = -125.76\n",
      "  Episode 49/100: reward = -115.60\n",
      "  Episode 50/100: reward = -123.91\n",
      "  Episode 51/100: reward = -236.66\n",
      "  Episode 52/100: reward = -126.22\n",
      "  Episode 53/100: reward = -117.27\n",
      "  Episode 54/100: reward = -128.10\n",
      "  Episode 55/100: reward = -1.76\n",
      "  Episode 56/100: reward = -242.70\n",
      "  Episode 57/100: reward = -126.90\n",
      "  Episode 58/100: reward = -121.89\n",
      "  Episode 59/100: reward = -125.67\n",
      "  Episode 60/100: reward = -117.94\n",
      "  Episode 61/100: reward = -231.41\n",
      "  Episode 62/100: reward = -127.85\n",
      "  Episode 63/100: reward = -127.49\n",
      "  Episode 64/100: reward = -119.98\n",
      "  Episode 65/100: reward = -224.27\n",
      "  Episode 66/100: reward = -230.54\n",
      "  Episode 67/100: reward = -120.39\n",
      "  Episode 68/100: reward = -127.76\n",
      "  Episode 69/100: reward = -125.57\n",
      "  Episode 70/100: reward = -244.15\n",
      "  Episode 71/100: reward = -128.89\n",
      "  Episode 72/100: reward = -2.80\n",
      "  Episode 73/100: reward = -121.69\n",
      "  Episode 74/100: reward = -243.75\n",
      "  Episode 75/100: reward = -121.86\n",
      "  Episode 76/100: reward = -126.61\n",
      "  Episode 77/100: reward = -115.80\n",
      "  Episode 78/100: reward = -5.06\n",
      "  Episode 79/100: reward = -3.44\n",
      "  Episode 80/100: reward = -234.64\n",
      "  Episode 81/100: reward = -1.37\n",
      "  Episode 82/100: reward = -262.79\n",
      "  Episode 83/100: reward = -124.58\n",
      "  Episode 84/100: reward = -118.04\n",
      "  Episode 85/100: reward = -120.84\n",
      "  Episode 86/100: reward = -121.04\n",
      "  Episode 87/100: reward = -232.16\n",
      "  Episode 88/100: reward = -2.13\n",
      "  Episode 89/100: reward = -122.87\n",
      "  Episode 90/100: reward = -248.48\n",
      "  Episode 91/100: reward = -220.73\n",
      "  Episode 92/100: reward = -332.69\n",
      "  Episode 93/100: reward = -128.55\n",
      "  Episode 94/100: reward = -242.21\n",
      "  Episode 95/100: reward = -233.39\n",
      "  Episode 96/100: reward = -225.60\n",
      "  Episode 97/100: reward = -122.58\n",
      "  Episode 98/100: reward = -121.41\n",
      "  Episode 99/100: reward = -124.57\n",
      "  Episode 100/100: reward = -118.66\n",
      "✅ Model Pendulum_DQN_lr-0.0001.pth: avg reward = -143.96\n",
      "🎥 Recording episode 33 for Pendulum_DQN_lr-0.0001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_lr-0.0001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Pendulum_DQN_lr-0.0001.pth at ./videos/Pendulum-v1/Pendulum_DQN_lr-0.0001/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_lr-0.0003.pth\n",
      "  Episode 1/100: reward = -128.40\n",
      "  Episode 2/100: reward = -3.25\n",
      "  Episode 3/100: reward = -4.42\n",
      "  Episode 4/100: reward = -234.54\n",
      "  Episode 5/100: reward = -120.64\n",
      "  Episode 6/100: reward = -240.80\n",
      "  Episode 7/100: reward = -234.65\n",
      "  Episode 8/100: reward = -3.18\n",
      "  Episode 9/100: reward = -3.21\n",
      "  Episode 10/100: reward = -282.11\n",
      "  Episode 11/100: reward = -128.23\n",
      "  Episode 12/100: reward = -238.52\n",
      "  Episode 13/100: reward = -119.47\n",
      "  Episode 14/100: reward = -125.76\n",
      "  Episode 15/100: reward = -3.91\n",
      "  Episode 16/100: reward = -243.46\n",
      "  Episode 17/100: reward = -123.18\n",
      "  Episode 18/100: reward = -234.58\n",
      "  Episode 19/100: reward = -121.34\n",
      "  Episode 20/100: reward = -125.99\n",
      "  Episode 21/100: reward = -119.12\n",
      "  Episode 22/100: reward = -126.71\n",
      "  Episode 23/100: reward = -231.62\n",
      "  Episode 24/100: reward = -126.71\n",
      "  Episode 25/100: reward = -124.02\n",
      "  Episode 26/100: reward = -118.60\n",
      "  Episode 27/100: reward = -127.71\n",
      "  Episode 28/100: reward = -128.58\n",
      "  Episode 29/100: reward = -234.70\n",
      "  Episode 30/100: reward = -120.18\n",
      "  Episode 31/100: reward = -243.73\n",
      "  Episode 32/100: reward = -121.58\n",
      "  Episode 33/100: reward = -130.31\n",
      "  Episode 34/100: reward = -242.62\n",
      "  Episode 35/100: reward = -229.79\n",
      "  Episode 36/100: reward = -122.39\n",
      "  Episode 37/100: reward = -128.05\n",
      "  Episode 38/100: reward = -353.83\n",
      "  Episode 39/100: reward = -121.84\n",
      "  Episode 40/100: reward = -124.02\n",
      "  Episode 41/100: reward = -129.07\n",
      "  Episode 42/100: reward = -120.73\n",
      "  Episode 43/100: reward = -122.39\n",
      "  Episode 44/100: reward = -123.79\n",
      "  Episode 45/100: reward = -233.88\n",
      "  Episode 46/100: reward = -123.24\n",
      "  Episode 47/100: reward = -121.87\n",
      "  Episode 48/100: reward = -122.08\n",
      "  Episode 49/100: reward = -119.08\n",
      "  Episode 50/100: reward = -123.39\n",
      "  Episode 51/100: reward = -239.13\n",
      "  Episode 52/100: reward = -120.90\n",
      "  Episode 53/100: reward = -341.62\n",
      "  Episode 54/100: reward = -126.30\n",
      "  Episode 55/100: reward = -126.90\n",
      "  Episode 56/100: reward = -128.19\n",
      "  Episode 57/100: reward = -124.57\n",
      "  Episode 58/100: reward = -3.46\n",
      "  Episode 59/100: reward = -123.49\n",
      "  Episode 60/100: reward = -116.07\n",
      "  Episode 61/100: reward = -128.44\n",
      "  Episode 62/100: reward = -122.11\n",
      "  Episode 63/100: reward = -124.94\n",
      "  Episode 64/100: reward = -123.31\n",
      "  Episode 65/100: reward = -119.44\n",
      "  Episode 66/100: reward = -128.66\n",
      "  Episode 67/100: reward = -130.79\n",
      "  Episode 68/100: reward = -126.25\n",
      "  Episode 69/100: reward = -119.87\n",
      "  Episode 70/100: reward = -116.30\n",
      "  Episode 71/100: reward = -121.76\n",
      "  Episode 72/100: reward = -120.40\n",
      "  Episode 73/100: reward = -126.96\n",
      "  Episode 74/100: reward = -126.79\n",
      "  Episode 75/100: reward = -123.20\n",
      "  Episode 76/100: reward = -119.70\n",
      "  Episode 77/100: reward = -126.08\n",
      "  Episode 78/100: reward = -121.42\n",
      "  Episode 79/100: reward = -122.75\n",
      "  Episode 80/100: reward = -129.91\n",
      "  Episode 81/100: reward = -4.30\n",
      "  Episode 82/100: reward = -230.10\n",
      "  Episode 83/100: reward = -122.00\n",
      "  Episode 84/100: reward = -3.28\n",
      "  Episode 85/100: reward = -6.00\n",
      "  Episode 86/100: reward = -129.32\n",
      "  Episode 87/100: reward = -127.56\n",
      "  Episode 88/100: reward = -122.81\n",
      "  Episode 89/100: reward = -119.07\n",
      "  Episode 90/100: reward = -237.88\n",
      "  Episode 91/100: reward = -241.80\n",
      "  Episode 92/100: reward = -331.42\n",
      "  Episode 93/100: reward = -289.84\n",
      "  Episode 94/100: reward = -226.11\n",
      "  Episode 95/100: reward = -124.20\n",
      "  Episode 96/100: reward = -120.83\n",
      "  Episode 97/100: reward = -124.55\n",
      "  Episode 98/100: reward = -122.65\n",
      "  Episode 99/100: reward = -120.60\n",
      "  Episode 100/100: reward = -127.53\n",
      "✅ Model Pendulum_DQN_lr-0.0003.pth: avg reward = -141.97\n",
      "🎥 Recording episode 60 for Pendulum_DQN_lr-0.0003.pth...\n",
      "🎞️ Saved video for Pendulum_DQN_lr-0.0003.pth at ./videos/Pendulum-v1/Pendulum_DQN_lr-0.0003/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_lr-0.001.pth\n",
      "  Episode 1/100: reward = -123.11\n",
      "  Episode 2/100: reward = -3.58\n",
      "  Episode 3/100: reward = -123.73\n",
      "  Episode 4/100: reward = -4.45\n",
      "  Episode 5/100: reward = -5.14\n",
      "  Episode 6/100: reward = -4.96\n",
      "  Episode 7/100: reward = -119.54\n",
      "  Episode 8/100: reward = -126.66\n",
      "  Episode 9/100: reward = -116.41\n",
      "  Episode 10/100: reward = -127.64\n",
      "  Episode 11/100: reward = -121.09\n",
      "  Episode 12/100: reward = -233.84\n",
      "  Episode 13/100: reward = -128.40\n",
      "  Episode 14/100: reward = -123.48\n",
      "  Episode 15/100: reward = -126.92\n",
      "  Episode 16/100: reward = -4.42\n",
      "  Episode 17/100: reward = -122.05\n",
      "  Episode 18/100: reward = -225.73\n",
      "  Episode 19/100: reward = -225.70\n",
      "  Episode 20/100: reward = -123.47\n",
      "  Episode 21/100: reward = -4.14\n",
      "  Episode 22/100: reward = -119.56\n",
      "  Episode 23/100: reward = -127.46\n",
      "  Episode 24/100: reward = -230.12\n",
      "  Episode 25/100: reward = -3.57\n",
      "  Episode 26/100: reward = -245.19\n",
      "  Episode 27/100: reward = -250.03\n",
      "  Episode 28/100: reward = -236.51\n",
      "  Episode 29/100: reward = -236.69\n",
      "  Episode 30/100: reward = -3.88\n",
      "  Episode 31/100: reward = -116.83\n",
      "  Episode 32/100: reward = -128.62\n",
      "  Episode 33/100: reward = -122.05\n",
      "  Episode 34/100: reward = -124.55\n",
      "  Episode 35/100: reward = -235.30\n",
      "  Episode 36/100: reward = -122.15\n",
      "  Episode 37/100: reward = -7.52\n",
      "  Episode 38/100: reward = -4.36\n",
      "  Episode 39/100: reward = -3.63\n",
      "  Episode 40/100: reward = -239.31\n",
      "  Episode 41/100: reward = -125.28\n",
      "  Episode 42/100: reward = -120.32\n",
      "  Episode 43/100: reward = -116.37\n",
      "  Episode 44/100: reward = -123.65\n",
      "  Episode 45/100: reward = -123.85\n",
      "  Episode 46/100: reward = -236.62\n",
      "  Episode 47/100: reward = -122.80\n",
      "  Episode 48/100: reward = -245.93\n",
      "  Episode 49/100: reward = -125.21\n",
      "  Episode 50/100: reward = -125.22\n",
      "  Episode 51/100: reward = -126.17\n",
      "  Episode 52/100: reward = -289.12\n",
      "  Episode 53/100: reward = -124.98\n",
      "  Episode 54/100: reward = -121.84\n",
      "  Episode 55/100: reward = -128.06\n",
      "  Episode 56/100: reward = -236.52\n",
      "  Episode 57/100: reward = -3.62\n",
      "  Episode 58/100: reward = -121.80\n",
      "  Episode 59/100: reward = -117.51\n",
      "  Episode 60/100: reward = -250.26\n",
      "  Episode 61/100: reward = -122.27\n",
      "  Episode 62/100: reward = -242.83\n",
      "  Episode 63/100: reward = -117.79\n",
      "  Episode 64/100: reward = -3.78\n",
      "  Episode 65/100: reward = -120.88\n",
      "  Episode 66/100: reward = -116.48\n",
      "  Episode 67/100: reward = -128.08\n",
      "  Episode 68/100: reward = -239.74\n",
      "  Episode 69/100: reward = -357.14\n",
      "  Episode 70/100: reward = -235.79\n",
      "  Episode 71/100: reward = -247.09\n",
      "  Episode 72/100: reward = -124.92\n",
      "  Episode 73/100: reward = -118.33\n",
      "  Episode 74/100: reward = -116.71\n",
      "  Episode 75/100: reward = -124.75\n",
      "  Episode 76/100: reward = -3.95\n",
      "  Episode 77/100: reward = -123.38\n",
      "  Episode 78/100: reward = -123.54\n",
      "  Episode 79/100: reward = -123.19\n",
      "  Episode 80/100: reward = -240.77\n",
      "  Episode 81/100: reward = -5.28\n",
      "  Episode 82/100: reward = -122.90\n",
      "  Episode 83/100: reward = -245.78\n",
      "  Episode 84/100: reward = -252.15\n",
      "  Episode 85/100: reward = -231.18\n",
      "  Episode 86/100: reward = -6.20\n",
      "  Episode 87/100: reward = -246.63\n",
      "  Episode 88/100: reward = -3.56\n",
      "  Episode 89/100: reward = -129.70\n",
      "  Episode 90/100: reward = -119.67\n",
      "  Episode 91/100: reward = -128.53\n",
      "  Episode 92/100: reward = -260.55\n",
      "  Episode 93/100: reward = -128.06\n",
      "  Episode 94/100: reward = -128.92\n",
      "  Episode 95/100: reward = -231.44\n",
      "  Episode 96/100: reward = -3.64\n",
      "  Episode 97/100: reward = -237.12\n",
      "  Episode 98/100: reward = -240.49\n",
      "  Episode 99/100: reward = -122.25\n",
      "  Episode 100/100: reward = -231.79\n",
      "✅ Model Pendulum_DQN_lr-0.001.pth: avg reward = -138.44\n",
      "🎥 Recording episode 43 for Pendulum_DQN_lr-0.001.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_lr-0.001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Pendulum_DQN_lr-0.001.pth at ./videos/Pendulum-v1/Pendulum_DQN_lr-0.001/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_lr-0.01.pth\n",
      "  Episode 1/100: reward = -223.55\n",
      "  Episode 2/100: reward = -243.18\n",
      "  Episode 3/100: reward = -231.45\n",
      "  Episode 4/100: reward = -230.66\n",
      "  Episode 5/100: reward = -125.01\n",
      "  Episode 6/100: reward = -353.79\n",
      "  Episode 7/100: reward = -252.57\n",
      "  Episode 8/100: reward = -249.73\n",
      "  Episode 9/100: reward = -126.42\n",
      "  Episode 10/100: reward = -125.52\n",
      "  Episode 11/100: reward = -123.65\n",
      "  Episode 12/100: reward = -244.20\n",
      "  Episode 13/100: reward = -223.84\n",
      "  Episode 14/100: reward = -222.95\n",
      "  Episode 15/100: reward = -2.88\n",
      "  Episode 16/100: reward = -0.98\n",
      "  Episode 17/100: reward = -121.94\n",
      "  Episode 18/100: reward = -125.03\n",
      "  Episode 19/100: reward = -123.67\n",
      "  Episode 20/100: reward = -357.78\n",
      "  Episode 21/100: reward = -233.65\n",
      "  Episode 22/100: reward = -235.43\n",
      "  Episode 23/100: reward = -261.47\n",
      "  Episode 24/100: reward = -249.80\n",
      "  Episode 25/100: reward = -221.05\n",
      "  Episode 26/100: reward = -123.65\n",
      "  Episode 27/100: reward = -337.48\n",
      "  Episode 28/100: reward = -227.81\n",
      "  Episode 29/100: reward = -232.00\n",
      "  Episode 30/100: reward = -124.43\n",
      "  Episode 31/100: reward = -243.23\n",
      "  Episode 32/100: reward = -229.60\n",
      "  Episode 33/100: reward = -227.72\n",
      "  Episode 34/100: reward = -125.34\n",
      "  Episode 35/100: reward = -123.21\n",
      "  Episode 36/100: reward = -0.38\n",
      "  Episode 37/100: reward = -1.64\n",
      "  Episode 38/100: reward = -120.67\n",
      "  Episode 39/100: reward = -114.30\n",
      "  Episode 40/100: reward = -114.78\n",
      "  Episode 41/100: reward = -126.06\n",
      "  Episode 42/100: reward = -126.76\n",
      "  Episode 43/100: reward = -115.35\n",
      "  Episode 44/100: reward = -0.19\n",
      "  Episode 45/100: reward = -119.52\n",
      "  Episode 46/100: reward = -222.24\n",
      "  Episode 47/100: reward = -297.15\n",
      "  Episode 48/100: reward = -234.98\n",
      "  Episode 49/100: reward = -227.86\n",
      "  Episode 50/100: reward = -126.02\n",
      "  Episode 51/100: reward = -126.86\n",
      "  Episode 52/100: reward = -117.48\n",
      "  Episode 53/100: reward = -226.62\n",
      "  Episode 54/100: reward = -1.67\n",
      "  Episode 55/100: reward = -247.34\n",
      "  Episode 56/100: reward = -227.42\n",
      "  Episode 57/100: reward = -236.18\n",
      "  Episode 58/100: reward = -228.93\n",
      "  Episode 59/100: reward = -115.26\n",
      "  Episode 60/100: reward = -125.72\n",
      "  Episode 61/100: reward = -223.22\n",
      "  Episode 62/100: reward = -230.34\n",
      "  Episode 63/100: reward = -335.59\n",
      "  Episode 64/100: reward = -339.60\n",
      "  Episode 65/100: reward = -0.98\n",
      "  Episode 66/100: reward = -240.18\n",
      "  Episode 67/100: reward = -120.77\n",
      "  Episode 68/100: reward = -123.94\n",
      "  Episode 69/100: reward = -0.23\n",
      "  Episode 70/100: reward = -124.36\n",
      "  Episode 71/100: reward = -0.81\n",
      "  Episode 72/100: reward = -0.20\n",
      "  Episode 73/100: reward = -247.71\n",
      "  Episode 74/100: reward = -127.16\n",
      "  Episode 75/100: reward = -128.73\n",
      "  Episode 76/100: reward = -232.75\n",
      "  Episode 77/100: reward = -120.01\n",
      "  Episode 78/100: reward = -120.39\n",
      "  Episode 79/100: reward = -241.90\n",
      "  Episode 80/100: reward = -126.69\n",
      "  Episode 81/100: reward = -119.09\n",
      "  Episode 82/100: reward = -119.39\n",
      "  Episode 83/100: reward = -342.17\n",
      "  Episode 84/100: reward = -116.25\n",
      "  Episode 85/100: reward = -122.39\n",
      "  Episode 86/100: reward = -219.14\n",
      "  Episode 87/100: reward = -124.36\n",
      "  Episode 88/100: reward = -222.42\n",
      "  Episode 89/100: reward = -123.21\n",
      "  Episode 90/100: reward = -114.66\n",
      "  Episode 91/100: reward = -2.52\n",
      "  Episode 92/100: reward = -117.45\n",
      "  Episode 93/100: reward = -119.64\n",
      "  Episode 94/100: reward = -1.19\n",
      "  Episode 95/100: reward = -123.22\n",
      "  Episode 96/100: reward = -0.52\n",
      "  Episode 97/100: reward = -120.55\n",
      "  Episode 98/100: reward = -115.04\n",
      "  Episode 99/100: reward = -127.43\n",
      "  Episode 100/100: reward = -121.43\n",
      "✅ Model Pendulum_DQN_lr-0.01.pth: avg reward = -160.64\n",
      "🎥 Recording episode 77 for Pendulum_DQN_lr-0.01.pth...\n",
      "🎞️ Saved video for Pendulum_DQN_lr-0.01.pth at ./videos/Pendulum-v1/Pendulum_DQN_lr-0.01/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_lr-1e-05.pth\n",
      "  Episode 1/100: reward = -1454.93\n",
      "  Episode 2/100: reward = -1579.31\n",
      "  Episode 3/100: reward = -1171.32\n",
      "  Episode 4/100: reward = -1732.87\n",
      "  Episode 5/100: reward = -1224.07\n",
      "  Episode 6/100: reward = -1716.29\n",
      "  Episode 7/100: reward = -1575.52\n",
      "  Episode 8/100: reward = -1007.28\n",
      "  Episode 9/100: reward = -1567.71\n",
      "  Episode 10/100: reward = -1022.51\n",
      "  Episode 11/100: reward = -1341.89\n",
      "  Episode 12/100: reward = -1447.66\n",
      "  Episode 13/100: reward = -1043.33\n",
      "  Episode 14/100: reward = -1513.06\n",
      "  Episode 15/100: reward = -1044.12\n",
      "  Episode 16/100: reward = -1391.58\n",
      "  Episode 17/100: reward = -1049.65\n",
      "  Episode 18/100: reward = -1537.76\n",
      "  Episode 19/100: reward = -1436.84\n",
      "  Episode 20/100: reward = -1053.35\n",
      "  Episode 21/100: reward = -1492.51\n",
      "  Episode 22/100: reward = -1243.89\n",
      "  Episode 23/100: reward = -1482.70\n",
      "  Episode 24/100: reward = -1091.80\n",
      "  Episode 25/100: reward = -1419.13\n",
      "  Episode 26/100: reward = -1431.01\n",
      "  Episode 27/100: reward = -1317.42\n",
      "  Episode 28/100: reward = -1451.31\n",
      "  Episode 29/100: reward = -1429.52\n",
      "  Episode 30/100: reward = -1399.16\n",
      "  Episode 31/100: reward = -1061.87\n",
      "  Episode 32/100: reward = -1091.85\n",
      "  Episode 33/100: reward = -1571.61\n",
      "  Episode 34/100: reward = -1542.46\n",
      "  Episode 35/100: reward = -1047.07\n",
      "  Episode 36/100: reward = -1050.80\n",
      "  Episode 37/100: reward = -1338.81\n",
      "  Episode 38/100: reward = -1130.51\n",
      "  Episode 39/100: reward = -1552.94\n",
      "  Episode 40/100: reward = -1369.62\n",
      "  Episode 41/100: reward = -1014.35\n",
      "  Episode 42/100: reward = -1109.31\n",
      "  Episode 43/100: reward = -1467.10\n",
      "  Episode 44/100: reward = -1360.56\n",
      "  Episode 45/100: reward = -1435.21\n",
      "  Episode 46/100: reward = -1051.49\n",
      "  Episode 47/100: reward = -1041.54\n",
      "  Episode 48/100: reward = -1580.26\n",
      "  Episode 49/100: reward = -1449.69\n",
      "  Episode 50/100: reward = -1373.17\n",
      "  Episode 51/100: reward = -1420.93\n",
      "  Episode 52/100: reward = -1057.71\n",
      "  Episode 53/100: reward = -1033.13\n",
      "  Episode 54/100: reward = -1057.64\n",
      "  Episode 55/100: reward = -1410.81\n",
      "  Episode 56/100: reward = -1375.14\n",
      "  Episode 57/100: reward = -1015.00\n",
      "  Episode 58/100: reward = -1246.44\n",
      "  Episode 59/100: reward = -1427.37\n",
      "  Episode 60/100: reward = -1021.01\n",
      "  Episode 61/100: reward = -1392.70\n",
      "  Episode 62/100: reward = -1382.29\n",
      "  Episode 63/100: reward = -1476.00\n",
      "  Episode 64/100: reward = -1429.99\n",
      "  Episode 65/100: reward = -1364.38\n",
      "  Episode 66/100: reward = -1454.63\n",
      "  Episode 67/100: reward = -1206.81\n",
      "  Episode 68/100: reward = -1021.84\n",
      "  Episode 69/100: reward = -1350.11\n",
      "  Episode 70/100: reward = -1373.15\n",
      "  Episode 71/100: reward = -1025.10\n",
      "  Episode 72/100: reward = -1313.47\n",
      "  Episode 73/100: reward = -1089.43\n",
      "  Episode 74/100: reward = -1025.57\n",
      "  Episode 75/100: reward = -1730.44\n",
      "  Episode 76/100: reward = -1396.45\n",
      "  Episode 77/100: reward = -1043.68\n",
      "  Episode 78/100: reward = -1258.54\n",
      "  Episode 79/100: reward = -1433.72\n",
      "  Episode 80/100: reward = -1423.72\n",
      "  Episode 81/100: reward = -1535.41\n",
      "  Episode 82/100: reward = -1282.34\n",
      "  Episode 83/100: reward = -1391.27\n",
      "  Episode 84/100: reward = -1570.89\n",
      "  Episode 85/100: reward = -1431.30\n",
      "  Episode 86/100: reward = -1208.56\n",
      "  Episode 87/100: reward = -1038.07\n",
      "  Episode 88/100: reward = -1011.56\n",
      "  Episode 89/100: reward = -1382.09\n",
      "  Episode 90/100: reward = -1037.47\n",
      "  Episode 91/100: reward = -1564.94\n",
      "  Episode 92/100: reward = -1433.57\n",
      "  Episode 93/100: reward = -1325.34\n",
      "  Episode 94/100: reward = -1426.63\n",
      "  Episode 95/100: reward = -1735.59\n",
      "  Episode 96/100: reward = -1464.72\n",
      "  Episode 97/100: reward = -1053.61\n",
      "  Episode 98/100: reward = -1030.98\n",
      "  Episode 99/100: reward = -1006.09\n",
      "  Episode 100/100: reward = -1347.46\n",
      "✅ Model Pendulum_DQN_lr-1e-05.pth: avg reward = -1308.45\n",
      "🎥 Recording episode 18 for Pendulum_DQN_lr-1e-05.pth...\n",
      "🎞️ Saved video for Pendulum_DQN_lr-1e-05.pth at ./videos/Pendulum-v1/Pendulum_DQN_lr-1e-05/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_mem-10000.pth\n",
      "  Episode 1/100: reward = -124.78\n",
      "  Episode 2/100: reward = -122.10\n",
      "  Episode 3/100: reward = -121.55\n",
      "  Episode 4/100: reward = -116.67\n",
      "  Episode 5/100: reward = -2.07\n",
      "  Episode 6/100: reward = -120.26\n",
      "  Episode 7/100: reward = -297.58\n",
      "  Episode 8/100: reward = -118.90\n",
      "  Episode 9/100: reward = -120.03\n",
      "  Episode 10/100: reward = -124.85\n",
      "  Episode 11/100: reward = -119.95\n",
      "  Episode 12/100: reward = -118.24\n",
      "  Episode 13/100: reward = -240.29\n",
      "  Episode 14/100: reward = -122.30\n",
      "  Episode 15/100: reward = -118.63\n",
      "  Episode 16/100: reward = -116.71\n",
      "  Episode 17/100: reward = -234.58\n",
      "  Episode 18/100: reward = -227.26\n",
      "  Episode 19/100: reward = -244.21\n",
      "  Episode 20/100: reward = -2.48\n",
      "  Episode 21/100: reward = -120.10\n",
      "  Episode 22/100: reward = -115.97\n",
      "  Episode 23/100: reward = -235.60\n",
      "  Episode 24/100: reward = -363.93\n",
      "  Episode 25/100: reward = -2.56\n",
      "  Episode 26/100: reward = -121.92\n",
      "  Episode 27/100: reward = -126.19\n",
      "  Episode 28/100: reward = -230.16\n",
      "  Episode 29/100: reward = -115.21\n",
      "  Episode 30/100: reward = -345.73\n",
      "  Episode 31/100: reward = -3.28\n",
      "  Episode 32/100: reward = -252.73\n",
      "  Episode 33/100: reward = -350.35\n",
      "  Episode 34/100: reward = -238.46\n",
      "  Episode 35/100: reward = -225.03\n",
      "  Episode 36/100: reward = -120.13\n",
      "  Episode 37/100: reward = -3.20\n",
      "  Episode 38/100: reward = -125.01\n",
      "  Episode 39/100: reward = -125.82\n",
      "  Episode 40/100: reward = -229.92\n",
      "  Episode 41/100: reward = -127.29\n",
      "  Episode 42/100: reward = -238.47\n",
      "  Episode 43/100: reward = -119.95\n",
      "  Episode 44/100: reward = -235.76\n",
      "  Episode 45/100: reward = -117.20\n",
      "  Episode 46/100: reward = -124.89\n",
      "  Episode 47/100: reward = -125.16\n",
      "  Episode 48/100: reward = -117.23\n",
      "  Episode 49/100: reward = -123.74\n",
      "  Episode 50/100: reward = -337.31\n",
      "  Episode 51/100: reward = -122.73\n",
      "  Episode 52/100: reward = -5.58\n",
      "  Episode 53/100: reward = -114.80\n",
      "  Episode 54/100: reward = -130.05\n",
      "  Episode 55/100: reward = -236.45\n",
      "  Episode 56/100: reward = -2.38\n",
      "  Episode 57/100: reward = -359.62\n",
      "  Episode 58/100: reward = -122.72\n",
      "  Episode 59/100: reward = -239.51\n",
      "  Episode 60/100: reward = -2.17\n",
      "  Episode 61/100: reward = -120.63\n",
      "  Episode 62/100: reward = -127.38\n",
      "  Episode 63/100: reward = -125.86\n",
      "  Episode 64/100: reward = -224.35\n",
      "  Episode 65/100: reward = -124.08\n",
      "  Episode 66/100: reward = -127.00\n",
      "  Episode 67/100: reward = -122.60\n",
      "  Episode 68/100: reward = -125.74\n",
      "  Episode 69/100: reward = -123.17\n",
      "  Episode 70/100: reward = -2.10\n",
      "  Episode 71/100: reward = -122.80\n",
      "  Episode 72/100: reward = -119.56\n",
      "  Episode 73/100: reward = -2.26\n",
      "  Episode 74/100: reward = -3.93\n",
      "  Episode 75/100: reward = -234.11\n",
      "  Episode 76/100: reward = -316.96\n",
      "  Episode 77/100: reward = -121.87\n",
      "  Episode 78/100: reward = -298.50\n",
      "  Episode 79/100: reward = -4.11\n",
      "  Episode 80/100: reward = -125.92\n",
      "  Episode 81/100: reward = -234.16\n",
      "  Episode 82/100: reward = -121.25\n",
      "  Episode 83/100: reward = -239.16\n",
      "  Episode 84/100: reward = -121.13\n",
      "  Episode 85/100: reward = -249.91\n",
      "  Episode 86/100: reward = -125.98\n",
      "  Episode 87/100: reward = -118.99\n",
      "  Episode 88/100: reward = -122.95\n",
      "  Episode 89/100: reward = -2.09\n",
      "  Episode 90/100: reward = -122.02\n",
      "  Episode 91/100: reward = -122.33\n",
      "  Episode 92/100: reward = -228.11\n",
      "  Episode 93/100: reward = -239.32\n",
      "  Episode 94/100: reward = -343.03\n",
      "  Episode 95/100: reward = -4.91\n",
      "  Episode 96/100: reward = -120.76\n",
      "  Episode 97/100: reward = -122.84\n",
      "  Episode 98/100: reward = -116.12\n",
      "  Episode 99/100: reward = -134.56\n",
      "  Episode 100/100: reward = -124.52\n",
      "✅ Model Pendulum_DQN_mem-10000.pth: avg reward = -148.55\n",
      "🎥 Recording episode 32 for Pendulum_DQN_mem-10000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_mem-10000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Pendulum_DQN_mem-10000.pth at ./videos/Pendulum-v1/Pendulum_DQN_mem-10000/\n",
      "\n",
      "🎬 Testing and recording model: Pendulum_DQN_mem-50000.pth\n",
      "  Episode 1/100: reward = -233.44\n",
      "  Episode 2/100: reward = -224.62\n",
      "  Episode 3/100: reward = -237.96\n",
      "  Episode 4/100: reward = -0.90\n",
      "  Episode 5/100: reward = -119.13\n",
      "  Episode 6/100: reward = -121.36\n",
      "  Episode 7/100: reward = -226.21\n",
      "  Episode 8/100: reward = -115.68\n",
      "  Episode 9/100: reward = -116.81\n",
      "  Episode 10/100: reward = -117.48\n",
      "  Episode 11/100: reward = -120.59\n",
      "  Episode 12/100: reward = -123.47\n",
      "  Episode 13/100: reward = -121.86\n",
      "  Episode 14/100: reward = -233.05\n",
      "  Episode 15/100: reward = -124.34\n",
      "  Episode 16/100: reward = -123.24\n",
      "  Episode 17/100: reward = -222.96\n",
      "  Episode 18/100: reward = -117.64\n",
      "  Episode 19/100: reward = -327.20\n",
      "  Episode 20/100: reward = -123.64\n",
      "  Episode 21/100: reward = -239.40\n",
      "  Episode 22/100: reward = -117.51\n",
      "  Episode 23/100: reward = -119.16\n",
      "  Episode 24/100: reward = -229.22\n",
      "  Episode 25/100: reward = -236.80\n",
      "  Episode 26/100: reward = -229.82\n",
      "  Episode 27/100: reward = -228.96\n",
      "  Episode 28/100: reward = -122.25\n",
      "  Episode 29/100: reward = -249.08\n",
      "  Episode 30/100: reward = -0.64\n",
      "  Episode 31/100: reward = -324.69\n",
      "  Episode 32/100: reward = -225.91\n",
      "  Episode 33/100: reward = -342.66\n",
      "  Episode 34/100: reward = -230.36\n",
      "  Episode 35/100: reward = -124.93\n",
      "  Episode 36/100: reward = -122.09\n",
      "  Episode 37/100: reward = -123.17\n",
      "  Episode 38/100: reward = -120.92\n",
      "  Episode 39/100: reward = -123.92\n",
      "  Episode 40/100: reward = -118.52\n",
      "  Episode 41/100: reward = -113.78\n",
      "  Episode 42/100: reward = -238.08\n",
      "  Episode 43/100: reward = -239.66\n",
      "  Episode 44/100: reward = -118.89\n",
      "  Episode 45/100: reward = -117.81\n",
      "  Episode 46/100: reward = -125.23\n",
      "  Episode 47/100: reward = -123.27\n",
      "  Episode 48/100: reward = -126.57\n",
      "  Episode 49/100: reward = -120.19\n",
      "  Episode 50/100: reward = -0.70\n",
      "  Episode 51/100: reward = -0.87\n",
      "  Episode 52/100: reward = -321.04\n",
      "  Episode 53/100: reward = -121.66\n",
      "  Episode 54/100: reward = -118.98\n",
      "  Episode 55/100: reward = -122.20\n",
      "  Episode 56/100: reward = -243.16\n",
      "  Episode 57/100: reward = -5.36\n",
      "  Episode 58/100: reward = -1.73\n",
      "  Episode 59/100: reward = -123.26\n",
      "  Episode 60/100: reward = -232.42\n",
      "  Episode 61/100: reward = -123.30\n",
      "  Episode 62/100: reward = -120.97\n",
      "  Episode 63/100: reward = -125.27\n",
      "  Episode 64/100: reward = -118.83\n",
      "  Episode 65/100: reward = -114.25\n",
      "  Episode 66/100: reward = -228.77\n",
      "  Episode 67/100: reward = -349.94\n",
      "  Episode 68/100: reward = -225.05\n",
      "  Episode 69/100: reward = -120.93\n",
      "  Episode 70/100: reward = -234.74\n",
      "  Episode 71/100: reward = -231.53\n",
      "  Episode 72/100: reward = -117.63\n",
      "  Episode 73/100: reward = -232.36\n",
      "  Episode 74/100: reward = -250.62\n",
      "  Episode 75/100: reward = -122.88\n",
      "  Episode 76/100: reward = -234.90\n",
      "  Episode 77/100: reward = -122.60\n",
      "  Episode 78/100: reward = -346.30\n",
      "  Episode 79/100: reward = -123.66\n",
      "  Episode 80/100: reward = -123.20\n",
      "  Episode 81/100: reward = -113.87\n",
      "  Episode 82/100: reward = -224.64\n",
      "  Episode 83/100: reward = -124.06\n",
      "  Episode 84/100: reward = -235.63\n",
      "  Episode 85/100: reward = -0.79\n",
      "  Episode 86/100: reward = -117.79\n",
      "  Episode 87/100: reward = -119.67\n",
      "  Episode 88/100: reward = -121.41\n",
      "  Episode 89/100: reward = -0.53\n",
      "  Episode 90/100: reward = -119.21\n",
      "  Episode 91/100: reward = -3.25\n",
      "  Episode 92/100: reward = -122.71\n",
      "  Episode 93/100: reward = -123.63\n",
      "  Episode 94/100: reward = -118.90\n",
      "  Episode 95/100: reward = -118.29\n",
      "  Episode 96/100: reward = -0.54\n",
      "  Episode 97/100: reward = -119.11\n",
      "  Episode 98/100: reward = -232.23\n",
      "  Episode 99/100: reward = -123.03\n",
      "  Episode 100/100: reward = -115.73\n",
      "✅ Model Pendulum_DQN_mem-50000.pth: avg reward = -153.19\n",
      "🎥 Recording episode 62 for Pendulum_DQN_mem-50000.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_mem-50000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎞️ Saved video for Pendulum_DQN_mem-50000.pth at ./videos/Pendulum-v1/Pendulum_DQN_mem-50000/\n",
      "\n",
      "🏁 All models tested and recorded successfully.\n"
     ]
    }
   ],
   "source": [
    "record_models_in_folder(\n",
    "    models_folder=\"./pendulum Models\",\n",
    "    env_name=\"Pendulum-v1\",\n",
    "    num_discrete_actions=5,\n",
    "    num_tests=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6740a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models_folder_with_wandb(\n",
    "    models_folder,\n",
    "    env_name,\n",
    "    num_tests=100,\n",
    "    num_discrete_actions=5,\n",
    "    wandb_project=\"RL A2 tests\",\n",
    "    seed=None\n",
    "):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_files = sorted([f for f in os.listdir(models_folder) if f.endswith(\".pth\")])\n",
    "    if not model_files:\n",
    "        print(\"No .pth files found in\", models_folder)\n",
    "        return\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model_path = os.path.join(models_folder, model_file)\n",
    "        run_name = f\"test_{os.path.splitext(model_file)[0]}_{int(time.time())}\"\n",
    "        print(f\"\\n=== Model: {model_file} ===\")\n",
    "        print(\"W&B run:\", run_name)\n",
    "\n",
    "        # Init W&B run\n",
    "        run = wandb.init(project=wandb_project, name=run_name, reinit=True, config={\n",
    "            \"env_name\": env_name,\n",
    "            \"model_file\": model_file,\n",
    "            \"num_tests\": num_tests,\n",
    "            \"num_discrete_actions\": num_discrete_actions\n",
    "        })\n",
    "\n",
    "        # Build environment\n",
    "        base_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        if isinstance(base_env.action_space, gym.spaces.Box):\n",
    "            env = DiscretizeActionWrapper(base_env, num_discrete_actions=num_discrete_actions)\n",
    "        else:\n",
    "            env = base_env\n",
    "\n",
    "        # Load model (infer sizes from env reset)\n",
    "        sample_state, _ = env.reset()\n",
    "        env.render()\n",
    "        n_observations = len(sample_state)\n",
    "        n_actions = env.action_space.n\n",
    "\n",
    "        model = DQN(n_observations, n_actions).to(device)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "\n",
    "        # --- Run multiple test episodes ---\n",
    "        episode_rewards = []\n",
    "        for ep in range(num_tests):\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state_tensor)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                done = terminated or truncated\n",
    "                state = next_state\n",
    "\n",
    "            episode_rewards.append(total_reward)\n",
    "            wandb.log({\"episode\": ep, \"episode_reward\": total_reward, \"steps\": steps})\n",
    "            print(f\"  Ep {ep+1}/{num_tests}: reward={total_reward:.2f} steps={steps}\")\n",
    "\n",
    "        avg_reward = float(np.mean(episode_rewards))\n",
    "        best_reward = float(np.max(episode_rewards))\n",
    "        worst_reward = float(np.min(episode_rewards))\n",
    "        wandb.log({\"avg_reward\": avg_reward, \"best_reward\": best_reward, \"worst_reward\": worst_reward})\n",
    "        print(f\"Finished evaluations. avg={avg_reward:.2f} best={best_reward:.2f} worst={worst_reward:.2f}\")\n",
    "\n",
    "        # --- Record a single random episode ---\n",
    "        record_idx = random.randrange(0, num_tests)\n",
    "        print(f\"Recording random episode index {record_idx} for model {model_file}\")\n",
    "\n",
    "        # Wrap fresh env for recording\n",
    "        video_folder = f\"videos/{env_name}/{os.path.splitext(model_file)[0]}\"\n",
    "        os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "        video_base = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        if isinstance(video_base.action_space, gym.spaces.Box):\n",
    "            video_env = DiscretizeActionWrapper(video_base, num_discrete_actions=num_discrete_actions)\n",
    "        else:\n",
    "            video_env = video_base\n",
    "\n",
    "        # Record only **the next episode**\n",
    "        video_env = RecordVideo(\n",
    "            video_env,\n",
    "            video_folder=video_folder,\n",
    "            episode_trigger=lambda e: True,  # record the first episode run\n",
    "            name_prefix=os.path.splitext(model_file)[0],\n",
    "            disable_logger=True\n",
    "        )\n",
    "\n",
    "        state, _ = video_env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        testing_start_time = time.time()\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_tensor)\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "        video_env.close()\n",
    "\n",
    "        # Log video to W&B\n",
    "        vid_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "        if vid_files:\n",
    "            vid_files = sorted(vid_files, key=lambda p: os.path.getmtime(os.path.join(video_folder, p)), reverse=True)\n",
    "            video_path = os.path.join(video_folder, vid_files[0])\n",
    "            print(\"Recorded video:\", video_path)\n",
    "            wandb.log({\"test_video\": wandb.Video(video_path, fps=30, format=\"mp4\")})\n",
    "        else:\n",
    "            print(\"Warning: no video file found in\", video_folder)\n",
    "\n",
    "        testing_end_time = time.time()\n",
    "        testing_duration = testing_end_time - testing_start_time\n",
    "        print(f\"Recording complete. Total reward: {total_reward:.2f}, Steps:{steps}, Duration: {testing_duration:.2f} seconds\")\n",
    "        # Log recorded episode stats\n",
    "        wandb.log({\n",
    "            \"recorded_episode_index\": record_idx,\n",
    "            \"recorded_episode_reward\": total_reward,\n",
    "            \"recorded_episode_steps\": steps,\n",
    "            \"recorded_episode_duration_sec\": testing_duration\n",
    "        })\n",
    "\n",
    "        run.finish()\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f31bffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_batch-128.pth ===\n",
      "W&B run: test_CartPole_DQN_batch-128_1763011249\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>██▇▇██▇█▇▇▇██▇▇▁█▁█▁██▁█▇▇█▁██▁██▁██▇▁██</td></tr><tr><td>steps</td><td>▁▁▁▁▂▁▁▁█▁▂█▁▁▁█▂▁███▂▁▁▁▂▁▁▂▁▁▁▂▂▁▁██▂▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-156.36</td></tr><tr><td>best_reward</td><td>-140</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-143</td></tr><tr><td>steps</td><td>143</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_mem-10000 _1763010782</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/a1c5fnna' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/a1c5fnna</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071302-a1c5fnna\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072049-8wdj4w8v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8wdj4w8v' target=\"_blank\">test_CartPole_DQN_batch-128_1763011249</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8wdj4w8v' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8wdj4w8v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 35 for model CartPole_DQN_batch-128.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_batch-128 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_batch-128\\CartPole_DQN_batch-128-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.44 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.43514</td></tr><tr><td>recorded_episode_index</td><td>35</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_batch-128_1763011249</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8wdj4w8v' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8wdj4w8v</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072049-8wdj4w8v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_batch-256.pth ===\n",
      "W&B run: test_CartPole_DQN_batch-256_1763011261\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072101-0lveehjp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0lveehjp' target=\"_blank\">test_CartPole_DQN_batch-256_1763011261</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0lveehjp' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0lveehjp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 35 for model CartPole_DQN_batch-256.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_batch-256 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_batch-256\\CartPole_DQN_batch-256-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.36 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.35848</td></tr><tr><td>recorded_episode_index</td><td>35</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_batch-256_1763011261</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0lveehjp' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0lveehjp</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072101-0lveehjp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_batch-64.pth ===\n",
      "W&B run: test_CartPole_DQN_batch-64_1763011270\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072110-c773rysk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/c773rysk' target=\"_blank\">test_CartPole_DQN_batch-64_1763011270</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/c773rysk' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/c773rysk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=166.00 steps=166\n",
      "  Ep 2/100: reward=329.00 steps=329\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=236.00 steps=236\n",
      "  Ep 7/100: reward=286.00 steps=286\n",
      "  Ep 8/100: reward=463.00 steps=463\n",
      "  Ep 9/100: reward=171.00 steps=171\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=147.00 steps=147\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=137.00 steps=137\n",
      "  Ep 14/100: reward=368.00 steps=368\n",
      "  Ep 15/100: reward=487.00 steps=487\n",
      "  Ep 16/100: reward=131.00 steps=131\n",
      "  Ep 17/100: reward=240.00 steps=240\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=173.00 steps=173\n",
      "  Ep 20/100: reward=390.00 steps=390\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=160.00 steps=160\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=155.00 steps=155\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=374.00 steps=374\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=467.00 steps=467\n",
      "  Ep 32/100: reward=284.00 steps=284\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=486.00 steps=486\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=336.00 steps=336\n",
      "  Ep 37/100: reward=167.00 steps=167\n",
      "  Ep 38/100: reward=290.00 steps=290\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=381.00 steps=381\n",
      "  Ep 41/100: reward=421.00 steps=421\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=422.00 steps=422\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=254.00 steps=254\n",
      "  Ep 49/100: reward=401.00 steps=401\n",
      "  Ep 50/100: reward=142.00 steps=142\n",
      "  Ep 51/100: reward=286.00 steps=286\n",
      "  Ep 52/100: reward=138.00 steps=138\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=424.00 steps=424\n",
      "  Ep 56/100: reward=292.00 steps=292\n",
      "  Ep 57/100: reward=270.00 steps=270\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=191.00 steps=191\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=437.00 steps=437\n",
      "  Ep 66/100: reward=410.00 steps=410\n",
      "  Ep 67/100: reward=259.00 steps=259\n",
      "  Ep 68/100: reward=114.00 steps=114\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=332.00 steps=332\n",
      "  Ep 71/100: reward=154.00 steps=154\n",
      "  Ep 72/100: reward=129.00 steps=129\n",
      "  Ep 73/100: reward=308.00 steps=308\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=298.00 steps=298\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=443.00 steps=443\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=440.00 steps=440\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=146.00 steps=146\n",
      "  Ep 87/100: reward=385.00 steps=385\n",
      "  Ep 88/100: reward=398.00 steps=398\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=459.00 steps=459\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=432.00 steps=432\n",
      "  Ep 96/100: reward=378.00 steps=378\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=494.00 steps=494\n",
      "  Ep 99/100: reward=404.00 steps=404\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=394.85 best=500.00 worst=114.00\n",
      "Recording random episode index 73 for model CartPole_DQN_batch-64.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_batch-64 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_batch-64\\CartPole_DQN_batch-64-episode-0.mp4\n",
      "Recording complete. Total reward: 180.00, Steps:180, Duration: 0.63 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇█████</td></tr><tr><td>episode_reward</td><td>▅██▄▇▁▁▁▃█▄██▅▂███▇▆▁▇▄█████▇▆██▇██▇▁██▆</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▅▃▇█▁█▃█▂▆▁█▇█▂█▆██▆█▄██▆█▁▁█▄███▇███▇▆▆</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>394.85</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.62739</td></tr><tr><td>recorded_episode_index</td><td>73</td></tr><tr><td>recorded_episode_reward</td><td>180</td></tr><tr><td>recorded_episode_steps</td><td>180</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_batch-64_1763011270</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/c773rysk' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/c773rysk</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072110-c773rysk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_eps-1000.pth ===\n",
      "W&B run: test_CartPole_DQN_eps-1000_1763011278\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072118-qkyc3ua2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qkyc3ua2' target=\"_blank\">test_CartPole_DQN_eps-1000_1763011278</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qkyc3ua2' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qkyc3ua2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 59 for model CartPole_DQN_eps-1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_eps-1000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_eps-1000\\CartPole_DQN_eps-1000-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.40 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.40421</td></tr><tr><td>recorded_episode_index</td><td>59</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_eps-1000_1763011278</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qkyc3ua2' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qkyc3ua2</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072118-qkyc3ua2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_eps-5000.pth ===\n",
      "W&B run: test_CartPole_DQN_eps-5000_1763011288\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072128-nqjs1cef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/nqjs1cef' target=\"_blank\">test_CartPole_DQN_eps-5000_1763011288</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/nqjs1cef' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/nqjs1cef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 55 for model CartPole_DQN_eps-5000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_eps-5000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_eps-5000\\CartPole_DQN_eps-5000-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.50 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.50452</td></tr><tr><td>recorded_episode_index</td><td>55</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_eps-5000_1763011288</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/nqjs1cef' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/nqjs1cef</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072128-nqjs1cef\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_gamma-0.1.pth ===\n",
      "W&B run: test_CartPole_DQN_gamma-0.1_1763011298\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072138-irsezrg8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/irsezrg8' target=\"_blank\">test_CartPole_DQN_gamma-0.1_1763011298</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/irsezrg8' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/irsezrg8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=9.00 steps=9\n",
      "  Ep 2/100: reward=11.00 steps=11\n",
      "  Ep 3/100: reward=10.00 steps=10\n",
      "  Ep 4/100: reward=11.00 steps=11\n",
      "  Ep 5/100: reward=9.00 steps=9\n",
      "  Ep 6/100: reward=10.00 steps=10\n",
      "  Ep 7/100: reward=10.00 steps=10\n",
      "  Ep 8/100: reward=10.00 steps=10\n",
      "  Ep 9/100: reward=10.00 steps=10\n",
      "  Ep 10/100: reward=28.00 steps=28\n",
      "  Ep 11/100: reward=8.00 steps=8\n",
      "  Ep 12/100: reward=11.00 steps=11\n",
      "  Ep 13/100: reward=9.00 steps=9\n",
      "  Ep 14/100: reward=8.00 steps=8\n",
      "  Ep 15/100: reward=10.00 steps=10\n",
      "  Ep 16/100: reward=10.00 steps=10\n",
      "  Ep 17/100: reward=10.00 steps=10\n",
      "  Ep 18/100: reward=10.00 steps=10\n",
      "  Ep 19/100: reward=10.00 steps=10\n",
      "  Ep 20/100: reward=10.00 steps=10\n",
      "  Ep 21/100: reward=11.00 steps=11\n",
      "  Ep 22/100: reward=9.00 steps=9\n",
      "  Ep 23/100: reward=18.00 steps=18\n",
      "  Ep 24/100: reward=10.00 steps=10\n",
      "  Ep 25/100: reward=13.00 steps=13\n",
      "  Ep 26/100: reward=9.00 steps=9\n",
      "  Ep 27/100: reward=9.00 steps=9\n",
      "  Ep 28/100: reward=10.00 steps=10\n",
      "  Ep 29/100: reward=9.00 steps=9\n",
      "  Ep 30/100: reward=10.00 steps=10\n",
      "  Ep 31/100: reward=10.00 steps=10\n",
      "  Ep 32/100: reward=11.00 steps=11\n",
      "  Ep 33/100: reward=10.00 steps=10\n",
      "  Ep 34/100: reward=11.00 steps=11\n",
      "  Ep 35/100: reward=10.00 steps=10\n",
      "  Ep 36/100: reward=10.00 steps=10\n",
      "  Ep 37/100: reward=8.00 steps=8\n",
      "  Ep 38/100: reward=9.00 steps=9\n",
      "  Ep 39/100: reward=9.00 steps=9\n",
      "  Ep 40/100: reward=17.00 steps=17\n",
      "  Ep 41/100: reward=11.00 steps=11\n",
      "  Ep 42/100: reward=9.00 steps=9\n",
      "  Ep 43/100: reward=9.00 steps=9\n",
      "  Ep 44/100: reward=9.00 steps=9\n",
      "  Ep 45/100: reward=10.00 steps=10\n",
      "  Ep 46/100: reward=10.00 steps=10\n",
      "  Ep 47/100: reward=10.00 steps=10\n",
      "  Ep 48/100: reward=10.00 steps=10\n",
      "  Ep 49/100: reward=9.00 steps=9\n",
      "  Ep 50/100: reward=10.00 steps=10\n",
      "  Ep 51/100: reward=10.00 steps=10\n",
      "  Ep 52/100: reward=10.00 steps=10\n",
      "  Ep 53/100: reward=11.00 steps=11\n",
      "  Ep 54/100: reward=10.00 steps=10\n",
      "  Ep 55/100: reward=10.00 steps=10\n",
      "  Ep 56/100: reward=9.00 steps=9\n",
      "  Ep 57/100: reward=9.00 steps=9\n",
      "  Ep 58/100: reward=9.00 steps=9\n",
      "  Ep 59/100: reward=10.00 steps=10\n",
      "  Ep 60/100: reward=10.00 steps=10\n",
      "  Ep 61/100: reward=13.00 steps=13\n",
      "  Ep 62/100: reward=9.00 steps=9\n",
      "  Ep 63/100: reward=9.00 steps=9\n",
      "  Ep 64/100: reward=9.00 steps=9\n",
      "  Ep 65/100: reward=10.00 steps=10\n",
      "  Ep 66/100: reward=9.00 steps=9\n",
      "  Ep 67/100: reward=9.00 steps=9\n",
      "  Ep 68/100: reward=8.00 steps=8\n",
      "  Ep 69/100: reward=9.00 steps=9\n",
      "  Ep 70/100: reward=10.00 steps=10\n",
      "  Ep 71/100: reward=10.00 steps=10\n",
      "  Ep 72/100: reward=10.00 steps=10\n",
      "  Ep 73/100: reward=10.00 steps=10\n",
      "  Ep 74/100: reward=10.00 steps=10\n",
      "  Ep 75/100: reward=10.00 steps=10\n",
      "  Ep 76/100: reward=10.00 steps=10\n",
      "  Ep 77/100: reward=9.00 steps=9\n",
      "  Ep 78/100: reward=11.00 steps=11\n",
      "  Ep 79/100: reward=8.00 steps=8\n",
      "  Ep 80/100: reward=10.00 steps=10\n",
      "  Ep 81/100: reward=10.00 steps=10\n",
      "  Ep 82/100: reward=9.00 steps=9\n",
      "  Ep 83/100: reward=10.00 steps=10\n",
      "  Ep 84/100: reward=10.00 steps=10\n",
      "  Ep 85/100: reward=10.00 steps=10\n",
      "  Ep 86/100: reward=10.00 steps=10\n",
      "  Ep 87/100: reward=10.00 steps=10\n",
      "  Ep 88/100: reward=9.00 steps=9\n",
      "  Ep 89/100: reward=10.00 steps=10\n",
      "  Ep 90/100: reward=10.00 steps=10\n",
      "  Ep 91/100: reward=8.00 steps=8\n",
      "  Ep 92/100: reward=10.00 steps=10\n",
      "  Ep 93/100: reward=10.00 steps=10\n",
      "  Ep 94/100: reward=10.00 steps=10\n",
      "  Ep 95/100: reward=10.00 steps=10\n",
      "  Ep 96/100: reward=11.00 steps=11\n",
      "  Ep 97/100: reward=10.00 steps=10\n",
      "  Ep 98/100: reward=17.00 steps=17\n",
      "  Ep 99/100: reward=10.00 steps=10\n",
      "  Ep 100/100: reward=11.00 steps=11\n",
      "Finished evaluations. avg=10.20 best=28.00 worst=8.00\n",
      "Recording random episode index 11 for model CartPole_DQN_gamma-0.1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_gamma-0.1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_gamma-0.1\\CartPole_DQN_gamma-0.1-episode-0.mp4\n",
      "Recording complete. Total reward: 10.00, Steps:10, Duration: 0.24 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▂▁▂▂█▂▁▅▂▁▁▂▂▁▄▁▁▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂▁▂▂▁▂▂▂▂</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▂▂▁▂▂█▂▁▁▂▂▂▂▁▂▂▂▂▁▄▁▁▂▂▂▃▁▁▁▂▂▁▁▂▁▂▁▂▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>10.2</td></tr><tr><td>best_reward</td><td>28</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>11</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.23606</td></tr><tr><td>recorded_episode_index</td><td>11</td></tr><tr><td>recorded_episode_reward</td><td>10</td></tr><tr><td>recorded_episode_steps</td><td>10</td></tr><tr><td>steps</td><td>11</td></tr><tr><td>worst_reward</td><td>8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_gamma-0.1_1763011298</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/irsezrg8' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/irsezrg8</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072138-irsezrg8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_gamma-0.5.pth ===\n",
      "W&B run: test_CartPole_DQN_gamma-0.5_1763011302\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072142-3d3ldmbo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/3d3ldmbo' target=\"_blank\">test_CartPole_DQN_gamma-0.5_1763011302</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/3d3ldmbo' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/3d3ldmbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=361.00 steps=361\n",
      "  Ep 2/100: reward=318.00 steps=318\n",
      "  Ep 3/100: reward=304.00 steps=304\n",
      "  Ep 4/100: reward=362.00 steps=362\n",
      "  Ep 5/100: reward=338.00 steps=338\n",
      "  Ep 6/100: reward=321.00 steps=321\n",
      "  Ep 7/100: reward=306.00 steps=306\n",
      "  Ep 8/100: reward=321.00 steps=321\n",
      "  Ep 9/100: reward=372.00 steps=372\n",
      "  Ep 10/100: reward=392.00 steps=392\n",
      "  Ep 11/100: reward=323.00 steps=323\n",
      "  Ep 12/100: reward=337.00 steps=337\n",
      "  Ep 13/100: reward=384.00 steps=384\n",
      "  Ep 14/100: reward=328.00 steps=328\n",
      "  Ep 15/100: reward=390.00 steps=390\n",
      "  Ep 16/100: reward=388.00 steps=388\n",
      "  Ep 17/100: reward=327.00 steps=327\n",
      "  Ep 18/100: reward=314.00 steps=314\n",
      "  Ep 19/100: reward=342.00 steps=342\n",
      "  Ep 20/100: reward=285.00 steps=285\n",
      "  Ep 21/100: reward=305.00 steps=305\n",
      "  Ep 22/100: reward=331.00 steps=331\n",
      "  Ep 23/100: reward=349.00 steps=349\n",
      "  Ep 24/100: reward=329.00 steps=329\n",
      "  Ep 25/100: reward=346.00 steps=346\n",
      "  Ep 26/100: reward=329.00 steps=329\n",
      "  Ep 27/100: reward=284.00 steps=284\n",
      "  Ep 28/100: reward=316.00 steps=316\n",
      "  Ep 29/100: reward=340.00 steps=340\n",
      "  Ep 30/100: reward=378.00 steps=378\n",
      "  Ep 31/100: reward=310.00 steps=310\n",
      "  Ep 32/100: reward=340.00 steps=340\n",
      "  Ep 33/100: reward=340.00 steps=340\n",
      "  Ep 34/100: reward=326.00 steps=326\n",
      "  Ep 35/100: reward=386.00 steps=386\n",
      "  Ep 36/100: reward=363.00 steps=363\n",
      "  Ep 37/100: reward=409.00 steps=409\n",
      "  Ep 38/100: reward=327.00 steps=327\n",
      "  Ep 39/100: reward=307.00 steps=307\n",
      "  Ep 40/100: reward=311.00 steps=311\n",
      "  Ep 41/100: reward=349.00 steps=349\n",
      "  Ep 42/100: reward=307.00 steps=307\n",
      "  Ep 43/100: reward=351.00 steps=351\n",
      "  Ep 44/100: reward=372.00 steps=372\n",
      "  Ep 45/100: reward=282.00 steps=282\n",
      "  Ep 46/100: reward=324.00 steps=324\n",
      "  Ep 47/100: reward=288.00 steps=288\n",
      "  Ep 48/100: reward=365.00 steps=365\n",
      "  Ep 49/100: reward=330.00 steps=330\n",
      "  Ep 50/100: reward=388.00 steps=388\n",
      "  Ep 51/100: reward=408.00 steps=408\n",
      "  Ep 52/100: reward=297.00 steps=297\n",
      "  Ep 53/100: reward=329.00 steps=329\n",
      "  Ep 54/100: reward=326.00 steps=326\n",
      "  Ep 55/100: reward=365.00 steps=365\n",
      "  Ep 56/100: reward=356.00 steps=356\n",
      "  Ep 57/100: reward=333.00 steps=333\n",
      "  Ep 58/100: reward=340.00 steps=340\n",
      "  Ep 59/100: reward=305.00 steps=305\n",
      "  Ep 60/100: reward=355.00 steps=355\n",
      "  Ep 61/100: reward=293.00 steps=293\n",
      "  Ep 62/100: reward=371.00 steps=371\n",
      "  Ep 63/100: reward=336.00 steps=336\n",
      "  Ep 64/100: reward=356.00 steps=356\n",
      "  Ep 65/100: reward=340.00 steps=340\n",
      "  Ep 66/100: reward=361.00 steps=361\n",
      "  Ep 67/100: reward=395.00 steps=395\n",
      "  Ep 68/100: reward=392.00 steps=392\n",
      "  Ep 69/100: reward=354.00 steps=354\n",
      "  Ep 70/100: reward=287.00 steps=287\n",
      "  Ep 71/100: reward=300.00 steps=300\n",
      "  Ep 72/100: reward=372.00 steps=372\n",
      "  Ep 73/100: reward=290.00 steps=290\n",
      "  Ep 74/100: reward=315.00 steps=315\n",
      "  Ep 75/100: reward=343.00 steps=343\n",
      "  Ep 76/100: reward=323.00 steps=323\n",
      "  Ep 77/100: reward=298.00 steps=298\n",
      "  Ep 78/100: reward=343.00 steps=343\n",
      "  Ep 79/100: reward=302.00 steps=302\n",
      "  Ep 80/100: reward=302.00 steps=302\n",
      "  Ep 81/100: reward=306.00 steps=306\n",
      "  Ep 82/100: reward=309.00 steps=309\n",
      "  Ep 83/100: reward=299.00 steps=299\n",
      "  Ep 84/100: reward=314.00 steps=314\n",
      "  Ep 85/100: reward=305.00 steps=305\n",
      "  Ep 86/100: reward=381.00 steps=381\n",
      "  Ep 87/100: reward=361.00 steps=361\n",
      "  Ep 88/100: reward=310.00 steps=310\n",
      "  Ep 89/100: reward=298.00 steps=298\n",
      "  Ep 90/100: reward=304.00 steps=304\n",
      "  Ep 91/100: reward=301.00 steps=301\n",
      "  Ep 92/100: reward=363.00 steps=363\n",
      "  Ep 93/100: reward=274.00 steps=274\n",
      "  Ep 94/100: reward=322.00 steps=322\n",
      "  Ep 95/100: reward=298.00 steps=298\n",
      "  Ep 96/100: reward=317.00 steps=317\n",
      "  Ep 97/100: reward=315.00 steps=315\n",
      "  Ep 98/100: reward=327.00 steps=327\n",
      "  Ep 99/100: reward=317.00 steps=317\n",
      "  Ep 100/100: reward=312.00 steps=312\n",
      "Finished evaluations. avg=333.15 best=409.00 worst=274.00\n",
      "Recording random episode index 36 for model CartPole_DQN_gamma-0.5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_gamma-0.5 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_gamma-0.5\\CartPole_DQN_gamma-0.5-episode-0.mp4\n",
      "Recording complete. Total reward: 298.00, Steps:298, Duration: 0.92 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▅▃▂▄▃▇▃▄▇▇▄▂▄▃▆▄▃▇█▂▅▆▃▅▇▁▃▅▅▁▅▇▂▁▂▁▂▂▂▃</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▆▃▄▃▃▄▄▃▄▃▄▆▄▅▃▂▆▄█▄▄▆▄▇▅▆▂▃▅▄▂▂▆▃▂▁▂▃▄▃</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>333.15</td></tr><tr><td>best_reward</td><td>409</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>312</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.91631</td></tr><tr><td>recorded_episode_index</td><td>36</td></tr><tr><td>recorded_episode_reward</td><td>298</td></tr><tr><td>recorded_episode_steps</td><td>298</td></tr><tr><td>steps</td><td>312</td></tr><tr><td>worst_reward</td><td>274</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_gamma-0.5_1763011302</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/3d3ldmbo' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/3d3ldmbo</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072142-3d3ldmbo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_gamma-0.9.pth ===\n",
      "W&B run: test_CartPole_DQN_gamma-0.9_1763011309\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072149-56aswvzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/56aswvzu' target=\"_blank\">test_CartPole_DQN_gamma-0.9_1763011309</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/56aswvzu' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/56aswvzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=180.00 steps=180\n",
      "  Ep 2/100: reward=202.00 steps=202\n",
      "  Ep 3/100: reward=180.00 steps=180\n",
      "  Ep 4/100: reward=155.00 steps=155\n",
      "  Ep 5/100: reward=149.00 steps=149\n",
      "  Ep 6/100: reward=194.00 steps=194\n",
      "  Ep 7/100: reward=142.00 steps=142\n",
      "  Ep 8/100: reward=212.00 steps=212\n",
      "  Ep 9/100: reward=163.00 steps=163\n",
      "  Ep 10/100: reward=185.00 steps=185\n",
      "  Ep 11/100: reward=157.00 steps=157\n",
      "  Ep 12/100: reward=148.00 steps=148\n",
      "  Ep 13/100: reward=179.00 steps=179\n",
      "  Ep 14/100: reward=259.00 steps=259\n",
      "  Ep 15/100: reward=160.00 steps=160\n",
      "  Ep 16/100: reward=375.00 steps=375\n",
      "  Ep 17/100: reward=168.00 steps=168\n",
      "  Ep 18/100: reward=148.00 steps=148\n",
      "  Ep 19/100: reward=184.00 steps=184\n",
      "  Ep 20/100: reward=201.00 steps=201\n",
      "  Ep 21/100: reward=223.00 steps=223\n",
      "  Ep 22/100: reward=182.00 steps=182\n",
      "  Ep 23/100: reward=164.00 steps=164\n",
      "  Ep 24/100: reward=197.00 steps=197\n",
      "  Ep 25/100: reward=277.00 steps=277\n",
      "  Ep 26/100: reward=185.00 steps=185\n",
      "  Ep 27/100: reward=176.00 steps=176\n",
      "  Ep 28/100: reward=183.00 steps=183\n",
      "  Ep 29/100: reward=153.00 steps=153\n",
      "  Ep 30/100: reward=154.00 steps=154\n",
      "  Ep 31/100: reward=184.00 steps=184\n",
      "  Ep 32/100: reward=184.00 steps=184\n",
      "  Ep 33/100: reward=182.00 steps=182\n",
      "  Ep 34/100: reward=176.00 steps=176\n",
      "  Ep 35/100: reward=161.00 steps=161\n",
      "  Ep 36/100: reward=160.00 steps=160\n",
      "  Ep 37/100: reward=169.00 steps=169\n",
      "  Ep 38/100: reward=198.00 steps=198\n",
      "  Ep 39/100: reward=195.00 steps=195\n",
      "  Ep 40/100: reward=239.00 steps=239\n",
      "  Ep 41/100: reward=177.00 steps=177\n",
      "  Ep 42/100: reward=169.00 steps=169\n",
      "  Ep 43/100: reward=173.00 steps=173\n",
      "  Ep 44/100: reward=195.00 steps=195\n",
      "  Ep 45/100: reward=167.00 steps=167\n",
      "  Ep 46/100: reward=188.00 steps=188\n",
      "  Ep 47/100: reward=162.00 steps=162\n",
      "  Ep 48/100: reward=183.00 steps=183\n",
      "  Ep 49/100: reward=179.00 steps=179\n",
      "  Ep 50/100: reward=141.00 steps=141\n",
      "  Ep 51/100: reward=178.00 steps=178\n",
      "  Ep 52/100: reward=311.00 steps=311\n",
      "  Ep 53/100: reward=242.00 steps=242\n",
      "  Ep 54/100: reward=157.00 steps=157\n",
      "  Ep 55/100: reward=191.00 steps=191\n",
      "  Ep 56/100: reward=178.00 steps=178\n",
      "  Ep 57/100: reward=155.00 steps=155\n",
      "  Ep 58/100: reward=247.00 steps=247\n",
      "  Ep 59/100: reward=164.00 steps=164\n",
      "  Ep 60/100: reward=192.00 steps=192\n",
      "  Ep 61/100: reward=172.00 steps=172\n",
      "  Ep 62/100: reward=175.00 steps=175\n",
      "  Ep 63/100: reward=156.00 steps=156\n",
      "  Ep 64/100: reward=180.00 steps=180\n",
      "  Ep 65/100: reward=169.00 steps=169\n",
      "  Ep 66/100: reward=215.00 steps=215\n",
      "  Ep 67/100: reward=191.00 steps=191\n",
      "  Ep 68/100: reward=172.00 steps=172\n",
      "  Ep 69/100: reward=162.00 steps=162\n",
      "  Ep 70/100: reward=185.00 steps=185\n",
      "  Ep 71/100: reward=193.00 steps=193\n",
      "  Ep 72/100: reward=206.00 steps=206\n",
      "  Ep 73/100: reward=196.00 steps=196\n",
      "  Ep 74/100: reward=152.00 steps=152\n",
      "  Ep 75/100: reward=161.00 steps=161\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=208.00 steps=208\n",
      "  Ep 78/100: reward=203.00 steps=203\n",
      "  Ep 79/100: reward=163.00 steps=163\n",
      "  Ep 80/100: reward=147.00 steps=147\n",
      "  Ep 81/100: reward=183.00 steps=183\n",
      "  Ep 82/100: reward=194.00 steps=194\n",
      "  Ep 83/100: reward=202.00 steps=202\n",
      "  Ep 84/100: reward=191.00 steps=191\n",
      "  Ep 85/100: reward=238.00 steps=238\n",
      "  Ep 86/100: reward=181.00 steps=181\n",
      "  Ep 87/100: reward=227.00 steps=227\n",
      "  Ep 88/100: reward=250.00 steps=250\n",
      "  Ep 89/100: reward=202.00 steps=202\n",
      "  Ep 90/100: reward=198.00 steps=198\n",
      "  Ep 91/100: reward=190.00 steps=190\n",
      "  Ep 92/100: reward=220.00 steps=220\n",
      "  Ep 93/100: reward=164.00 steps=164\n",
      "  Ep 94/100: reward=181.00 steps=181\n",
      "  Ep 95/100: reward=179.00 steps=179\n",
      "  Ep 96/100: reward=158.00 steps=158\n",
      "  Ep 97/100: reward=144.00 steps=144\n",
      "  Ep 98/100: reward=175.00 steps=175\n",
      "  Ep 99/100: reward=181.00 steps=181\n",
      "  Ep 100/100: reward=250.00 steps=250\n",
      "Finished evaluations. avg=190.46 best=500.00 worst=141.00\n",
      "Recording random episode index 9 for model CartPole_DQN_gamma-0.9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_gamma-0.9 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_gamma-0.9\\CartPole_DQN_gamma-0.9-episode-0.mp4\n",
      "Recording complete. Total reward: 174.00, Steps:174, Duration: 0.63 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▂▂▂▁▂▁▁▃▁▂▂▁▂▂▂▂▂▂▂▁▁▂▂▁▃▂▂▁▂▂▂▂█▂▂▃▂▂▂▂</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▁▂▂▃▂▃▁▄▁▂▂▂▂▂▂▂▁▂▄▃▁▂▁▂▂▁▂▂▂█▁▂▂▃▂▃▁▂▃</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>190.46</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>250</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.63222</td></tr><tr><td>recorded_episode_index</td><td>9</td></tr><tr><td>recorded_episode_reward</td><td>174</td></tr><tr><td>recorded_episode_steps</td><td>174</td></tr><tr><td>steps</td><td>250</td></tr><tr><td>worst_reward</td><td>141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_gamma-0.9_1763011309</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/56aswvzu' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/56aswvzu</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072149-56aswvzu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_gamma-0.99.pth ===\n",
      "W&B run: test_CartPole_DQN_gamma-0.99_1763011315\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072155-2qfshpiz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2qfshpiz' target=\"_blank\">test_CartPole_DQN_gamma-0.99_1763011315</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2qfshpiz' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2qfshpiz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 53 for model CartPole_DQN_gamma-0.99.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_gamma-0.99 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_gamma-0.99\\CartPole_DQN_gamma-0.99-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.47 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.4723</td></tr><tr><td>recorded_episode_index</td><td>53</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_gamma-0.99_1763011315</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2qfshpiz' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2qfshpiz</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072155-2qfshpiz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_lr-0.0001.pth ===\n",
      "W&B run: test_CartPole_DQN_lr-0.0001_1763011325\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072205-0o7lzn58</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0o7lzn58' target=\"_blank\">test_CartPole_DQN_lr-0.0001_1763011325</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0o7lzn58' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0o7lzn58</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=63.00 steps=63\n",
      "  Ep 2/100: reward=66.00 steps=66\n",
      "  Ep 3/100: reward=64.00 steps=64\n",
      "  Ep 4/100: reward=62.00 steps=62\n",
      "  Ep 5/100: reward=70.00 steps=70\n",
      "  Ep 6/100: reward=63.00 steps=63\n",
      "  Ep 7/100: reward=58.00 steps=58\n",
      "  Ep 8/100: reward=76.00 steps=76\n",
      "  Ep 9/100: reward=61.00 steps=61\n",
      "  Ep 10/100: reward=65.00 steps=65\n",
      "  Ep 11/100: reward=66.00 steps=66\n",
      "  Ep 12/100: reward=70.00 steps=70\n",
      "  Ep 13/100: reward=74.00 steps=74\n",
      "  Ep 14/100: reward=68.00 steps=68\n",
      "  Ep 15/100: reward=70.00 steps=70\n",
      "  Ep 16/100: reward=58.00 steps=58\n",
      "  Ep 17/100: reward=62.00 steps=62\n",
      "  Ep 18/100: reward=64.00 steps=64\n",
      "  Ep 19/100: reward=56.00 steps=56\n",
      "  Ep 20/100: reward=68.00 steps=68\n",
      "  Ep 21/100: reward=66.00 steps=66\n",
      "  Ep 22/100: reward=70.00 steps=70\n",
      "  Ep 23/100: reward=64.00 steps=64\n",
      "  Ep 24/100: reward=64.00 steps=64\n",
      "  Ep 25/100: reward=68.00 steps=68\n",
      "  Ep 26/100: reward=71.00 steps=71\n",
      "  Ep 27/100: reward=67.00 steps=67\n",
      "  Ep 28/100: reward=56.00 steps=56\n",
      "  Ep 29/100: reward=62.00 steps=62\n",
      "  Ep 30/100: reward=64.00 steps=64\n",
      "  Ep 31/100: reward=75.00 steps=75\n",
      "  Ep 32/100: reward=66.00 steps=66\n",
      "  Ep 33/100: reward=64.00 steps=64\n",
      "  Ep 34/100: reward=58.00 steps=58\n",
      "  Ep 35/100: reward=75.00 steps=75\n",
      "  Ep 36/100: reward=60.00 steps=60\n",
      "  Ep 37/100: reward=62.00 steps=62\n",
      "  Ep 38/100: reward=72.00 steps=72\n",
      "  Ep 39/100: reward=66.00 steps=66\n",
      "  Ep 40/100: reward=66.00 steps=66\n",
      "  Ep 41/100: reward=64.00 steps=64\n",
      "  Ep 42/100: reward=64.00 steps=64\n",
      "  Ep 43/100: reward=76.00 steps=76\n",
      "  Ep 44/100: reward=64.00 steps=64\n",
      "  Ep 45/100: reward=65.00 steps=65\n",
      "  Ep 46/100: reward=72.00 steps=72\n",
      "  Ep 47/100: reward=62.00 steps=62\n",
      "  Ep 48/100: reward=72.00 steps=72\n",
      "  Ep 49/100: reward=72.00 steps=72\n",
      "  Ep 50/100: reward=64.00 steps=64\n",
      "  Ep 51/100: reward=72.00 steps=72\n",
      "  Ep 52/100: reward=65.00 steps=65\n",
      "  Ep 53/100: reward=68.00 steps=68\n",
      "  Ep 54/100: reward=61.00 steps=61\n",
      "  Ep 55/100: reward=59.00 steps=59\n",
      "  Ep 56/100: reward=66.00 steps=66\n",
      "  Ep 57/100: reward=67.00 steps=67\n",
      "  Ep 58/100: reward=77.00 steps=77\n",
      "  Ep 59/100: reward=65.00 steps=65\n",
      "  Ep 60/100: reward=68.00 steps=68\n",
      "  Ep 61/100: reward=64.00 steps=64\n",
      "  Ep 62/100: reward=60.00 steps=60\n",
      "  Ep 63/100: reward=66.00 steps=66\n",
      "  Ep 64/100: reward=64.00 steps=64\n",
      "  Ep 65/100: reward=64.00 steps=64\n",
      "  Ep 66/100: reward=61.00 steps=61\n",
      "  Ep 67/100: reward=68.00 steps=68\n",
      "  Ep 68/100: reward=68.00 steps=68\n",
      "  Ep 69/100: reward=64.00 steps=64\n",
      "  Ep 70/100: reward=71.00 steps=71\n",
      "  Ep 71/100: reward=72.00 steps=72\n",
      "  Ep 72/100: reward=66.00 steps=66\n",
      "  Ep 73/100: reward=58.00 steps=58\n",
      "  Ep 74/100: reward=63.00 steps=63\n",
      "  Ep 75/100: reward=62.00 steps=62\n",
      "  Ep 76/100: reward=66.00 steps=66\n",
      "  Ep 77/100: reward=68.00 steps=68\n",
      "  Ep 78/100: reward=76.00 steps=76\n",
      "  Ep 79/100: reward=64.00 steps=64\n",
      "  Ep 80/100: reward=58.00 steps=58\n",
      "  Ep 81/100: reward=64.00 steps=64\n",
      "  Ep 82/100: reward=64.00 steps=64\n",
      "  Ep 83/100: reward=70.00 steps=70\n",
      "  Ep 84/100: reward=60.00 steps=60\n",
      "  Ep 85/100: reward=69.00 steps=69\n",
      "  Ep 86/100: reward=66.00 steps=66\n",
      "  Ep 87/100: reward=64.00 steps=64\n",
      "  Ep 88/100: reward=74.00 steps=74\n",
      "  Ep 89/100: reward=63.00 steps=63\n",
      "  Ep 90/100: reward=63.00 steps=63\n",
      "  Ep 91/100: reward=60.00 steps=60\n",
      "  Ep 92/100: reward=65.00 steps=65\n",
      "  Ep 93/100: reward=61.00 steps=61\n",
      "  Ep 94/100: reward=61.00 steps=61\n",
      "  Ep 95/100: reward=61.00 steps=61\n",
      "  Ep 96/100: reward=70.00 steps=70\n",
      "  Ep 97/100: reward=60.00 steps=60\n",
      "  Ep 98/100: reward=75.00 steps=75\n",
      "  Ep 99/100: reward=60.00 steps=60\n",
      "  Ep 100/100: reward=60.00 steps=60\n",
      "Finished evaluations. avg=65.56 best=77.00 worst=56.00\n",
      "Recording random episode index 87 for model CartPole_DQN_lr-0.0001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_lr-0.0001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_lr-0.0001\\CartPole_DQN_lr-0.0001-episode-0.mp4\n",
      "Recording complete. Total reward: 67.00, Steps:67, Duration: 0.39 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▃▃▆▃▄▅▆▃▅▄▅▅▁▃▇▂▂▃▄▆▆▆█▅▄▅▄▂▅▂▆▂▄▄▇▄▃▃▆▂</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▄▄▂▄▇▃▁▄▆▆▁▃▄▇▂▄█▆▆▆█▅▄▄▄▅▄▆▄▃▄▄▆▂▄▃▄▃▆▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>65.56</td></tr><tr><td>best_reward</td><td>77</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>60</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.39326</td></tr><tr><td>recorded_episode_index</td><td>87</td></tr><tr><td>recorded_episode_reward</td><td>67</td></tr><tr><td>recorded_episode_steps</td><td>67</td></tr><tr><td>steps</td><td>60</td></tr><tr><td>worst_reward</td><td>56</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_lr-0.0001_1763011325</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0o7lzn58' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/0o7lzn58</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072205-0o7lzn58\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_lr-0.0003.pth ===\n",
      "W&B run: test_CartPole_DQN_lr-0.0003_1763011329\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072209-4mwhgm6j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4mwhgm6j' target=\"_blank\">test_CartPole_DQN_lr-0.0003_1763011329</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4mwhgm6j' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4mwhgm6j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 71 for model CartPole_DQN_lr-0.0003.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_lr-0.0003 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_lr-0.0003\\CartPole_DQN_lr-0.0003-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.51 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.50607</td></tr><tr><td>recorded_episode_index</td><td>71</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_lr-0.0003_1763011329</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4mwhgm6j' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4mwhgm6j</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072209-4mwhgm6j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_lr-0.001.pth ===\n",
      "W&B run: test_CartPole_DQN_lr-0.001_1763011339\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072219-96c8u90u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/96c8u90u' target=\"_blank\">test_CartPole_DQN_lr-0.001_1763011339</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/96c8u90u' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/96c8u90u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 31 for model CartPole_DQN_lr-0.001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_lr-0.001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_lr-0.001\\CartPole_DQN_lr-0.001-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.46 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.46073</td></tr><tr><td>recorded_episode_index</td><td>31</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_lr-0.001_1763011339</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/96c8u90u' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/96c8u90u</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072219-96c8u90u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_lr-0.01.pth ===\n",
      "W&B run: test_CartPole_DQN_lr-0.01_1763011349\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072229-u39mm1ix</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u39mm1ix' target=\"_blank\">test_CartPole_DQN_lr-0.01_1763011349</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u39mm1ix' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u39mm1ix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 51 for model CartPole_DQN_lr-0.01.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_lr-0.01 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_lr-0.01\\CartPole_DQN_lr-0.01-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.46 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.45815</td></tr><tr><td>recorded_episode_index</td><td>51</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_lr-0.01_1763011349</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u39mm1ix' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u39mm1ix</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072229-u39mm1ix\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_lr-1e-05.pth ===\n",
      "W&B run: test_CartPole_DQN_lr-1e-05_1763011359\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072239-fsu3nvr7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/fsu3nvr7' target=\"_blank\">test_CartPole_DQN_lr-1e-05_1763011359</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/fsu3nvr7' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/fsu3nvr7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=9.00 steps=9\n",
      "  Ep 2/100: reward=8.00 steps=8\n",
      "  Ep 3/100: reward=10.00 steps=10\n",
      "  Ep 4/100: reward=10.00 steps=10\n",
      "  Ep 5/100: reward=10.00 steps=10\n",
      "  Ep 6/100: reward=9.00 steps=9\n",
      "  Ep 7/100: reward=8.00 steps=8\n",
      "  Ep 8/100: reward=9.00 steps=9\n",
      "  Ep 9/100: reward=10.00 steps=10\n",
      "  Ep 10/100: reward=9.00 steps=9\n",
      "  Ep 11/100: reward=9.00 steps=9\n",
      "  Ep 12/100: reward=9.00 steps=9\n",
      "  Ep 13/100: reward=9.00 steps=9\n",
      "  Ep 14/100: reward=9.00 steps=9\n",
      "  Ep 15/100: reward=8.00 steps=8\n",
      "  Ep 16/100: reward=10.00 steps=10\n",
      "  Ep 17/100: reward=9.00 steps=9\n",
      "  Ep 18/100: reward=9.00 steps=9\n",
      "  Ep 19/100: reward=8.00 steps=8\n",
      "  Ep 20/100: reward=10.00 steps=10\n",
      "  Ep 21/100: reward=9.00 steps=9\n",
      "  Ep 22/100: reward=10.00 steps=10\n",
      "  Ep 23/100: reward=9.00 steps=9\n",
      "  Ep 24/100: reward=9.00 steps=9\n",
      "  Ep 25/100: reward=10.00 steps=10\n",
      "  Ep 26/100: reward=9.00 steps=9\n",
      "  Ep 27/100: reward=9.00 steps=9\n",
      "  Ep 28/100: reward=10.00 steps=10\n",
      "  Ep 29/100: reward=9.00 steps=9\n",
      "  Ep 30/100: reward=8.00 steps=8\n",
      "  Ep 31/100: reward=9.00 steps=9\n",
      "  Ep 32/100: reward=10.00 steps=10\n",
      "  Ep 33/100: reward=10.00 steps=10\n",
      "  Ep 34/100: reward=8.00 steps=8\n",
      "  Ep 35/100: reward=10.00 steps=10\n",
      "  Ep 36/100: reward=10.00 steps=10\n",
      "  Ep 37/100: reward=10.00 steps=10\n",
      "  Ep 38/100: reward=10.00 steps=10\n",
      "  Ep 39/100: reward=10.00 steps=10\n",
      "  Ep 40/100: reward=8.00 steps=8\n",
      "  Ep 41/100: reward=8.00 steps=8\n",
      "  Ep 42/100: reward=10.00 steps=10\n",
      "  Ep 43/100: reward=8.00 steps=8\n",
      "  Ep 44/100: reward=10.00 steps=10\n",
      "  Ep 45/100: reward=9.00 steps=9\n",
      "  Ep 46/100: reward=9.00 steps=9\n",
      "  Ep 47/100: reward=10.00 steps=10\n",
      "  Ep 48/100: reward=9.00 steps=9\n",
      "  Ep 49/100: reward=10.00 steps=10\n",
      "  Ep 50/100: reward=11.00 steps=11\n",
      "  Ep 51/100: reward=9.00 steps=9\n",
      "  Ep 52/100: reward=9.00 steps=9\n",
      "  Ep 53/100: reward=9.00 steps=9\n",
      "  Ep 54/100: reward=9.00 steps=9\n",
      "  Ep 55/100: reward=8.00 steps=8\n",
      "  Ep 56/100: reward=9.00 steps=9\n",
      "  Ep 57/100: reward=9.00 steps=9\n",
      "  Ep 58/100: reward=9.00 steps=9\n",
      "  Ep 59/100: reward=9.00 steps=9\n",
      "  Ep 60/100: reward=9.00 steps=9\n",
      "  Ep 61/100: reward=10.00 steps=10\n",
      "  Ep 62/100: reward=10.00 steps=10\n",
      "  Ep 63/100: reward=10.00 steps=10\n",
      "  Ep 64/100: reward=10.00 steps=10\n",
      "  Ep 65/100: reward=11.00 steps=11\n",
      "  Ep 66/100: reward=9.00 steps=9\n",
      "  Ep 67/100: reward=10.00 steps=10\n",
      "  Ep 68/100: reward=8.00 steps=8\n",
      "  Ep 69/100: reward=9.00 steps=9\n",
      "  Ep 70/100: reward=10.00 steps=10\n",
      "  Ep 71/100: reward=10.00 steps=10\n",
      "  Ep 72/100: reward=9.00 steps=9\n",
      "  Ep 73/100: reward=10.00 steps=10\n",
      "  Ep 74/100: reward=9.00 steps=9\n",
      "  Ep 75/100: reward=9.00 steps=9\n",
      "  Ep 76/100: reward=9.00 steps=9\n",
      "  Ep 77/100: reward=10.00 steps=10\n",
      "  Ep 78/100: reward=10.00 steps=10\n",
      "  Ep 79/100: reward=10.00 steps=10\n",
      "  Ep 80/100: reward=10.00 steps=10\n",
      "  Ep 81/100: reward=9.00 steps=9\n",
      "  Ep 82/100: reward=10.00 steps=10\n",
      "  Ep 83/100: reward=10.00 steps=10\n",
      "  Ep 84/100: reward=8.00 steps=8\n",
      "  Ep 85/100: reward=10.00 steps=10\n",
      "  Ep 86/100: reward=9.00 steps=9\n",
      "  Ep 87/100: reward=9.00 steps=9\n",
      "  Ep 88/100: reward=9.00 steps=9\n",
      "  Ep 89/100: reward=10.00 steps=10\n",
      "  Ep 90/100: reward=10.00 steps=10\n",
      "  Ep 91/100: reward=9.00 steps=9\n",
      "  Ep 92/100: reward=10.00 steps=10\n",
      "  Ep 93/100: reward=10.00 steps=10\n",
      "  Ep 94/100: reward=9.00 steps=9\n",
      "  Ep 95/100: reward=9.00 steps=9\n",
      "  Ep 96/100: reward=9.00 steps=9\n",
      "  Ep 97/100: reward=9.00 steps=9\n",
      "  Ep 98/100: reward=9.00 steps=9\n",
      "  Ep 99/100: reward=9.00 steps=9\n",
      "  Ep 100/100: reward=9.00 steps=9\n",
      "Finished evaluations. avg=9.31 best=11.00 worst=8.00\n",
      "Recording random episode index 63 for model CartPole_DQN_lr-1e-05.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_lr-1e-05 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_lr-1e-05\\CartPole_DQN_lr-1e-05-episode-0.mp4\n",
      "Recording complete. Total reward: 10.00, Steps:10, Duration: 0.30 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▁█▅▅▁▅▅██▅█▁█████▁▁██▅▅█▅▁▅█▁█▅▅▅███▅▅▅▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▅█▅▁█▁█▅█▅▅▅▁▅███▅██▅▅▅▁▅▅▅███▅▅▅█▅█▅█▅▅</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>9.31</td></tr><tr><td>best_reward</td><td>11</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>9</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.30329</td></tr><tr><td>recorded_episode_index</td><td>63</td></tr><tr><td>recorded_episode_reward</td><td>10</td></tr><tr><td>recorded_episode_steps</td><td>10</td></tr><tr><td>steps</td><td>9</td></tr><tr><td>worst_reward</td><td>8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_lr-1e-05_1763011359</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/fsu3nvr7' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/fsu3nvr7</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072239-fsu3nvr7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_mem-10000.pth ===\n",
      "W&B run: test_CartPole_DQN_mem-10000_1763011362\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072242-b0jagjep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/b0jagjep' target=\"_blank\">test_CartPole_DQN_mem-10000_1763011362</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/b0jagjep' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/b0jagjep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=500.00 steps=500\n",
      "  Ep 2/100: reward=500.00 steps=500\n",
      "  Ep 3/100: reward=500.00 steps=500\n",
      "  Ep 4/100: reward=500.00 steps=500\n",
      "  Ep 5/100: reward=500.00 steps=500\n",
      "  Ep 6/100: reward=500.00 steps=500\n",
      "  Ep 7/100: reward=500.00 steps=500\n",
      "  Ep 8/100: reward=500.00 steps=500\n",
      "  Ep 9/100: reward=500.00 steps=500\n",
      "  Ep 10/100: reward=500.00 steps=500\n",
      "  Ep 11/100: reward=500.00 steps=500\n",
      "  Ep 12/100: reward=500.00 steps=500\n",
      "  Ep 13/100: reward=500.00 steps=500\n",
      "  Ep 14/100: reward=500.00 steps=500\n",
      "  Ep 15/100: reward=500.00 steps=500\n",
      "  Ep 16/100: reward=500.00 steps=500\n",
      "  Ep 17/100: reward=500.00 steps=500\n",
      "  Ep 18/100: reward=500.00 steps=500\n",
      "  Ep 19/100: reward=500.00 steps=500\n",
      "  Ep 20/100: reward=500.00 steps=500\n",
      "  Ep 21/100: reward=500.00 steps=500\n",
      "  Ep 22/100: reward=500.00 steps=500\n",
      "  Ep 23/100: reward=500.00 steps=500\n",
      "  Ep 24/100: reward=500.00 steps=500\n",
      "  Ep 25/100: reward=500.00 steps=500\n",
      "  Ep 26/100: reward=500.00 steps=500\n",
      "  Ep 27/100: reward=500.00 steps=500\n",
      "  Ep 28/100: reward=500.00 steps=500\n",
      "  Ep 29/100: reward=500.00 steps=500\n",
      "  Ep 30/100: reward=500.00 steps=500\n",
      "  Ep 31/100: reward=500.00 steps=500\n",
      "  Ep 32/100: reward=500.00 steps=500\n",
      "  Ep 33/100: reward=500.00 steps=500\n",
      "  Ep 34/100: reward=500.00 steps=500\n",
      "  Ep 35/100: reward=500.00 steps=500\n",
      "  Ep 36/100: reward=500.00 steps=500\n",
      "  Ep 37/100: reward=500.00 steps=500\n",
      "  Ep 38/100: reward=500.00 steps=500\n",
      "  Ep 39/100: reward=500.00 steps=500\n",
      "  Ep 40/100: reward=500.00 steps=500\n",
      "  Ep 41/100: reward=500.00 steps=500\n",
      "  Ep 42/100: reward=500.00 steps=500\n",
      "  Ep 43/100: reward=500.00 steps=500\n",
      "  Ep 44/100: reward=500.00 steps=500\n",
      "  Ep 45/100: reward=500.00 steps=500\n",
      "  Ep 46/100: reward=500.00 steps=500\n",
      "  Ep 47/100: reward=500.00 steps=500\n",
      "  Ep 48/100: reward=500.00 steps=500\n",
      "  Ep 49/100: reward=500.00 steps=500\n",
      "  Ep 50/100: reward=500.00 steps=500\n",
      "  Ep 51/100: reward=500.00 steps=500\n",
      "  Ep 52/100: reward=500.00 steps=500\n",
      "  Ep 53/100: reward=500.00 steps=500\n",
      "  Ep 54/100: reward=500.00 steps=500\n",
      "  Ep 55/100: reward=500.00 steps=500\n",
      "  Ep 56/100: reward=500.00 steps=500\n",
      "  Ep 57/100: reward=500.00 steps=500\n",
      "  Ep 58/100: reward=500.00 steps=500\n",
      "  Ep 59/100: reward=500.00 steps=500\n",
      "  Ep 60/100: reward=500.00 steps=500\n",
      "  Ep 61/100: reward=500.00 steps=500\n",
      "  Ep 62/100: reward=500.00 steps=500\n",
      "  Ep 63/100: reward=500.00 steps=500\n",
      "  Ep 64/100: reward=500.00 steps=500\n",
      "  Ep 65/100: reward=500.00 steps=500\n",
      "  Ep 66/100: reward=500.00 steps=500\n",
      "  Ep 67/100: reward=500.00 steps=500\n",
      "  Ep 68/100: reward=500.00 steps=500\n",
      "  Ep 69/100: reward=500.00 steps=500\n",
      "  Ep 70/100: reward=500.00 steps=500\n",
      "  Ep 71/100: reward=500.00 steps=500\n",
      "  Ep 72/100: reward=500.00 steps=500\n",
      "  Ep 73/100: reward=500.00 steps=500\n",
      "  Ep 74/100: reward=500.00 steps=500\n",
      "  Ep 75/100: reward=500.00 steps=500\n",
      "  Ep 76/100: reward=500.00 steps=500\n",
      "  Ep 77/100: reward=500.00 steps=500\n",
      "  Ep 78/100: reward=500.00 steps=500\n",
      "  Ep 79/100: reward=500.00 steps=500\n",
      "  Ep 80/100: reward=500.00 steps=500\n",
      "  Ep 81/100: reward=500.00 steps=500\n",
      "  Ep 82/100: reward=500.00 steps=500\n",
      "  Ep 83/100: reward=500.00 steps=500\n",
      "  Ep 84/100: reward=500.00 steps=500\n",
      "  Ep 85/100: reward=500.00 steps=500\n",
      "  Ep 86/100: reward=500.00 steps=500\n",
      "  Ep 87/100: reward=500.00 steps=500\n",
      "  Ep 88/100: reward=500.00 steps=500\n",
      "  Ep 89/100: reward=500.00 steps=500\n",
      "  Ep 90/100: reward=500.00 steps=500\n",
      "  Ep 91/100: reward=500.00 steps=500\n",
      "  Ep 92/100: reward=500.00 steps=500\n",
      "  Ep 93/100: reward=500.00 steps=500\n",
      "  Ep 94/100: reward=500.00 steps=500\n",
      "  Ep 95/100: reward=500.00 steps=500\n",
      "  Ep 96/100: reward=500.00 steps=500\n",
      "  Ep 97/100: reward=500.00 steps=500\n",
      "  Ep 98/100: reward=500.00 steps=500\n",
      "  Ep 99/100: reward=500.00 steps=500\n",
      "  Ep 100/100: reward=500.00 steps=500\n",
      "Finished evaluations. avg=500.00 best=500.00 worst=500.00\n",
      "Recording random episode index 48 for model CartPole_DQN_mem-10000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_mem-10000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_mem-10000\\CartPole_DQN_mem-10000-episode-0.mp4\n",
      "Recording complete. Total reward: 500.00, Steps:500, Duration: 1.56 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>500</td></tr><tr><td>best_reward</td><td>500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.56118</td></tr><tr><td>recorded_episode_index</td><td>48</td></tr><tr><td>recorded_episode_reward</td><td>500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_mem-10000_1763011362</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/b0jagjep' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/b0jagjep</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072242-b0jagjep\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: CartPole_DQN_mem-50000.pth ===\n",
      "W&B run: test_CartPole_DQN_mem-50000_1763011373\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072253-92cyk2rd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/92cyk2rd' target=\"_blank\">test_CartPole_DQN_mem-50000_1763011373</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/92cyk2rd' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/92cyk2rd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=161.00 steps=161\n",
      "  Ep 2/100: reward=166.00 steps=166\n",
      "  Ep 3/100: reward=161.00 steps=161\n",
      "  Ep 4/100: reward=163.00 steps=163\n",
      "  Ep 5/100: reward=167.00 steps=167\n",
      "  Ep 6/100: reward=165.00 steps=165\n",
      "  Ep 7/100: reward=185.00 steps=185\n",
      "  Ep 8/100: reward=172.00 steps=172\n",
      "  Ep 9/100: reward=160.00 steps=160\n",
      "  Ep 10/100: reward=162.00 steps=162\n",
      "  Ep 11/100: reward=177.00 steps=177\n",
      "  Ep 12/100: reward=184.00 steps=184\n",
      "  Ep 13/100: reward=163.00 steps=163\n",
      "  Ep 14/100: reward=179.00 steps=179\n",
      "  Ep 15/100: reward=175.00 steps=175\n",
      "  Ep 16/100: reward=175.00 steps=175\n",
      "  Ep 17/100: reward=167.00 steps=167\n",
      "  Ep 18/100: reward=179.00 steps=179\n",
      "  Ep 19/100: reward=167.00 steps=167\n",
      "  Ep 20/100: reward=182.00 steps=182\n",
      "  Ep 21/100: reward=160.00 steps=160\n",
      "  Ep 22/100: reward=164.00 steps=164\n",
      "  Ep 23/100: reward=181.00 steps=181\n",
      "  Ep 24/100: reward=165.00 steps=165\n",
      "  Ep 25/100: reward=180.00 steps=180\n",
      "  Ep 26/100: reward=180.00 steps=180\n",
      "  Ep 27/100: reward=173.00 steps=173\n",
      "  Ep 28/100: reward=171.00 steps=171\n",
      "  Ep 29/100: reward=166.00 steps=166\n",
      "  Ep 30/100: reward=168.00 steps=168\n",
      "  Ep 31/100: reward=176.00 steps=176\n",
      "  Ep 32/100: reward=167.00 steps=167\n",
      "  Ep 33/100: reward=173.00 steps=173\n",
      "  Ep 34/100: reward=177.00 steps=177\n",
      "  Ep 35/100: reward=169.00 steps=169\n",
      "  Ep 36/100: reward=173.00 steps=173\n",
      "  Ep 37/100: reward=163.00 steps=163\n",
      "  Ep 38/100: reward=186.00 steps=186\n",
      "  Ep 39/100: reward=171.00 steps=171\n",
      "  Ep 40/100: reward=160.00 steps=160\n",
      "  Ep 41/100: reward=158.00 steps=158\n",
      "  Ep 42/100: reward=166.00 steps=166\n",
      "  Ep 43/100: reward=170.00 steps=170\n",
      "  Ep 44/100: reward=165.00 steps=165\n",
      "  Ep 45/100: reward=161.00 steps=161\n",
      "  Ep 46/100: reward=160.00 steps=160\n",
      "  Ep 47/100: reward=160.00 steps=160\n",
      "  Ep 48/100: reward=164.00 steps=164\n",
      "  Ep 49/100: reward=173.00 steps=173\n",
      "  Ep 50/100: reward=162.00 steps=162\n",
      "  Ep 51/100: reward=159.00 steps=159\n",
      "  Ep 52/100: reward=173.00 steps=173\n",
      "  Ep 53/100: reward=164.00 steps=164\n",
      "  Ep 54/100: reward=158.00 steps=158\n",
      "  Ep 55/100: reward=160.00 steps=160\n",
      "  Ep 56/100: reward=160.00 steps=160\n",
      "  Ep 57/100: reward=174.00 steps=174\n",
      "  Ep 58/100: reward=175.00 steps=175\n",
      "  Ep 59/100: reward=168.00 steps=168\n",
      "  Ep 60/100: reward=165.00 steps=165\n",
      "  Ep 61/100: reward=174.00 steps=174\n",
      "  Ep 62/100: reward=160.00 steps=160\n",
      "  Ep 63/100: reward=176.00 steps=176\n",
      "  Ep 64/100: reward=167.00 steps=167\n",
      "  Ep 65/100: reward=164.00 steps=164\n",
      "  Ep 66/100: reward=159.00 steps=159\n",
      "  Ep 67/100: reward=159.00 steps=159\n",
      "  Ep 68/100: reward=178.00 steps=178\n",
      "  Ep 69/100: reward=174.00 steps=174\n",
      "  Ep 70/100: reward=163.00 steps=163\n",
      "  Ep 71/100: reward=164.00 steps=164\n",
      "  Ep 72/100: reward=167.00 steps=167\n",
      "  Ep 73/100: reward=174.00 steps=174\n",
      "  Ep 74/100: reward=177.00 steps=177\n",
      "  Ep 75/100: reward=160.00 steps=160\n",
      "  Ep 76/100: reward=160.00 steps=160\n",
      "  Ep 77/100: reward=176.00 steps=176\n",
      "  Ep 78/100: reward=170.00 steps=170\n",
      "  Ep 79/100: reward=158.00 steps=158\n",
      "  Ep 80/100: reward=185.00 steps=185\n",
      "  Ep 81/100: reward=160.00 steps=160\n",
      "  Ep 82/100: reward=175.00 steps=175\n",
      "  Ep 83/100: reward=173.00 steps=173\n",
      "  Ep 84/100: reward=184.00 steps=184\n",
      "  Ep 85/100: reward=158.00 steps=158\n",
      "  Ep 86/100: reward=183.00 steps=183\n",
      "  Ep 87/100: reward=180.00 steps=180\n",
      "  Ep 88/100: reward=177.00 steps=177\n",
      "  Ep 89/100: reward=167.00 steps=167\n",
      "  Ep 90/100: reward=185.00 steps=185\n",
      "  Ep 91/100: reward=167.00 steps=167\n",
      "  Ep 92/100: reward=175.00 steps=175\n",
      "  Ep 93/100: reward=168.00 steps=168\n",
      "  Ep 94/100: reward=171.00 steps=171\n",
      "  Ep 95/100: reward=160.00 steps=160\n",
      "  Ep 96/100: reward=168.00 steps=168\n",
      "  Ep 97/100: reward=171.00 steps=171\n",
      "  Ep 98/100: reward=160.00 steps=160\n",
      "  Ep 99/100: reward=158.00 steps=158\n",
      "  Ep 100/100: reward=171.00 steps=171\n",
      "Finished evaluations. avg=169.04 best=186.00 worst=158.00\n",
      "Recording random episode index 59 for model CartPole_DQN_mem-50000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\CartPole-v1\\CartPole_DQN_mem-50000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/CartPole-v1/CartPole_DQN_mem-50000\\CartPole_DQN_mem-50000-episode-0.mp4\n",
      "Recording complete. Total reward: 174.00, Steps:174, Duration: 0.69 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▃▃▅▂▆▆▃█▁▂▇▅▄▅▂▅▁▂▁▂▂▁▅▅▆▁▇▂▃▅▁█▆▃▃▄▅▁▅▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▃▂▃▃█▂▆█▆▅▃▇▃▄▅▄▃▂▂▃▃▂▄▅▂▃▁▁▅▃▂▂▄▂▅▇▄▂▄▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>169.04</td></tr><tr><td>best_reward</td><td>186</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>171</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.69171</td></tr><tr><td>recorded_episode_index</td><td>59</td></tr><tr><td>recorded_episode_reward</td><td>174</td></tr><tr><td>recorded_episode_steps</td><td>174</td></tr><tr><td>steps</td><td>171</td></tr><tr><td>worst_reward</td><td>158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_CartPole_DQN_mem-50000_1763011373</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/92cyk2rd' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/92cyk2rd</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072253-92cyk2rd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_models_folder_with_wandb(\n",
    "    env_name=\"CartPole-v1\",\n",
    "    num_discrete_actions=5,\n",
    "    models_folder=\"./cartPole models\",\n",
    "    num_tests=100,\n",
    "    wandb_project=\"RL A2 tests\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fe48d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_eps-1000.pth ===\n",
      "W&B run: test_Pendulum_DQN_eps-1000_1763011379\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072259-k9u2fmv9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/k9u2fmv9' target=\"_blank\">test_Pendulum_DQN_eps-1000_1763011379</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/k9u2fmv9' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/k9u2fmv9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-1.60 steps=200\n",
      "  Ep 2/100: reward=-312.99 steps=200\n",
      "  Ep 3/100: reward=-0.76 steps=200\n",
      "  Ep 4/100: reward=-222.63 steps=200\n",
      "  Ep 5/100: reward=-2.53 steps=200\n",
      "  Ep 6/100: reward=-345.55 steps=200\n",
      "  Ep 7/100: reward=-126.08 steps=200\n",
      "  Ep 8/100: reward=-126.07 steps=200\n",
      "  Ep 9/100: reward=-123.12 steps=200\n",
      "  Ep 10/100: reward=-125.10 steps=200\n",
      "  Ep 11/100: reward=-120.84 steps=200\n",
      "  Ep 12/100: reward=-236.27 steps=200\n",
      "  Ep 13/100: reward=-126.25 steps=200\n",
      "  Ep 14/100: reward=-237.72 steps=200\n",
      "  Ep 15/100: reward=-118.20 steps=200\n",
      "  Ep 16/100: reward=-124.75 steps=200\n",
      "  Ep 17/100: reward=-0.91 steps=200\n",
      "  Ep 18/100: reward=-118.84 steps=200\n",
      "  Ep 19/100: reward=-117.49 steps=200\n",
      "  Ep 20/100: reward=-231.51 steps=200\n",
      "  Ep 21/100: reward=-227.53 steps=200\n",
      "  Ep 22/100: reward=-3.25 steps=200\n",
      "  Ep 23/100: reward=-0.93 steps=200\n",
      "  Ep 24/100: reward=-228.46 steps=200\n",
      "  Ep 25/100: reward=-124.38 steps=200\n",
      "  Ep 26/100: reward=-4.55 steps=200\n",
      "  Ep 27/100: reward=-120.06 steps=200\n",
      "  Ep 28/100: reward=-1.14 steps=200\n",
      "  Ep 29/100: reward=-114.99 steps=200\n",
      "  Ep 30/100: reward=-125.45 steps=200\n",
      "  Ep 31/100: reward=-122.39 steps=200\n",
      "  Ep 32/100: reward=-122.48 steps=200\n",
      "  Ep 33/100: reward=-232.49 steps=200\n",
      "  Ep 34/100: reward=-224.25 steps=200\n",
      "  Ep 35/100: reward=-240.91 steps=200\n",
      "  Ep 36/100: reward=-122.58 steps=200\n",
      "  Ep 37/100: reward=-120.37 steps=200\n",
      "  Ep 38/100: reward=-122.22 steps=200\n",
      "  Ep 39/100: reward=-238.40 steps=200\n",
      "  Ep 40/100: reward=-1.77 steps=200\n",
      "  Ep 41/100: reward=-230.33 steps=200\n",
      "  Ep 42/100: reward=-1.13 steps=200\n",
      "  Ep 43/100: reward=-125.37 steps=200\n",
      "  Ep 44/100: reward=-228.69 steps=200\n",
      "  Ep 45/100: reward=-122.56 steps=200\n",
      "  Ep 46/100: reward=-127.10 steps=200\n",
      "  Ep 47/100: reward=-125.51 steps=200\n",
      "  Ep 48/100: reward=-124.22 steps=200\n",
      "  Ep 49/100: reward=-123.42 steps=200\n",
      "  Ep 50/100: reward=-235.42 steps=200\n",
      "  Ep 51/100: reward=-123.45 steps=200\n",
      "  Ep 52/100: reward=-123.84 steps=200\n",
      "  Ep 53/100: reward=-122.92 steps=200\n",
      "  Ep 54/100: reward=-234.14 steps=200\n",
      "  Ep 55/100: reward=-2.31 steps=200\n",
      "  Ep 56/100: reward=-277.38 steps=200\n",
      "  Ep 57/100: reward=-0.90 steps=200\n",
      "  Ep 58/100: reward=-223.71 steps=200\n",
      "  Ep 59/100: reward=-1.31 steps=200\n",
      "  Ep 60/100: reward=-125.18 steps=200\n",
      "  Ep 61/100: reward=-226.15 steps=200\n",
      "  Ep 62/100: reward=-120.00 steps=200\n",
      "  Ep 63/100: reward=-333.62 steps=200\n",
      "  Ep 64/100: reward=-121.71 steps=200\n",
      "  Ep 65/100: reward=-118.93 steps=200\n",
      "  Ep 66/100: reward=-121.19 steps=200\n",
      "  Ep 67/100: reward=-0.87 steps=200\n",
      "  Ep 68/100: reward=-1.16 steps=200\n",
      "  Ep 69/100: reward=-133.11 steps=200\n",
      "  Ep 70/100: reward=-119.29 steps=200\n",
      "  Ep 71/100: reward=-123.66 steps=200\n",
      "  Ep 72/100: reward=-325.98 steps=200\n",
      "  Ep 73/100: reward=-228.35 steps=200\n",
      "  Ep 74/100: reward=-2.55 steps=200\n",
      "  Ep 75/100: reward=-124.62 steps=200\n",
      "  Ep 76/100: reward=-128.78 steps=200\n",
      "  Ep 77/100: reward=-1.27 steps=200\n",
      "  Ep 78/100: reward=-313.83 steps=200\n",
      "  Ep 79/100: reward=-121.92 steps=200\n",
      "  Ep 80/100: reward=-1.76 steps=200\n",
      "  Ep 81/100: reward=-126.20 steps=200\n",
      "  Ep 82/100: reward=-117.65 steps=200\n",
      "  Ep 83/100: reward=-339.85 steps=200\n",
      "  Ep 84/100: reward=-1.89 steps=200\n",
      "  Ep 85/100: reward=-122.77 steps=200\n",
      "  Ep 86/100: reward=-5.39 steps=200\n",
      "  Ep 87/100: reward=-236.97 steps=200\n",
      "  Ep 88/100: reward=-347.79 steps=200\n",
      "  Ep 89/100: reward=-117.33 steps=200\n",
      "  Ep 90/100: reward=-122.57 steps=200\n",
      "  Ep 91/100: reward=-124.45 steps=200\n",
      "  Ep 92/100: reward=-1.32 steps=200\n",
      "  Ep 93/100: reward=-5.66 steps=200\n",
      "  Ep 94/100: reward=-226.86 steps=200\n",
      "  Ep 95/100: reward=-114.43 steps=200\n",
      "  Ep 96/100: reward=-0.76 steps=200\n",
      "  Ep 97/100: reward=-122.17 steps=200\n",
      "  Ep 98/100: reward=-114.48 steps=200\n",
      "  Ep 99/100: reward=-118.15 steps=200\n",
      "  Ep 100/100: reward=-118.82 steps=200\n",
      "Finished evaluations. avg=-131.51 best=-0.76 worst=-347.79\n",
      "Recording random episode index 98 for model Pendulum_DQN_eps-1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_eps-1000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_eps-1000\\Pendulum_DQN_eps-1000-episode-0.mp4\n",
      "Recording complete. Total reward: -122.07, Steps:200, Duration: 1.03 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>█▂▄▁▆▃▆▃██▆▅▆██▃▆▆▆█▃▆▁▆▆▅▁▅▂▆▆▁█▆▁▆▃▆█▆</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-131.50932</td></tr><tr><td>best_reward</td><td>-0.75789</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-118.82051</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.02837</td></tr><tr><td>recorded_episode_index</td><td>98</td></tr><tr><td>recorded_episode_reward</td><td>-122.06727</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-347.79408</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_eps-1000_1763011379</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/k9u2fmv9' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/k9u2fmv9</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072259-k9u2fmv9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_eps-5000.pth ===\n",
      "W&B run: test_Pendulum_DQN_eps-5000_1763011385\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072305-n5eq1pu3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/n5eq1pu3' target=\"_blank\">test_Pendulum_DQN_eps-5000_1763011385</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/n5eq1pu3' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/n5eq1pu3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-117.41 steps=200\n",
      "  Ep 2/100: reward=-115.41 steps=200\n",
      "  Ep 3/100: reward=-120.31 steps=200\n",
      "  Ep 4/100: reward=-219.22 steps=200\n",
      "  Ep 5/100: reward=-116.80 steps=200\n",
      "  Ep 6/100: reward=-114.43 steps=200\n",
      "  Ep 7/100: reward=-124.56 steps=200\n",
      "  Ep 8/100: reward=-120.66 steps=200\n",
      "  Ep 9/100: reward=-125.02 steps=200\n",
      "  Ep 10/100: reward=-324.71 steps=200\n",
      "  Ep 11/100: reward=-128.77 steps=200\n",
      "  Ep 12/100: reward=-121.92 steps=200\n",
      "  Ep 13/100: reward=-308.99 steps=200\n",
      "  Ep 14/100: reward=-232.23 steps=200\n",
      "  Ep 15/100: reward=-123.69 steps=200\n",
      "  Ep 16/100: reward=-229.12 steps=200\n",
      "  Ep 17/100: reward=-126.13 steps=200\n",
      "  Ep 18/100: reward=-119.64 steps=200\n",
      "  Ep 19/100: reward=-279.28 steps=200\n",
      "  Ep 20/100: reward=-122.19 steps=200\n",
      "  Ep 21/100: reward=-118.70 steps=200\n",
      "  Ep 22/100: reward=-1.09 steps=200\n",
      "  Ep 23/100: reward=-119.16 steps=200\n",
      "  Ep 24/100: reward=-0.92 steps=200\n",
      "  Ep 25/100: reward=-0.89 steps=200\n",
      "  Ep 26/100: reward=-123.31 steps=200\n",
      "  Ep 27/100: reward=-114.52 steps=200\n",
      "  Ep 28/100: reward=-120.60 steps=200\n",
      "  Ep 29/100: reward=-314.61 steps=200\n",
      "  Ep 30/100: reward=-119.28 steps=200\n",
      "  Ep 31/100: reward=-127.70 steps=200\n",
      "  Ep 32/100: reward=-217.94 steps=200\n",
      "  Ep 33/100: reward=-225.10 steps=200\n",
      "  Ep 34/100: reward=-126.86 steps=200\n",
      "  Ep 35/100: reward=-123.96 steps=200\n",
      "  Ep 36/100: reward=-123.02 steps=200\n",
      "  Ep 37/100: reward=-118.23 steps=200\n",
      "  Ep 38/100: reward=-124.76 steps=200\n",
      "  Ep 39/100: reward=-118.13 steps=200\n",
      "  Ep 40/100: reward=-224.61 steps=200\n",
      "  Ep 41/100: reward=-322.68 steps=200\n",
      "  Ep 42/100: reward=-126.26 steps=200\n",
      "  Ep 43/100: reward=-124.29 steps=200\n",
      "  Ep 44/100: reward=-118.47 steps=200\n",
      "  Ep 45/100: reward=-123.99 steps=200\n",
      "  Ep 46/100: reward=-225.02 steps=200\n",
      "  Ep 47/100: reward=-229.53 steps=200\n",
      "  Ep 48/100: reward=-233.13 steps=200\n",
      "  Ep 49/100: reward=-119.37 steps=200\n",
      "  Ep 50/100: reward=-114.33 steps=200\n",
      "  Ep 51/100: reward=-235.89 steps=200\n",
      "  Ep 52/100: reward=-233.08 steps=200\n",
      "  Ep 53/100: reward=-119.03 steps=200\n",
      "  Ep 54/100: reward=-226.45 steps=200\n",
      "  Ep 55/100: reward=-316.83 steps=200\n",
      "  Ep 56/100: reward=-0.79 steps=200\n",
      "  Ep 57/100: reward=-123.91 steps=200\n",
      "  Ep 58/100: reward=-124.34 steps=200\n",
      "  Ep 59/100: reward=-126.89 steps=200\n",
      "  Ep 60/100: reward=-1.49 steps=200\n",
      "  Ep 61/100: reward=-224.37 steps=200\n",
      "  Ep 62/100: reward=-117.95 steps=200\n",
      "  Ep 63/100: reward=-1.02 steps=200\n",
      "  Ep 64/100: reward=-116.65 steps=200\n",
      "  Ep 65/100: reward=-125.86 steps=200\n",
      "  Ep 66/100: reward=-119.03 steps=200\n",
      "  Ep 67/100: reward=-1.04 steps=200\n",
      "  Ep 68/100: reward=-124.87 steps=200\n",
      "  Ep 69/100: reward=-3.54 steps=200\n",
      "  Ep 70/100: reward=-125.83 steps=200\n",
      "  Ep 71/100: reward=-116.36 steps=200\n",
      "  Ep 72/100: reward=-117.32 steps=200\n",
      "  Ep 73/100: reward=-283.45 steps=200\n",
      "  Ep 74/100: reward=-2.82 steps=200\n",
      "  Ep 75/100: reward=-117.37 steps=200\n",
      "  Ep 76/100: reward=-122.27 steps=200\n",
      "  Ep 77/100: reward=-250.51 steps=200\n",
      "  Ep 78/100: reward=-127.14 steps=200\n",
      "  Ep 79/100: reward=-118.67 steps=200\n",
      "  Ep 80/100: reward=-123.26 steps=200\n",
      "  Ep 81/100: reward=-122.48 steps=200\n",
      "  Ep 82/100: reward=-1.07 steps=200\n",
      "  Ep 83/100: reward=-125.45 steps=200\n",
      "  Ep 84/100: reward=-118.42 steps=200\n",
      "  Ep 85/100: reward=-114.64 steps=200\n",
      "  Ep 86/100: reward=-116.26 steps=200\n",
      "  Ep 87/100: reward=-116.69 steps=200\n",
      "  Ep 88/100: reward=-115.73 steps=200\n",
      "  Ep 89/100: reward=-118.63 steps=200\n",
      "  Ep 90/100: reward=-123.56 steps=200\n",
      "  Ep 91/100: reward=-120.23 steps=200\n",
      "  Ep 92/100: reward=-319.20 steps=200\n",
      "  Ep 93/100: reward=-122.11 steps=200\n",
      "  Ep 94/100: reward=-118.32 steps=200\n",
      "  Ep 95/100: reward=-220.21 steps=200\n",
      "  Ep 96/100: reward=-119.57 steps=200\n",
      "  Ep 97/100: reward=-1.43 steps=200\n",
      "  Ep 98/100: reward=-126.27 steps=200\n",
      "  Ep 99/100: reward=-124.52 steps=200\n",
      "  Ep 100/100: reward=-116.92 steps=200\n",
      "Finished evaluations. avg=-139.01 best=-0.79 worst=-324.71\n",
      "Recording random episode index 1 for model Pendulum_DQN_eps-5000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_eps-5000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_eps-5000\\Pendulum_DQN_eps-5000-episode-0.mp4\n",
      "Recording complete. Total reward: -118.60, Steps:200, Duration: 0.96 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇█████</td></tr><tr><td>episode_reward</td><td>▅▃▅▆▅▅▅▅▅█▅▃▅▅▅▅▅▅▃▃▆▃█▅██▅▅███▅▅▅▅▁▅▅█▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-139.00687</td></tr><tr><td>best_reward</td><td>-0.78638</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-116.91779</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.96006</td></tr><tr><td>recorded_episode_index</td><td>1</td></tr><tr><td>recorded_episode_reward</td><td>-118.59759</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-324.7111</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_eps-5000_1763011385</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/n5eq1pu3' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/n5eq1pu3</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072305-n5eq1pu3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_gamma-0.1.pth ===\n",
      "W&B run: test_Pendulum_DQN_gamma-0.1_1763011392\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072312-toe9x3yo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/toe9x3yo' target=\"_blank\">test_Pendulum_DQN_gamma-0.1_1763011392</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/toe9x3yo' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/toe9x3yo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-1506.85 steps=200\n",
      "  Ep 2/100: reward=-1525.71 steps=200\n",
      "  Ep 3/100: reward=-1417.36 steps=200\n",
      "  Ep 4/100: reward=-1510.63 steps=200\n",
      "  Ep 5/100: reward=-1497.19 steps=200\n",
      "  Ep 6/100: reward=-1508.07 steps=200\n",
      "  Ep 7/100: reward=-1441.06 steps=200\n",
      "  Ep 8/100: reward=-1536.18 steps=200\n",
      "  Ep 9/100: reward=-1445.86 steps=200\n",
      "  Ep 10/100: reward=-1438.52 steps=200\n",
      "  Ep 11/100: reward=-1432.96 steps=200\n",
      "  Ep 12/100: reward=-1486.48 steps=200\n",
      "  Ep 13/100: reward=-1498.85 steps=200\n",
      "  Ep 14/100: reward=-1498.61 steps=200\n",
      "  Ep 15/100: reward=-1288.67 steps=200\n",
      "  Ep 16/100: reward=-1482.38 steps=200\n",
      "  Ep 17/100: reward=-1424.91 steps=200\n",
      "  Ep 18/100: reward=-1483.79 steps=200\n",
      "  Ep 19/100: reward=-1516.96 steps=200\n",
      "  Ep 20/100: reward=-1498.00 steps=200\n",
      "  Ep 21/100: reward=-1516.44 steps=200\n",
      "  Ep 22/100: reward=-1406.58 steps=200\n",
      "  Ep 23/100: reward=-1500.36 steps=200\n",
      "  Ep 24/100: reward=-1501.32 steps=200\n",
      "  Ep 25/100: reward=-1367.75 steps=200\n",
      "  Ep 26/100: reward=-1528.12 steps=200\n",
      "  Ep 27/100: reward=-1431.55 steps=200\n",
      "  Ep 28/100: reward=-1494.80 steps=200\n",
      "  Ep 29/100: reward=-1486.86 steps=200\n",
      "  Ep 30/100: reward=-1414.35 steps=200\n",
      "  Ep 31/100: reward=-1353.58 steps=200\n",
      "  Ep 32/100: reward=-1518.79 steps=200\n",
      "  Ep 33/100: reward=-1538.23 steps=200\n",
      "  Ep 34/100: reward=-1511.94 steps=200\n",
      "  Ep 35/100: reward=-1443.29 steps=200\n",
      "  Ep 36/100: reward=-1519.42 steps=200\n",
      "  Ep 37/100: reward=-1488.57 steps=200\n",
      "  Ep 38/100: reward=-1472.83 steps=200\n",
      "  Ep 39/100: reward=-1477.49 steps=200\n",
      "  Ep 40/100: reward=-1491.28 steps=200\n",
      "  Ep 41/100: reward=-1363.80 steps=200\n",
      "  Ep 42/100: reward=-1490.81 steps=200\n",
      "  Ep 43/100: reward=-1489.34 steps=200\n",
      "  Ep 44/100: reward=-1528.53 steps=200\n",
      "  Ep 45/100: reward=-1496.44 steps=200\n",
      "  Ep 46/100: reward=-1494.67 steps=200\n",
      "  Ep 47/100: reward=-1495.31 steps=200\n",
      "  Ep 48/100: reward=-1477.51 steps=200\n",
      "  Ep 49/100: reward=-1423.03 steps=200\n",
      "  Ep 50/100: reward=-1502.43 steps=200\n",
      "  Ep 51/100: reward=-1514.02 steps=200\n",
      "  Ep 52/100: reward=-1495.69 steps=200\n",
      "  Ep 53/100: reward=-1448.85 steps=200\n",
      "  Ep 54/100: reward=-1394.58 steps=200\n",
      "  Ep 55/100: reward=-1447.63 steps=200\n",
      "  Ep 56/100: reward=-1522.80 steps=200\n",
      "  Ep 57/100: reward=-1440.25 steps=200\n",
      "  Ep 58/100: reward=-1416.20 steps=200\n",
      "  Ep 59/100: reward=-1550.53 steps=200\n",
      "  Ep 60/100: reward=-1422.19 steps=200\n",
      "  Ep 61/100: reward=-1428.86 steps=200\n",
      "  Ep 62/100: reward=-1461.17 steps=200\n",
      "  Ep 63/100: reward=-1511.66 steps=200\n",
      "  Ep 64/100: reward=-1431.39 steps=200\n",
      "  Ep 65/100: reward=-1566.55 steps=200\n",
      "  Ep 66/100: reward=-1524.66 steps=200\n",
      "  Ep 67/100: reward=-1507.82 steps=200\n",
      "  Ep 68/100: reward=-1396.08 steps=200\n",
      "  Ep 69/100: reward=-1497.59 steps=200\n",
      "  Ep 70/100: reward=-1506.35 steps=200\n",
      "  Ep 71/100: reward=-1374.17 steps=200\n",
      "  Ep 72/100: reward=-1431.99 steps=200\n",
      "  Ep 73/100: reward=-1475.10 steps=200\n",
      "  Ep 74/100: reward=-1504.08 steps=200\n",
      "  Ep 75/100: reward=-1497.37 steps=200\n",
      "  Ep 76/100: reward=-1517.60 steps=200\n",
      "  Ep 77/100: reward=-1524.74 steps=200\n",
      "  Ep 78/100: reward=-1451.94 steps=200\n",
      "  Ep 79/100: reward=-1501.89 steps=200\n",
      "  Ep 80/100: reward=-1428.19 steps=200\n",
      "  Ep 81/100: reward=-1391.62 steps=200\n",
      "  Ep 82/100: reward=-1458.75 steps=200\n",
      "  Ep 83/100: reward=-1496.81 steps=200\n",
      "  Ep 84/100: reward=-1526.88 steps=200\n",
      "  Ep 85/100: reward=-1418.91 steps=200\n",
      "  Ep 86/100: reward=-1475.82 steps=200\n",
      "  Ep 87/100: reward=-1491.03 steps=200\n",
      "  Ep 88/100: reward=-1439.87 steps=200\n",
      "  Ep 89/100: reward=-1492.34 steps=200\n",
      "  Ep 90/100: reward=-1509.33 steps=200\n",
      "  Ep 91/100: reward=-1525.89 steps=200\n",
      "  Ep 92/100: reward=-1518.51 steps=200\n",
      "  Ep 93/100: reward=-1476.00 steps=200\n",
      "  Ep 94/100: reward=-1525.20 steps=200\n",
      "  Ep 95/100: reward=-1513.47 steps=200\n",
      "  Ep 96/100: reward=-1509.18 steps=200\n",
      "  Ep 97/100: reward=-1521.38 steps=200\n",
      "  Ep 98/100: reward=-1504.33 steps=200\n",
      "  Ep 99/100: reward=-1506.86 steps=200\n",
      "  Ep 100/100: reward=-1538.19 steps=200\n",
      "Finished evaluations. avg=-1476.97 best=-1288.67 worst=-1566.55\n",
      "Recording random episode index 48 for model Pendulum_DQN_gamma-0.1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_gamma-0.1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_gamma-0.1\\Pendulum_DQN_gamma-0.1-episode-0.mp4\n",
      "Recording complete. Total reward: -1483.95, Steps:200, Duration: 1.02 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▂▅▂▂▂▅▂▁▆▂▁▃▆█▁▁▃▂▂▂▂▄▆▄▅▅▅▄▂▅▇▅▁▆▂▂▂▁▂▂</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-1476.96761</td></tr><tr><td>best_reward</td><td>-1288.67324</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-1538.19496</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.02119</td></tr><tr><td>recorded_episode_index</td><td>48</td></tr><tr><td>recorded_episode_reward</td><td>-1483.94547</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-1566.55139</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_gamma-0.1_1763011392</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/toe9x3yo' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/toe9x3yo</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072312-toe9x3yo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_gamma-0.5.pth ===\n",
      "W&B run: test_Pendulum_DQN_gamma-0.5_1763011398\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072318-im3sel1u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/im3sel1u' target=\"_blank\">test_Pendulum_DQN_gamma-0.5_1763011398</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/im3sel1u' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/im3sel1u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-1271.39 steps=200\n",
      "  Ep 2/100: reward=-797.34 steps=200\n",
      "  Ep 3/100: reward=-1265.01 steps=200\n",
      "  Ep 4/100: reward=-1495.40 steps=200\n",
      "  Ep 5/100: reward=-1293.62 steps=200\n",
      "  Ep 6/100: reward=-1301.17 steps=200\n",
      "  Ep 7/100: reward=-1328.88 steps=200\n",
      "  Ep 8/100: reward=-1501.30 steps=200\n",
      "  Ep 9/100: reward=-1341.89 steps=200\n",
      "  Ep 10/100: reward=-1325.98 steps=200\n",
      "  Ep 11/100: reward=-1495.08 steps=200\n",
      "  Ep 12/100: reward=-1308.21 steps=200\n",
      "  Ep 13/100: reward=-1493.13 steps=200\n",
      "  Ep 14/100: reward=-1308.85 steps=200\n",
      "  Ep 15/100: reward=-1289.96 steps=200\n",
      "  Ep 16/100: reward=-1394.06 steps=200\n",
      "  Ep 17/100: reward=-1395.71 steps=200\n",
      "  Ep 18/100: reward=-1343.33 steps=200\n",
      "  Ep 19/100: reward=-1314.54 steps=200\n",
      "  Ep 20/100: reward=-1337.74 steps=200\n",
      "  Ep 21/100: reward=-1500.31 steps=200\n",
      "  Ep 22/100: reward=-3.30 steps=200\n",
      "  Ep 23/100: reward=-1339.51 steps=200\n",
      "  Ep 24/100: reward=-1347.59 steps=200\n",
      "  Ep 25/100: reward=-1322.59 steps=200\n",
      "  Ep 26/100: reward=-1355.62 steps=200\n",
      "  Ep 27/100: reward=-1011.67 steps=200\n",
      "  Ep 28/100: reward=-1254.69 steps=200\n",
      "  Ep 29/100: reward=-1504.91 steps=200\n",
      "  Ep 30/100: reward=-1350.25 steps=200\n",
      "  Ep 31/100: reward=-1332.16 steps=200\n",
      "  Ep 32/100: reward=-1229.58 steps=200\n",
      "  Ep 33/100: reward=-1313.46 steps=200\n",
      "  Ep 34/100: reward=-1192.38 steps=200\n",
      "  Ep 35/100: reward=-1323.14 steps=200\n",
      "  Ep 36/100: reward=-1498.90 steps=200\n",
      "  Ep 37/100: reward=-1283.39 steps=200\n",
      "  Ep 38/100: reward=-2.65 steps=200\n",
      "  Ep 39/100: reward=-1.32 steps=200\n",
      "  Ep 40/100: reward=-1495.83 steps=200\n",
      "  Ep 41/100: reward=-3.18 steps=200\n",
      "  Ep 42/100: reward=-1493.83 steps=200\n",
      "  Ep 43/100: reward=-1.97 steps=200\n",
      "  Ep 44/100: reward=-1314.83 steps=200\n",
      "  Ep 45/100: reward=-1508.42 steps=200\n",
      "  Ep 46/100: reward=-1497.40 steps=200\n",
      "  Ep 47/100: reward=-1496.56 steps=200\n",
      "  Ep 48/100: reward=-1326.05 steps=200\n",
      "  Ep 49/100: reward=-1317.08 steps=200\n",
      "  Ep 50/100: reward=-1319.45 steps=200\n",
      "  Ep 51/100: reward=-3.39 steps=200\n",
      "  Ep 52/100: reward=-1264.46 steps=200\n",
      "  Ep 53/100: reward=-1292.46 steps=200\n",
      "  Ep 54/100: reward=-2.02 steps=200\n",
      "  Ep 55/100: reward=-1315.55 steps=200\n",
      "  Ep 56/100: reward=-1492.18 steps=200\n",
      "  Ep 57/100: reward=-1317.30 steps=200\n",
      "  Ep 58/100: reward=-1304.25 steps=200\n",
      "  Ep 59/100: reward=-1322.05 steps=200\n",
      "  Ep 60/100: reward=-1316.52 steps=200\n",
      "  Ep 61/100: reward=-1318.49 steps=200\n",
      "  Ep 62/100: reward=-1262.40 steps=200\n",
      "  Ep 63/100: reward=-684.28 steps=200\n",
      "  Ep 64/100: reward=-1496.04 steps=200\n",
      "  Ep 65/100: reward=-1344.87 steps=200\n",
      "  Ep 66/100: reward=-1493.06 steps=200\n",
      "  Ep 67/100: reward=-1379.73 steps=200\n",
      "  Ep 68/100: reward=-1177.21 steps=200\n",
      "  Ep 69/100: reward=-1096.52 steps=200\n",
      "  Ep 70/100: reward=-1300.55 steps=200\n",
      "  Ep 71/100: reward=-1334.08 steps=200\n",
      "  Ep 72/100: reward=-1491.88 steps=200\n",
      "  Ep 73/100: reward=-1268.86 steps=200\n",
      "  Ep 74/100: reward=-1515.63 steps=200\n",
      "  Ep 75/100: reward=-1266.68 steps=200\n",
      "  Ep 76/100: reward=-1312.49 steps=200\n",
      "  Ep 77/100: reward=-1302.71 steps=200\n",
      "  Ep 78/100: reward=-1499.25 steps=200\n",
      "  Ep 79/100: reward=-1270.98 steps=200\n",
      "  Ep 80/100: reward=-1322.73 steps=200\n",
      "  Ep 81/100: reward=-1491.76 steps=200\n",
      "  Ep 82/100: reward=-1497.30 steps=200\n",
      "  Ep 83/100: reward=-1495.32 steps=200\n",
      "  Ep 84/100: reward=-3.08 steps=200\n",
      "  Ep 85/100: reward=-1289.92 steps=200\n",
      "  Ep 86/100: reward=-1270.19 steps=200\n",
      "  Ep 87/100: reward=-1234.97 steps=200\n",
      "  Ep 88/100: reward=-1343.83 steps=200\n",
      "  Ep 89/100: reward=-1326.80 steps=200\n",
      "  Ep 90/100: reward=-2.42 steps=200\n",
      "  Ep 91/100: reward=-1318.37 steps=200\n",
      "  Ep 92/100: reward=-1496.79 steps=200\n",
      "  Ep 93/100: reward=-1.47 steps=200\n",
      "  Ep 94/100: reward=-1508.35 steps=200\n",
      "  Ep 95/100: reward=-1312.54 steps=200\n",
      "  Ep 96/100: reward=-1314.98 steps=200\n",
      "  Ep 97/100: reward=-1325.17 steps=200\n",
      "  Ep 98/100: reward=-1259.53 steps=200\n",
      "  Ep 99/100: reward=-1498.43 steps=200\n",
      "  Ep 100/100: reward=-1275.10 steps=200\n",
      "Finished evaluations. avg=-1205.80 best=-1.32 worst=-1515.63\n",
      "Recording random episode index 99 for model Pendulum_DQN_gamma-0.5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_gamma-0.5 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_gamma-0.5\\Pendulum_DQN_gamma-0.5-episode-0.mp4\n",
      "Recording complete. Total reward: -1499.72, Steps:200, Duration: 0.95 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▂▂▁▂▂▂▂▂▂▁▂▂▂▂▂▂███▁▁▂▂▂▂▂▅▂▃▂▂▁▂▂▁▂▂▂█▂</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-1205.79616</td></tr><tr><td>best_reward</td><td>-1.32314</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-1275.09549</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.95329</td></tr><tr><td>recorded_episode_index</td><td>99</td></tr><tr><td>recorded_episode_reward</td><td>-1499.71928</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-1515.63054</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_gamma-0.5_1763011398</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/im3sel1u' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/im3sel1u</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072318-im3sel1u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_gamma-0.9.pth ===\n",
      "W&B run: test_Pendulum_DQN_gamma-0.9_1763011405\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072325-u03pvcit</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u03pvcit' target=\"_blank\">test_Pendulum_DQN_gamma-0.9_1763011405</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u03pvcit' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u03pvcit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-243.54 steps=200\n",
      "  Ep 2/100: reward=-122.40 steps=200\n",
      "  Ep 3/100: reward=-304.93 steps=200\n",
      "  Ep 4/100: reward=-304.59 steps=200\n",
      "  Ep 5/100: reward=-128.57 steps=200\n",
      "  Ep 6/100: reward=-122.97 steps=200\n",
      "  Ep 7/100: reward=-337.10 steps=200\n",
      "  Ep 8/100: reward=-131.00 steps=200\n",
      "  Ep 9/100: reward=-2.09 steps=200\n",
      "  Ep 10/100: reward=-276.59 steps=200\n",
      "  Ep 11/100: reward=-126.88 steps=200\n",
      "  Ep 12/100: reward=-271.37 steps=200\n",
      "  Ep 13/100: reward=-349.39 steps=200\n",
      "  Ep 14/100: reward=-1.92 steps=200\n",
      "  Ep 15/100: reward=-131.07 steps=200\n",
      "  Ep 16/100: reward=-241.09 steps=200\n",
      "  Ep 17/100: reward=-118.97 steps=200\n",
      "  Ep 18/100: reward=-130.66 steps=200\n",
      "  Ep 19/100: reward=-0.92 steps=200\n",
      "  Ep 20/100: reward=-371.38 steps=200\n",
      "  Ep 21/100: reward=-133.62 steps=200\n",
      "  Ep 22/100: reward=-131.83 steps=200\n",
      "  Ep 23/100: reward=-131.65 steps=200\n",
      "  Ep 24/100: reward=-134.26 steps=200\n",
      "  Ep 25/100: reward=-248.58 steps=200\n",
      "  Ep 26/100: reward=-1.65 steps=200\n",
      "  Ep 27/100: reward=-130.58 steps=200\n",
      "  Ep 28/100: reward=-250.75 steps=200\n",
      "  Ep 29/100: reward=-133.00 steps=200\n",
      "  Ep 30/100: reward=-1.44 steps=200\n",
      "  Ep 31/100: reward=-384.60 steps=200\n",
      "  Ep 32/100: reward=-272.06 steps=200\n",
      "  Ep 33/100: reward=-123.43 steps=200\n",
      "  Ep 34/100: reward=-268.52 steps=200\n",
      "  Ep 35/100: reward=-130.40 steps=200\n",
      "  Ep 36/100: reward=-269.30 steps=200\n",
      "  Ep 37/100: reward=-244.77 steps=200\n",
      "  Ep 38/100: reward=-129.38 steps=200\n",
      "  Ep 39/100: reward=-128.04 steps=200\n",
      "  Ep 40/100: reward=-126.22 steps=200\n",
      "  Ep 41/100: reward=-127.27 steps=200\n",
      "  Ep 42/100: reward=-131.43 steps=200\n",
      "  Ep 43/100: reward=-129.52 steps=200\n",
      "  Ep 44/100: reward=-267.29 steps=200\n",
      "  Ep 45/100: reward=-131.85 steps=200\n",
      "  Ep 46/100: reward=-131.52 steps=200\n",
      "  Ep 47/100: reward=-118.26 steps=200\n",
      "  Ep 48/100: reward=-2.57 steps=200\n",
      "  Ep 49/100: reward=-118.96 steps=200\n",
      "  Ep 50/100: reward=-128.94 steps=200\n",
      "  Ep 51/100: reward=-131.43 steps=200\n",
      "  Ep 52/100: reward=-131.90 steps=200\n",
      "  Ep 53/100: reward=-0.83 steps=200\n",
      "  Ep 54/100: reward=-326.67 steps=200\n",
      "  Ep 55/100: reward=-132.58 steps=200\n",
      "  Ep 56/100: reward=-1.33 steps=200\n",
      "  Ep 57/100: reward=-131.77 steps=200\n",
      "  Ep 58/100: reward=-131.91 steps=200\n",
      "  Ep 59/100: reward=-250.24 steps=200\n",
      "  Ep 60/100: reward=-138.53 steps=200\n",
      "  Ep 61/100: reward=-132.02 steps=200\n",
      "  Ep 62/100: reward=-133.06 steps=200\n",
      "  Ep 63/100: reward=-132.37 steps=200\n",
      "  Ep 64/100: reward=-124.92 steps=200\n",
      "  Ep 65/100: reward=-268.53 steps=200\n",
      "  Ep 66/100: reward=-135.66 steps=200\n",
      "  Ep 67/100: reward=-133.39 steps=200\n",
      "  Ep 68/100: reward=-381.81 steps=200\n",
      "  Ep 69/100: reward=-2.45 steps=200\n",
      "  Ep 70/100: reward=-238.53 steps=200\n",
      "  Ep 71/100: reward=-121.36 steps=200\n",
      "  Ep 72/100: reward=-1.66 steps=200\n",
      "  Ep 73/100: reward=-250.79 steps=200\n",
      "  Ep 74/100: reward=-119.84 steps=200\n",
      "  Ep 75/100: reward=-126.76 steps=200\n",
      "  Ep 76/100: reward=-272.98 steps=200\n",
      "  Ep 77/100: reward=-1.84 steps=200\n",
      "  Ep 78/100: reward=-132.94 steps=200\n",
      "  Ep 79/100: reward=-132.60 steps=200\n",
      "  Ep 80/100: reward=-127.54 steps=200\n",
      "  Ep 81/100: reward=-129.59 steps=200\n",
      "  Ep 82/100: reward=-254.44 steps=200\n",
      "  Ep 83/100: reward=-239.14 steps=200\n",
      "  Ep 84/100: reward=-253.73 steps=200\n",
      "  Ep 85/100: reward=-132.76 steps=200\n",
      "  Ep 86/100: reward=-131.65 steps=200\n",
      "  Ep 87/100: reward=-122.53 steps=200\n",
      "  Ep 88/100: reward=-0.89 steps=200\n",
      "  Ep 89/100: reward=-1.35 steps=200\n",
      "  Ep 90/100: reward=-0.82 steps=200\n",
      "  Ep 91/100: reward=-297.70 steps=200\n",
      "  Ep 92/100: reward=-1.25 steps=200\n",
      "  Ep 93/100: reward=-1.00 steps=200\n",
      "  Ep 94/100: reward=-346.27 steps=200\n",
      "  Ep 95/100: reward=-138.08 steps=200\n",
      "  Ep 96/100: reward=-239.74 steps=200\n",
      "  Ep 97/100: reward=-351.39 steps=200\n",
      "  Ep 98/100: reward=-244.91 steps=200\n",
      "  Ep 99/100: reward=-130.18 steps=200\n",
      "  Ep 100/100: reward=-130.30 steps=200\n",
      "Finished evaluations. avg=-158.75 best=-0.82 worst=-384.60\n",
      "Recording random episode index 71 for model Pendulum_DQN_gamma-0.9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_gamma-0.9 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_gamma-0.9\\Pendulum_DQN_gamma-0.9-episode-0.mp4\n",
      "Recording complete. Total reward: -130.72, Steps:200, Duration: 1.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▂▆▂▆▃█▄▆█▆█▃█▆▃▃▆▆▃▆▆▆█▆▆▆▆▁█▄▆▆▆▆▃▆█▃█▆</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-158.75134</td></tr><tr><td>best_reward</td><td>-0.8247</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-130.29832</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.00757</td></tr><tr><td>recorded_episode_index</td><td>71</td></tr><tr><td>recorded_episode_reward</td><td>-130.72206</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-384.59723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_gamma-0.9_1763011405</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u03pvcit' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/u03pvcit</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072325-u03pvcit\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_gamma-0.99.pth ===\n",
      "W&B run: test_Pendulum_DQN_gamma-0.99_1763011412\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072332-z8ngoqcb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/z8ngoqcb' target=\"_blank\">test_Pendulum_DQN_gamma-0.99_1763011412</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/z8ngoqcb' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/z8ngoqcb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-119.86 steps=200\n",
      "  Ep 2/100: reward=-117.45 steps=200\n",
      "  Ep 3/100: reward=-227.59 steps=200\n",
      "  Ep 4/100: reward=-126.65 steps=200\n",
      "  Ep 5/100: reward=-219.45 steps=200\n",
      "  Ep 6/100: reward=-248.51 steps=200\n",
      "  Ep 7/100: reward=-124.86 steps=200\n",
      "  Ep 8/100: reward=-118.62 steps=200\n",
      "  Ep 9/100: reward=-1.81 steps=200\n",
      "  Ep 10/100: reward=-126.19 steps=200\n",
      "  Ep 11/100: reward=-308.67 steps=200\n",
      "  Ep 12/100: reward=-124.99 steps=200\n",
      "  Ep 13/100: reward=-254.57 steps=200\n",
      "  Ep 14/100: reward=-1.69 steps=200\n",
      "  Ep 15/100: reward=-119.31 steps=200\n",
      "  Ep 16/100: reward=-231.29 steps=200\n",
      "  Ep 17/100: reward=-125.63 steps=200\n",
      "  Ep 18/100: reward=-115.67 steps=200\n",
      "  Ep 19/100: reward=-231.05 steps=200\n",
      "  Ep 20/100: reward=-241.41 steps=200\n",
      "  Ep 21/100: reward=-120.79 steps=200\n",
      "  Ep 22/100: reward=-1.28 steps=200\n",
      "  Ep 23/100: reward=-117.94 steps=200\n",
      "  Ep 24/100: reward=-238.69 steps=200\n",
      "  Ep 25/100: reward=-123.58 steps=200\n",
      "  Ep 26/100: reward=-122.10 steps=200\n",
      "  Ep 27/100: reward=-121.18 steps=200\n",
      "  Ep 28/100: reward=-285.85 steps=200\n",
      "  Ep 29/100: reward=-125.78 steps=200\n",
      "  Ep 30/100: reward=-268.36 steps=200\n",
      "  Ep 31/100: reward=-126.01 steps=200\n",
      "  Ep 32/100: reward=-116.97 steps=200\n",
      "  Ep 33/100: reward=-2.84 steps=200\n",
      "  Ep 34/100: reward=-347.33 steps=200\n",
      "  Ep 35/100: reward=-234.36 steps=200\n",
      "  Ep 36/100: reward=-1.38 steps=200\n",
      "  Ep 37/100: reward=-1.38 steps=200\n",
      "  Ep 38/100: reward=-126.32 steps=200\n",
      "  Ep 39/100: reward=-230.66 steps=200\n",
      "  Ep 40/100: reward=-235.43 steps=200\n",
      "  Ep 41/100: reward=-116.02 steps=200\n",
      "  Ep 42/100: reward=-117.54 steps=200\n",
      "  Ep 43/100: reward=-124.49 steps=200\n",
      "  Ep 44/100: reward=-118.90 steps=200\n",
      "  Ep 45/100: reward=-247.29 steps=200\n",
      "  Ep 46/100: reward=-120.95 steps=200\n",
      "  Ep 47/100: reward=-119.33 steps=200\n",
      "  Ep 48/100: reward=-120.07 steps=200\n",
      "  Ep 49/100: reward=-127.98 steps=200\n",
      "  Ep 50/100: reward=-126.30 steps=200\n",
      "  Ep 51/100: reward=-116.43 steps=200\n",
      "  Ep 52/100: reward=-224.38 steps=200\n",
      "  Ep 53/100: reward=-115.82 steps=200\n",
      "  Ep 54/100: reward=-224.09 steps=200\n",
      "  Ep 55/100: reward=-119.96 steps=200\n",
      "  Ep 56/100: reward=-121.94 steps=200\n",
      "  Ep 57/100: reward=-121.76 steps=200\n",
      "  Ep 58/100: reward=-336.91 steps=200\n",
      "  Ep 59/100: reward=-225.07 steps=200\n",
      "  Ep 60/100: reward=-125.89 steps=200\n",
      "  Ep 61/100: reward=-118.80 steps=200\n",
      "  Ep 62/100: reward=-342.72 steps=200\n",
      "  Ep 63/100: reward=-124.84 steps=200\n",
      "  Ep 64/100: reward=-126.71 steps=200\n",
      "  Ep 65/100: reward=-3.30 steps=200\n",
      "  Ep 66/100: reward=-231.08 steps=200\n",
      "  Ep 67/100: reward=-122.08 steps=200\n",
      "  Ep 68/100: reward=-117.79 steps=200\n",
      "  Ep 69/100: reward=-230.36 steps=200\n",
      "  Ep 70/100: reward=-119.01 steps=200\n",
      "  Ep 71/100: reward=-124.33 steps=200\n",
      "  Ep 72/100: reward=-117.28 steps=200\n",
      "  Ep 73/100: reward=-122.93 steps=200\n",
      "  Ep 74/100: reward=-291.66 steps=200\n",
      "  Ep 75/100: reward=-116.21 steps=200\n",
      "  Ep 76/100: reward=-233.32 steps=200\n",
      "  Ep 77/100: reward=-1.97 steps=200\n",
      "  Ep 78/100: reward=-221.41 steps=200\n",
      "  Ep 79/100: reward=-122.83 steps=200\n",
      "  Ep 80/100: reward=-2.28 steps=200\n",
      "  Ep 81/100: reward=-120.63 steps=200\n",
      "  Ep 82/100: reward=-1.23 steps=200\n",
      "  Ep 83/100: reward=-119.64 steps=200\n",
      "  Ep 84/100: reward=-124.92 steps=200\n",
      "  Ep 85/100: reward=-225.32 steps=200\n",
      "  Ep 86/100: reward=-1.57 steps=200\n",
      "  Ep 87/100: reward=-253.09 steps=200\n",
      "  Ep 88/100: reward=-124.79 steps=200\n",
      "  Ep 89/100: reward=-2.30 steps=200\n",
      "  Ep 90/100: reward=-229.33 steps=200\n",
      "  Ep 91/100: reward=-121.63 steps=200\n",
      "  Ep 92/100: reward=-1.12 steps=200\n",
      "  Ep 93/100: reward=-126.89 steps=200\n",
      "  Ep 94/100: reward=-230.42 steps=200\n",
      "  Ep 95/100: reward=-124.20 steps=200\n",
      "  Ep 96/100: reward=-234.90 steps=200\n",
      "  Ep 97/100: reward=-114.34 steps=200\n",
      "  Ep 98/100: reward=-123.20 steps=200\n",
      "  Ep 99/100: reward=-120.97 steps=200\n",
      "  Ep 100/100: reward=-116.76 steps=200\n",
      "Finished evaluations. avg=-146.13 best=-1.12 worst=-347.33\n",
      "Recording random episode index 71 for model Pendulum_DQN_gamma-0.99.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_gamma-0.99 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_gamma-0.99\\Pendulum_DQN_gamma-0.99-episode-0.mp4\n",
      "Recording complete. Total reward: -268.62, Steps:200, Duration: 0.99 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▃▅▄▃▆▆▃▃▆█▆▆▅▅▁▃▆▆▃▆▅▆▆▃▆▆▃▅▁▆▃▆▆▂▆▃█▃▃▆</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-146.12759</td></tr><tr><td>best_reward</td><td>-1.12055</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-116.76029</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.98757</td></tr><tr><td>recorded_episode_index</td><td>71</td></tr><tr><td>recorded_episode_reward</td><td>-268.61698</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-347.33178</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_gamma-0.99_1763011412</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/z8ngoqcb' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/z8ngoqcb</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072332-z8ngoqcb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_lr-0.0001.pth ===\n",
      "W&B run: test_Pendulum_DQN_lr-0.0001_1763011419\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072339-1t068gs5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1t068gs5' target=\"_blank\">test_Pendulum_DQN_lr-0.0001_1763011419</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1t068gs5' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1t068gs5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-118.93 steps=200\n",
      "  Ep 2/100: reward=-121.15 steps=200\n",
      "  Ep 3/100: reward=-234.62 steps=200\n",
      "  Ep 4/100: reward=-1.85 steps=200\n",
      "  Ep 5/100: reward=-114.63 steps=200\n",
      "  Ep 6/100: reward=-119.79 steps=200\n",
      "  Ep 7/100: reward=-117.02 steps=200\n",
      "  Ep 8/100: reward=-1.48 steps=200\n",
      "  Ep 9/100: reward=-126.23 steps=200\n",
      "  Ep 10/100: reward=-126.39 steps=200\n",
      "  Ep 11/100: reward=-114.03 steps=200\n",
      "  Ep 12/100: reward=-233.98 steps=200\n",
      "  Ep 13/100: reward=-1.70 steps=200\n",
      "  Ep 14/100: reward=-123.87 steps=200\n",
      "  Ep 15/100: reward=-122.13 steps=200\n",
      "  Ep 16/100: reward=-123.18 steps=200\n",
      "  Ep 17/100: reward=-118.93 steps=200\n",
      "  Ep 18/100: reward=-1.34 steps=200\n",
      "  Ep 19/100: reward=-126.79 steps=200\n",
      "  Ep 20/100: reward=-229.55 steps=200\n",
      "  Ep 21/100: reward=-122.91 steps=200\n",
      "  Ep 22/100: reward=-126.33 steps=200\n",
      "  Ep 23/100: reward=-120.83 steps=200\n",
      "  Ep 24/100: reward=-121.28 steps=200\n",
      "  Ep 25/100: reward=-236.15 steps=200\n",
      "  Ep 26/100: reward=-247.59 steps=200\n",
      "  Ep 27/100: reward=-238.93 steps=200\n",
      "  Ep 28/100: reward=-117.95 steps=200\n",
      "  Ep 29/100: reward=-122.48 steps=200\n",
      "  Ep 30/100: reward=-120.82 steps=200\n",
      "  Ep 31/100: reward=-318.90 steps=200\n",
      "  Ep 32/100: reward=-127.04 steps=200\n",
      "  Ep 33/100: reward=-122.65 steps=200\n",
      "  Ep 34/100: reward=-221.03 steps=200\n",
      "  Ep 35/100: reward=-116.39 steps=200\n",
      "  Ep 36/100: reward=-126.90 steps=200\n",
      "  Ep 37/100: reward=-121.60 steps=200\n",
      "  Ep 38/100: reward=-121.02 steps=200\n",
      "  Ep 39/100: reward=-1.54 steps=200\n",
      "  Ep 40/100: reward=-119.50 steps=200\n",
      "  Ep 41/100: reward=-331.18 steps=200\n",
      "  Ep 42/100: reward=-124.75 steps=200\n",
      "  Ep 43/100: reward=-116.24 steps=200\n",
      "  Ep 44/100: reward=-234.18 steps=200\n",
      "  Ep 45/100: reward=-1.40 steps=200\n",
      "  Ep 46/100: reward=-121.24 steps=200\n",
      "  Ep 47/100: reward=-243.40 steps=200\n",
      "  Ep 48/100: reward=-117.85 steps=200\n",
      "  Ep 49/100: reward=-1.94 steps=200\n",
      "  Ep 50/100: reward=-114.92 steps=200\n",
      "  Ep 51/100: reward=-235.79 steps=200\n",
      "  Ep 52/100: reward=-125.59 steps=200\n",
      "  Ep 53/100: reward=-247.21 steps=200\n",
      "  Ep 54/100: reward=-118.39 steps=200\n",
      "  Ep 55/100: reward=-126.05 steps=200\n",
      "  Ep 56/100: reward=-2.30 steps=200\n",
      "  Ep 57/100: reward=-118.93 steps=200\n",
      "  Ep 58/100: reward=-126.33 steps=200\n",
      "  Ep 59/100: reward=-121.70 steps=200\n",
      "  Ep 60/100: reward=-120.69 steps=200\n",
      "  Ep 61/100: reward=-121.49 steps=200\n",
      "  Ep 62/100: reward=-125.84 steps=200\n",
      "  Ep 63/100: reward=-299.44 steps=200\n",
      "  Ep 64/100: reward=-120.51 steps=200\n",
      "  Ep 65/100: reward=-126.63 steps=200\n",
      "  Ep 66/100: reward=-124.41 steps=200\n",
      "  Ep 67/100: reward=-121.62 steps=200\n",
      "  Ep 68/100: reward=-123.35 steps=200\n",
      "  Ep 69/100: reward=-117.74 steps=200\n",
      "  Ep 70/100: reward=-237.89 steps=200\n",
      "  Ep 71/100: reward=-334.09 steps=200\n",
      "  Ep 72/100: reward=-116.57 steps=200\n",
      "  Ep 73/100: reward=-129.48 steps=200\n",
      "  Ep 74/100: reward=-121.50 steps=200\n",
      "  Ep 75/100: reward=-120.56 steps=200\n",
      "  Ep 76/100: reward=-115.95 steps=200\n",
      "  Ep 77/100: reward=-236.63 steps=200\n",
      "  Ep 78/100: reward=-327.90 steps=200\n",
      "  Ep 79/100: reward=-235.34 steps=200\n",
      "  Ep 80/100: reward=-117.58 steps=200\n",
      "  Ep 81/100: reward=-125.49 steps=200\n",
      "  Ep 82/100: reward=-119.92 steps=200\n",
      "  Ep 83/100: reward=-119.81 steps=200\n",
      "  Ep 84/100: reward=-118.54 steps=200\n",
      "  Ep 85/100: reward=-232.91 steps=200\n",
      "  Ep 86/100: reward=-120.07 steps=200\n",
      "  Ep 87/100: reward=-128.91 steps=200\n",
      "  Ep 88/100: reward=-120.84 steps=200\n",
      "  Ep 89/100: reward=-241.67 steps=200\n",
      "  Ep 90/100: reward=-119.25 steps=200\n",
      "  Ep 91/100: reward=-127.44 steps=200\n",
      "  Ep 92/100: reward=-119.49 steps=200\n",
      "  Ep 93/100: reward=-120.51 steps=200\n",
      "  Ep 94/100: reward=-131.05 steps=200\n",
      "  Ep 95/100: reward=-123.59 steps=200\n",
      "  Ep 96/100: reward=-126.29 steps=200\n",
      "  Ep 97/100: reward=-121.09 steps=200\n",
      "  Ep 98/100: reward=-237.37 steps=200\n",
      "  Ep 99/100: reward=-299.45 steps=200\n",
      "  Ep 100/100: reward=-244.49 steps=200\n",
      "Finished evaluations. avg=-144.72 best=-1.34 worst=-334.09\n",
      "Recording random episode index 81 for model Pendulum_DQN_lr-0.0001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_lr-0.0001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_lr-0.0001\\Pendulum_DQN_lr-0.0001-episode-0.mp4\n",
      "Recording complete. Total reward: -125.24, Steps:200, Duration: 1.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▃█▆▆█▆▃▅▅▆▃▅▅▃▅▅▆▃█▅▃▅█▆▅▅▂▅▆▁▅▃▁▆▅▆▅▅▅▃</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-144.72243</td></tr><tr><td>best_reward</td><td>-1.34143</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-244.48778</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.99646</td></tr><tr><td>recorded_episode_index</td><td>81</td></tr><tr><td>recorded_episode_reward</td><td>-125.23958</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-334.09302</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_lr-0.0001_1763011419</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1t068gs5' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1t068gs5</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072339-1t068gs5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_lr-0.0003.pth ===\n",
      "W&B run: test_Pendulum_DQN_lr-0.0003_1763011425\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072345-stkwxepg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/stkwxepg' target=\"_blank\">test_Pendulum_DQN_lr-0.0003_1763011425</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/stkwxepg' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/stkwxepg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-3.15 steps=200\n",
      "  Ep 2/100: reward=-118.27 steps=200\n",
      "  Ep 3/100: reward=-117.07 steps=200\n",
      "  Ep 4/100: reward=-123.41 steps=200\n",
      "  Ep 5/100: reward=-124.28 steps=200\n",
      "  Ep 6/100: reward=-121.30 steps=200\n",
      "  Ep 7/100: reward=-4.73 steps=200\n",
      "  Ep 8/100: reward=-244.92 steps=200\n",
      "  Ep 9/100: reward=-124.59 steps=200\n",
      "  Ep 10/100: reward=-125.84 steps=200\n",
      "  Ep 11/100: reward=-124.73 steps=200\n",
      "  Ep 12/100: reward=-239.07 steps=200\n",
      "  Ep 13/100: reward=-4.15 steps=200\n",
      "  Ep 14/100: reward=-233.24 steps=200\n",
      "  Ep 15/100: reward=-119.53 steps=200\n",
      "  Ep 16/100: reward=-3.64 steps=200\n",
      "  Ep 17/100: reward=-235.47 steps=200\n",
      "  Ep 18/100: reward=-348.85 steps=200\n",
      "  Ep 19/100: reward=-124.51 steps=200\n",
      "  Ep 20/100: reward=-238.18 steps=200\n",
      "  Ep 21/100: reward=-3.61 steps=200\n",
      "  Ep 22/100: reward=-3.49 steps=200\n",
      "  Ep 23/100: reward=-124.66 steps=200\n",
      "  Ep 24/100: reward=-122.33 steps=200\n",
      "  Ep 25/100: reward=-229.45 steps=200\n",
      "  Ep 26/100: reward=-243.24 steps=200\n",
      "  Ep 27/100: reward=-127.60 steps=200\n",
      "  Ep 28/100: reward=-124.30 steps=200\n",
      "  Ep 29/100: reward=-226.73 steps=200\n",
      "  Ep 30/100: reward=-3.44 steps=200\n",
      "  Ep 31/100: reward=-236.82 steps=200\n",
      "  Ep 32/100: reward=-122.87 steps=200\n",
      "  Ep 33/100: reward=-120.47 steps=200\n",
      "  Ep 34/100: reward=-251.34 steps=200\n",
      "  Ep 35/100: reward=-120.08 steps=200\n",
      "  Ep 36/100: reward=-3.16 steps=200\n",
      "  Ep 37/100: reward=-346.75 steps=200\n",
      "  Ep 38/100: reward=-3.46 steps=200\n",
      "  Ep 39/100: reward=-118.12 steps=200\n",
      "  Ep 40/100: reward=-119.32 steps=200\n",
      "  Ep 41/100: reward=-3.34 steps=200\n",
      "  Ep 42/100: reward=-127.67 steps=200\n",
      "  Ep 43/100: reward=-125.39 steps=200\n",
      "  Ep 44/100: reward=-123.74 steps=200\n",
      "  Ep 45/100: reward=-236.37 steps=200\n",
      "  Ep 46/100: reward=-125.43 steps=200\n",
      "  Ep 47/100: reward=-120.92 steps=200\n",
      "  Ep 48/100: reward=-237.01 steps=200\n",
      "  Ep 49/100: reward=-119.30 steps=200\n",
      "  Ep 50/100: reward=-124.99 steps=200\n",
      "  Ep 51/100: reward=-238.83 steps=200\n",
      "  Ep 52/100: reward=-119.92 steps=200\n",
      "  Ep 53/100: reward=-120.65 steps=200\n",
      "  Ep 54/100: reward=-123.73 steps=200\n",
      "  Ep 55/100: reward=-3.32 steps=200\n",
      "  Ep 56/100: reward=-116.36 steps=200\n",
      "  Ep 57/100: reward=-127.52 steps=200\n",
      "  Ep 58/100: reward=-3.44 steps=200\n",
      "  Ep 59/100: reward=-121.11 steps=200\n",
      "  Ep 60/100: reward=-121.51 steps=200\n",
      "  Ep 61/100: reward=-242.98 steps=200\n",
      "  Ep 62/100: reward=-128.38 steps=200\n",
      "  Ep 63/100: reward=-238.05 steps=200\n",
      "  Ep 64/100: reward=-371.63 steps=200\n",
      "  Ep 65/100: reward=-124.72 steps=200\n",
      "  Ep 66/100: reward=-230.74 steps=200\n",
      "  Ep 67/100: reward=-231.25 steps=200\n",
      "  Ep 68/100: reward=-129.31 steps=200\n",
      "  Ep 69/100: reward=-122.97 steps=200\n",
      "  Ep 70/100: reward=-122.66 steps=200\n",
      "  Ep 71/100: reward=-126.48 steps=200\n",
      "  Ep 72/100: reward=-125.28 steps=200\n",
      "  Ep 73/100: reward=-120.79 steps=200\n",
      "  Ep 74/100: reward=-3.41 steps=200\n",
      "  Ep 75/100: reward=-117.34 steps=200\n",
      "  Ep 76/100: reward=-236.37 steps=200\n",
      "  Ep 77/100: reward=-327.22 steps=200\n",
      "  Ep 78/100: reward=-229.43 steps=200\n",
      "  Ep 79/100: reward=-126.43 steps=200\n",
      "  Ep 80/100: reward=-121.62 steps=200\n",
      "  Ep 81/100: reward=-240.41 steps=200\n",
      "  Ep 82/100: reward=-235.90 steps=200\n",
      "  Ep 83/100: reward=-124.92 steps=200\n",
      "  Ep 84/100: reward=-3.58 steps=200\n",
      "  Ep 85/100: reward=-128.03 steps=200\n",
      "  Ep 86/100: reward=-246.07 steps=200\n",
      "  Ep 87/100: reward=-126.40 steps=200\n",
      "  Ep 88/100: reward=-116.19 steps=200\n",
      "  Ep 89/100: reward=-119.44 steps=200\n",
      "  Ep 90/100: reward=-125.37 steps=200\n",
      "  Ep 91/100: reward=-121.67 steps=200\n",
      "  Ep 92/100: reward=-337.68 steps=200\n",
      "  Ep 93/100: reward=-231.79 steps=200\n",
      "  Ep 94/100: reward=-230.18 steps=200\n",
      "  Ep 95/100: reward=-118.85 steps=200\n",
      "  Ep 96/100: reward=-3.26 steps=200\n",
      "  Ep 97/100: reward=-248.54 steps=200\n",
      "  Ep 98/100: reward=-117.48 steps=200\n",
      "  Ep 99/100: reward=-123.22 steps=200\n",
      "  Ep 100/100: reward=-127.93 steps=200\n",
      "Finished evaluations. avg=-144.75 best=-3.15 worst=-371.63\n",
      "Recording random episode index 38 for model Pendulum_DQN_lr-0.0003.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_lr-0.0003 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_lr-0.0003\\Pendulum_DQN_lr-0.0003-episode-0.mp4\n",
      "Recording complete. Total reward: -3.72, Steps:200, Duration: 0.98 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▆▆▃▆▄▄█▆▄▆▆▁█▆█▆▆▆▆▆▃▁▆▄▄▆▆▆▆▂▆▃▄▆▆▆▆▄█▆</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-144.74652</td></tr><tr><td>best_reward</td><td>-3.15238</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-127.92665</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.97528</td></tr><tr><td>recorded_episode_index</td><td>38</td></tr><tr><td>recorded_episode_reward</td><td>-3.72053</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-371.63104</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_lr-0.0003_1763011425</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/stkwxepg' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/stkwxepg</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072345-stkwxepg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_lr-0.001.pth ===\n",
      "W&B run: test_Pendulum_DQN_lr-0.001_1763011432\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072352-7m5evale</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7m5evale' target=\"_blank\">test_Pendulum_DQN_lr-0.001_1763011432</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7m5evale' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7m5evale</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-118.80 steps=200\n",
      "  Ep 2/100: reward=-118.30 steps=200\n",
      "  Ep 3/100: reward=-5.68 steps=200\n",
      "  Ep 4/100: reward=-129.64 steps=200\n",
      "  Ep 5/100: reward=-240.46 steps=200\n",
      "  Ep 6/100: reward=-4.47 steps=200\n",
      "  Ep 7/100: reward=-128.55 steps=200\n",
      "  Ep 8/100: reward=-245.62 steps=200\n",
      "  Ep 9/100: reward=-125.01 steps=200\n",
      "  Ep 10/100: reward=-243.62 steps=200\n",
      "  Ep 11/100: reward=-3.95 steps=200\n",
      "  Ep 12/100: reward=-120.20 steps=200\n",
      "  Ep 13/100: reward=-3.60 steps=200\n",
      "  Ep 14/100: reward=-343.61 steps=200\n",
      "  Ep 15/100: reward=-248.58 steps=200\n",
      "  Ep 16/100: reward=-278.47 steps=200\n",
      "  Ep 17/100: reward=-243.88 steps=200\n",
      "  Ep 18/100: reward=-121.81 steps=200\n",
      "  Ep 19/100: reward=-116.94 steps=200\n",
      "  Ep 20/100: reward=-6.10 steps=200\n",
      "  Ep 21/100: reward=-127.77 steps=200\n",
      "  Ep 22/100: reward=-233.89 steps=200\n",
      "  Ep 23/100: reward=-243.97 steps=200\n",
      "  Ep 24/100: reward=-236.18 steps=200\n",
      "  Ep 25/100: reward=-119.81 steps=200\n",
      "  Ep 26/100: reward=-232.36 steps=200\n",
      "  Ep 27/100: reward=-120.25 steps=200\n",
      "  Ep 28/100: reward=-232.48 steps=200\n",
      "  Ep 29/100: reward=-121.20 steps=200\n",
      "  Ep 30/100: reward=-119.65 steps=200\n",
      "  Ep 31/100: reward=-116.71 steps=200\n",
      "  Ep 32/100: reward=-116.93 steps=200\n",
      "  Ep 33/100: reward=-226.85 steps=200\n",
      "  Ep 34/100: reward=-3.54 steps=200\n",
      "  Ep 35/100: reward=-3.70 steps=200\n",
      "  Ep 36/100: reward=-3.87 steps=200\n",
      "  Ep 37/100: reward=-344.20 steps=200\n",
      "  Ep 38/100: reward=-127.50 steps=200\n",
      "  Ep 39/100: reward=-251.98 steps=200\n",
      "  Ep 40/100: reward=-253.29 steps=200\n",
      "  Ep 41/100: reward=-235.90 steps=200\n",
      "  Ep 42/100: reward=-252.27 steps=200\n",
      "  Ep 43/100: reward=-127.18 steps=200\n",
      "  Ep 44/100: reward=-226.61 steps=200\n",
      "  Ep 45/100: reward=-126.56 steps=200\n",
      "  Ep 46/100: reward=-116.62 steps=200\n",
      "  Ep 47/100: reward=-121.18 steps=200\n",
      "  Ep 48/100: reward=-237.61 steps=200\n",
      "  Ep 49/100: reward=-121.57 steps=200\n",
      "  Ep 50/100: reward=-237.09 steps=200\n",
      "  Ep 51/100: reward=-3.85 steps=200\n",
      "  Ep 52/100: reward=-125.10 steps=200\n",
      "  Ep 53/100: reward=-3.78 steps=200\n",
      "  Ep 54/100: reward=-125.77 steps=200\n",
      "  Ep 55/100: reward=-117.48 steps=200\n",
      "  Ep 56/100: reward=-124.86 steps=200\n",
      "  Ep 57/100: reward=-240.18 steps=200\n",
      "  Ep 58/100: reward=-121.14 steps=200\n",
      "  Ep 59/100: reward=-127.62 steps=200\n",
      "  Ep 60/100: reward=-117.27 steps=200\n",
      "  Ep 61/100: reward=-4.31 steps=200\n",
      "  Ep 62/100: reward=-127.95 steps=200\n",
      "  Ep 63/100: reward=-127.55 steps=200\n",
      "  Ep 64/100: reward=-122.05 steps=200\n",
      "  Ep 65/100: reward=-118.13 steps=200\n",
      "  Ep 66/100: reward=-127.57 steps=200\n",
      "  Ep 67/100: reward=-4.89 steps=200\n",
      "  Ep 68/100: reward=-125.99 steps=200\n",
      "  Ep 69/100: reward=-5.33 steps=200\n",
      "  Ep 70/100: reward=-4.08 steps=200\n",
      "  Ep 71/100: reward=-227.34 steps=200\n",
      "  Ep 72/100: reward=-3.78 steps=200\n",
      "  Ep 73/100: reward=-126.30 steps=200\n",
      "  Ep 74/100: reward=-124.62 steps=200\n",
      "  Ep 75/100: reward=-251.40 steps=200\n",
      "  Ep 76/100: reward=-231.04 steps=200\n",
      "  Ep 77/100: reward=-121.24 steps=200\n",
      "  Ep 78/100: reward=-117.39 steps=200\n",
      "  Ep 79/100: reward=-122.96 steps=200\n",
      "  Ep 80/100: reward=-119.01 steps=200\n",
      "  Ep 81/100: reward=-122.01 steps=200\n",
      "  Ep 82/100: reward=-126.05 steps=200\n",
      "  Ep 83/100: reward=-237.10 steps=200\n",
      "  Ep 84/100: reward=-125.30 steps=200\n",
      "  Ep 85/100: reward=-119.68 steps=200\n",
      "  Ep 86/100: reward=-128.27 steps=200\n",
      "  Ep 87/100: reward=-121.29 steps=200\n",
      "  Ep 88/100: reward=-124.63 steps=200\n",
      "  Ep 89/100: reward=-124.16 steps=200\n",
      "  Ep 90/100: reward=-127.96 steps=200\n",
      "  Ep 91/100: reward=-120.84 steps=200\n",
      "  Ep 92/100: reward=-247.91 steps=200\n",
      "  Ep 93/100: reward=-121.46 steps=200\n",
      "  Ep 94/100: reward=-122.84 steps=200\n",
      "  Ep 95/100: reward=-235.41 steps=200\n",
      "  Ep 96/100: reward=-251.83 steps=200\n",
      "  Ep 97/100: reward=-116.34 steps=200\n",
      "  Ep 98/100: reward=-127.53 steps=200\n",
      "  Ep 99/100: reward=-235.81 steps=200\n",
      "  Ep 100/100: reward=-120.87 steps=200\n",
      "Finished evaluations. avg=-142.63 best=-3.54 worst=-344.20\n",
      "Recording random episode index 97 for model Pendulum_DQN_lr-0.001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_lr-0.001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_lr-0.001\\Pendulum_DQN_lr-0.001-episode-0.mp4\n",
      "Recording complete. Total reward: -119.50, Steps:200, Duration: 1.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>episode_reward</td><td>▅▃▅▃▆▃▆█▅▃▃▆▆▃▁▃▃▅▆▆█▆▃▅▅▆▅▅██▆▆▆▆▆▆▆▆▃▃</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-142.63335</td></tr><tr><td>best_reward</td><td>-3.54191</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-120.86515</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.01065</td></tr><tr><td>recorded_episode_index</td><td>97</td></tr><tr><td>recorded_episode_reward</td><td>-119.49588</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-344.19874</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_lr-0.001_1763011432</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7m5evale' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7m5evale</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072352-7m5evale\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_lr-0.01.pth ===\n",
      "W&B run: test_Pendulum_DQN_lr-0.01_1763011439\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072359-tqoxe465</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/tqoxe465' target=\"_blank\">test_Pendulum_DQN_lr-0.01_1763011439</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/tqoxe465' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/tqoxe465</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-119.42 steps=200\n",
      "  Ep 2/100: reward=-336.95 steps=200\n",
      "  Ep 3/100: reward=-122.39 steps=200\n",
      "  Ep 4/100: reward=-286.59 steps=200\n",
      "  Ep 5/100: reward=-233.49 steps=200\n",
      "  Ep 6/100: reward=-224.66 steps=200\n",
      "  Ep 7/100: reward=-122.29 steps=200\n",
      "  Ep 8/100: reward=-113.78 steps=200\n",
      "  Ep 9/100: reward=-120.19 steps=200\n",
      "  Ep 10/100: reward=-221.30 steps=200\n",
      "  Ep 11/100: reward=-116.06 steps=200\n",
      "  Ep 12/100: reward=-119.78 steps=200\n",
      "  Ep 13/100: reward=-119.33 steps=200\n",
      "  Ep 14/100: reward=-126.16 steps=200\n",
      "  Ep 15/100: reward=-251.26 steps=200\n",
      "  Ep 16/100: reward=-119.89 steps=200\n",
      "  Ep 17/100: reward=-127.28 steps=200\n",
      "  Ep 18/100: reward=-116.94 steps=200\n",
      "  Ep 19/100: reward=-119.45 steps=200\n",
      "  Ep 20/100: reward=-252.61 steps=200\n",
      "  Ep 21/100: reward=-121.50 steps=200\n",
      "  Ep 22/100: reward=-126.49 steps=200\n",
      "  Ep 23/100: reward=-121.08 steps=200\n",
      "  Ep 24/100: reward=-0.70 steps=200\n",
      "  Ep 25/100: reward=-117.09 steps=200\n",
      "  Ep 26/100: reward=-0.36 steps=200\n",
      "  Ep 27/100: reward=-0.69 steps=200\n",
      "  Ep 28/100: reward=-243.85 steps=200\n",
      "  Ep 29/100: reward=-114.77 steps=200\n",
      "  Ep 30/100: reward=-121.91 steps=200\n",
      "  Ep 31/100: reward=-124.67 steps=200\n",
      "  Ep 32/100: reward=-0.31 steps=200\n",
      "  Ep 33/100: reward=-125.27 steps=200\n",
      "  Ep 34/100: reward=-322.09 steps=200\n",
      "  Ep 35/100: reward=-117.29 steps=200\n",
      "  Ep 36/100: reward=-120.94 steps=200\n",
      "  Ep 37/100: reward=-1.16 steps=200\n",
      "  Ep 38/100: reward=-349.42 steps=200\n",
      "  Ep 39/100: reward=-120.18 steps=200\n",
      "  Ep 40/100: reward=-0.32 steps=200\n",
      "  Ep 41/100: reward=-126.52 steps=200\n",
      "  Ep 42/100: reward=-126.89 steps=200\n",
      "  Ep 43/100: reward=-120.55 steps=200\n",
      "  Ep 44/100: reward=-118.28 steps=200\n",
      "  Ep 45/100: reward=-124.40 steps=200\n",
      "  Ep 46/100: reward=-113.80 steps=200\n",
      "  Ep 47/100: reward=-123.56 steps=200\n",
      "  Ep 48/100: reward=-119.01 steps=200\n",
      "  Ep 49/100: reward=-121.05 steps=200\n",
      "  Ep 50/100: reward=-114.69 steps=200\n",
      "  Ep 51/100: reward=-125.76 steps=200\n",
      "  Ep 52/100: reward=-0.58 steps=200\n",
      "  Ep 53/100: reward=-237.04 steps=200\n",
      "  Ep 54/100: reward=-123.05 steps=200\n",
      "  Ep 55/100: reward=-117.15 steps=200\n",
      "  Ep 56/100: reward=-117.90 steps=200\n",
      "  Ep 57/100: reward=-119.27 steps=200\n",
      "  Ep 58/100: reward=-121.05 steps=200\n",
      "  Ep 59/100: reward=-223.22 steps=200\n",
      "  Ep 60/100: reward=-127.06 steps=200\n",
      "  Ep 61/100: reward=-124.03 steps=200\n",
      "  Ep 62/100: reward=-229.85 steps=200\n",
      "  Ep 63/100: reward=-118.69 steps=200\n",
      "  Ep 64/100: reward=-0.27 steps=200\n",
      "  Ep 65/100: reward=-0.67 steps=200\n",
      "  Ep 66/100: reward=-2.37 steps=200\n",
      "  Ep 67/100: reward=-117.49 steps=200\n",
      "  Ep 68/100: reward=-0.92 steps=200\n",
      "  Ep 69/100: reward=-237.47 steps=200\n",
      "  Ep 70/100: reward=-123.26 steps=200\n",
      "  Ep 71/100: reward=-124.20 steps=200\n",
      "  Ep 72/100: reward=-0.55 steps=200\n",
      "  Ep 73/100: reward=-236.51 steps=200\n",
      "  Ep 74/100: reward=-125.08 steps=200\n",
      "  Ep 75/100: reward=-241.30 steps=200\n",
      "  Ep 76/100: reward=-0.28 steps=200\n",
      "  Ep 77/100: reward=-231.86 steps=200\n",
      "  Ep 78/100: reward=-117.34 steps=200\n",
      "  Ep 79/100: reward=-124.00 steps=200\n",
      "  Ep 80/100: reward=-123.17 steps=200\n",
      "  Ep 81/100: reward=-121.47 steps=200\n",
      "  Ep 82/100: reward=-124.99 steps=200\n",
      "  Ep 83/100: reward=-223.66 steps=200\n",
      "  Ep 84/100: reward=-0.89 steps=200\n",
      "  Ep 85/100: reward=-114.43 steps=200\n",
      "  Ep 86/100: reward=-123.20 steps=200\n",
      "  Ep 87/100: reward=-1.89 steps=200\n",
      "  Ep 88/100: reward=-116.97 steps=200\n",
      "  Ep 89/100: reward=-330.03 steps=200\n",
      "  Ep 90/100: reward=-0.26 steps=200\n",
      "  Ep 91/100: reward=-288.68 steps=200\n",
      "  Ep 92/100: reward=-119.02 steps=200\n",
      "  Ep 93/100: reward=-123.13 steps=200\n",
      "  Ep 94/100: reward=-123.17 steps=200\n",
      "  Ep 95/100: reward=-123.76 steps=200\n",
      "  Ep 96/100: reward=-126.34 steps=200\n",
      "  Ep 97/100: reward=-119.23 steps=200\n",
      "  Ep 98/100: reward=-222.68 steps=200\n",
      "  Ep 99/100: reward=-0.32 steps=200\n",
      "  Ep 100/100: reward=-117.93 steps=200\n",
      "Finished evaluations. avg=-129.42 best=-0.26 worst=-349.42\n",
      "Recording random episode index 77 for model Pendulum_DQN_lr-0.01.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_lr-0.01 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_lr-0.01\\Pendulum_DQN_lr-0.01-episode-0.mp4\n",
      "Recording complete. Total reward: -117.39, Steps:200, Duration: 0.98 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▅▂▅▅▅▅▁█▅▅▅█▄▅▅▅▅▅▁▅▅███▅▁▅▂▅▅▅▅▅█▅█▅▅▅█</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-129.42136</td></tr><tr><td>best_reward</td><td>-0.25863</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-117.93469</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.97825</td></tr><tr><td>recorded_episode_index</td><td>77</td></tr><tr><td>recorded_episode_reward</td><td>-117.39398</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-349.42487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_lr-0.01_1763011439</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/tqoxe465' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/tqoxe465</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072359-tqoxe465\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_lr-1e-05.pth ===\n",
      "W&B run: test_Pendulum_DQN_lr-1e-05_1763011445\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072405-hn8q02gq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hn8q02gq' target=\"_blank\">test_Pendulum_DQN_lr-1e-05_1763011445</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hn8q02gq' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hn8q02gq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-1114.64 steps=200\n",
      "  Ep 2/100: reward=-1041.91 steps=200\n",
      "  Ep 3/100: reward=-1734.03 steps=200\n",
      "  Ep 4/100: reward=-1375.22 steps=200\n",
      "  Ep 5/100: reward=-1461.58 steps=200\n",
      "  Ep 6/100: reward=-1392.45 steps=200\n",
      "  Ep 7/100: reward=-1026.06 steps=200\n",
      "  Ep 8/100: reward=-1393.43 steps=200\n",
      "  Ep 9/100: reward=-1396.22 steps=200\n",
      "  Ep 10/100: reward=-1230.36 steps=200\n",
      "  Ep 11/100: reward=-1052.30 steps=200\n",
      "  Ep 12/100: reward=-1426.58 steps=200\n",
      "  Ep 13/100: reward=-1398.32 steps=200\n",
      "  Ep 14/100: reward=-1378.62 steps=200\n",
      "  Ep 15/100: reward=-1396.69 steps=200\n",
      "  Ep 16/100: reward=-1430.99 steps=200\n",
      "  Ep 17/100: reward=-1012.63 steps=200\n",
      "  Ep 18/100: reward=-1052.31 steps=200\n",
      "  Ep 19/100: reward=-1398.61 steps=200\n",
      "  Ep 20/100: reward=-1052.21 steps=200\n",
      "  Ep 21/100: reward=-1730.71 steps=200\n",
      "  Ep 22/100: reward=-1024.68 steps=200\n",
      "  Ep 23/100: reward=-901.94 steps=200\n",
      "  Ep 24/100: reward=-1049.12 steps=200\n",
      "  Ep 25/100: reward=-1022.99 steps=200\n",
      "  Ep 26/100: reward=-1304.16 steps=200\n",
      "  Ep 27/100: reward=-1456.61 steps=200\n",
      "  Ep 28/100: reward=-1433.92 steps=200\n",
      "  Ep 29/100: reward=-1173.15 steps=200\n",
      "  Ep 30/100: reward=-1433.42 steps=200\n",
      "  Ep 31/100: reward=-1507.00 steps=200\n",
      "  Ep 32/100: reward=-1406.75 steps=200\n",
      "  Ep 33/100: reward=-1738.05 steps=200\n",
      "  Ep 34/100: reward=-1735.60 steps=200\n",
      "  Ep 35/100: reward=-1050.40 steps=200\n",
      "  Ep 36/100: reward=-1341.32 steps=200\n",
      "  Ep 37/100: reward=-1339.94 steps=200\n",
      "  Ep 38/100: reward=-1208.74 steps=200\n",
      "  Ep 39/100: reward=-1471.40 steps=200\n",
      "  Ep 40/100: reward=-1052.55 steps=200\n",
      "  Ep 41/100: reward=-1482.96 steps=200\n",
      "  Ep 42/100: reward=-1421.36 steps=200\n",
      "  Ep 43/100: reward=-1367.54 steps=200\n",
      "  Ep 44/100: reward=-1721.10 steps=200\n",
      "  Ep 45/100: reward=-1568.53 steps=200\n",
      "  Ep 46/100: reward=-1289.41 steps=200\n",
      "  Ep 47/100: reward=-1094.85 steps=200\n",
      "  Ep 48/100: reward=-1418.40 steps=200\n",
      "  Ep 49/100: reward=-1196.43 steps=200\n",
      "  Ep 50/100: reward=-1322.19 steps=200\n",
      "  Ep 51/100: reward=-1477.97 steps=200\n",
      "  Ep 52/100: reward=-1569.32 steps=200\n",
      "  Ep 53/100: reward=-1479.01 steps=200\n",
      "  Ep 54/100: reward=-1054.78 steps=200\n",
      "  Ep 55/100: reward=-1008.61 steps=200\n",
      "  Ep 56/100: reward=-1043.92 steps=200\n",
      "  Ep 57/100: reward=-1423.20 steps=200\n",
      "  Ep 58/100: reward=-1014.05 steps=200\n",
      "  Ep 59/100: reward=-1041.91 steps=200\n",
      "  Ep 60/100: reward=-993.43 steps=200\n",
      "  Ep 61/100: reward=-1572.06 steps=200\n",
      "  Ep 62/100: reward=-1052.40 steps=200\n",
      "  Ep 63/100: reward=-1454.15 steps=200\n",
      "  Ep 64/100: reward=-1180.95 steps=200\n",
      "  Ep 65/100: reward=-1043.25 steps=200\n",
      "  Ep 66/100: reward=-1444.41 steps=200\n",
      "  Ep 67/100: reward=-1389.37 steps=200\n",
      "  Ep 68/100: reward=-1441.35 steps=200\n",
      "  Ep 69/100: reward=-1431.11 steps=200\n",
      "  Ep 70/100: reward=-1053.03 steps=200\n",
      "  Ep 71/100: reward=-1167.30 steps=200\n",
      "  Ep 72/100: reward=-1050.61 steps=200\n",
      "  Ep 73/100: reward=-1411.68 steps=200\n",
      "  Ep 74/100: reward=-1048.38 steps=200\n",
      "  Ep 75/100: reward=-1411.97 steps=200\n",
      "  Ep 76/100: reward=-1037.91 steps=200\n",
      "  Ep 77/100: reward=-995.35 steps=200\n",
      "  Ep 78/100: reward=-1245.48 steps=200\n",
      "  Ep 79/100: reward=-1426.11 steps=200\n",
      "  Ep 80/100: reward=-1293.26 steps=200\n",
      "  Ep 81/100: reward=-1401.42 steps=200\n",
      "  Ep 82/100: reward=-1172.15 steps=200\n",
      "  Ep 83/100: reward=-1433.50 steps=200\n",
      "  Ep 84/100: reward=-1479.94 steps=200\n",
      "  Ep 85/100: reward=-1419.52 steps=200\n",
      "  Ep 86/100: reward=-1159.60 steps=200\n",
      "  Ep 87/100: reward=-1043.94 steps=200\n",
      "  Ep 88/100: reward=-1026.05 steps=200\n",
      "  Ep 89/100: reward=-993.02 steps=200\n",
      "  Ep 90/100: reward=-1716.81 steps=200\n",
      "  Ep 91/100: reward=-1009.40 steps=200\n",
      "  Ep 92/100: reward=-1558.00 steps=200\n",
      "  Ep 93/100: reward=-1433.50 steps=200\n",
      "  Ep 94/100: reward=-1490.74 steps=200\n",
      "  Ep 95/100: reward=-1369.07 steps=200\n",
      "  Ep 96/100: reward=-935.59 steps=200\n",
      "  Ep 97/100: reward=-1117.40 steps=200\n",
      "  Ep 98/100: reward=-1523.88 steps=200\n",
      "  Ep 99/100: reward=-1047.02 steps=200\n",
      "  Ep 100/100: reward=-1050.34 steps=200\n",
      "Finished evaluations. avg=-1286.68 best=-901.94 worst=-1738.05\n",
      "Recording random episode index 13 for model Pendulum_DQN_lr-1e-05.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_lr-1e-05 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_lr-1e-05\\Pendulum_DQN_lr-1e-05-episode-0.mp4\n",
      "Recording complete. Total reward: -1027.56, Steps:200, Duration: 1.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▆▇▄▄▄▇▄▄▄▄▅▃▄▆▃▁▄▁▂▂▇▇▄▇▇▃▇▄▇▇▄▇▄▄▁▃▄█▆▇</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-1286.67624</td></tr><tr><td>best_reward</td><td>-901.93944</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-1050.34408</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.99889</td></tr><tr><td>recorded_episode_index</td><td>13</td></tr><tr><td>recorded_episode_reward</td><td>-1027.56033</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-1738.05307</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_lr-1e-05_1763011445</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hn8q02gq' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hn8q02gq</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072405-hn8q02gq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_mem-10000.pth ===\n",
      "W&B run: test_Pendulum_DQN_mem-10000_1763011451\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072411-qf3p83m8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qf3p83m8' target=\"_blank\">test_Pendulum_DQN_mem-10000_1763011451</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qf3p83m8' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qf3p83m8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-121.33 steps=200\n",
      "  Ep 2/100: reward=-2.11 steps=200\n",
      "  Ep 3/100: reward=-300.34 steps=200\n",
      "  Ep 4/100: reward=-124.17 steps=200\n",
      "  Ep 5/100: reward=-119.83 steps=200\n",
      "  Ep 6/100: reward=-120.26 steps=200\n",
      "  Ep 7/100: reward=-123.66 steps=200\n",
      "  Ep 8/100: reward=-124.27 steps=200\n",
      "  Ep 9/100: reward=-124.78 steps=200\n",
      "  Ep 10/100: reward=-2.53 steps=200\n",
      "  Ep 11/100: reward=-2.36 steps=200\n",
      "  Ep 12/100: reward=-125.49 steps=200\n",
      "  Ep 13/100: reward=-336.96 steps=200\n",
      "  Ep 14/100: reward=-124.05 steps=200\n",
      "  Ep 15/100: reward=-120.13 steps=200\n",
      "  Ep 16/100: reward=-123.69 steps=200\n",
      "  Ep 17/100: reward=-240.78 steps=200\n",
      "  Ep 18/100: reward=-328.95 steps=200\n",
      "  Ep 19/100: reward=-120.81 steps=200\n",
      "  Ep 20/100: reward=-225.62 steps=200\n",
      "  Ep 21/100: reward=-121.23 steps=200\n",
      "  Ep 22/100: reward=-122.96 steps=200\n",
      "  Ep 23/100: reward=-228.37 steps=200\n",
      "  Ep 24/100: reward=-2.96 steps=200\n",
      "  Ep 25/100: reward=-125.75 steps=200\n",
      "  Ep 26/100: reward=-115.21 steps=200\n",
      "  Ep 27/100: reward=-242.01 steps=200\n",
      "  Ep 28/100: reward=-258.71 steps=200\n",
      "  Ep 29/100: reward=-2.15 steps=200\n",
      "  Ep 30/100: reward=-114.93 steps=200\n",
      "  Ep 31/100: reward=-125.96 steps=200\n",
      "  Ep 32/100: reward=-121.43 steps=200\n",
      "  Ep 33/100: reward=-239.68 steps=200\n",
      "  Ep 34/100: reward=-124.65 steps=200\n",
      "  Ep 35/100: reward=-218.47 steps=200\n",
      "  Ep 36/100: reward=-118.84 steps=200\n",
      "  Ep 37/100: reward=-2.28 steps=200\n",
      "  Ep 38/100: reward=-247.92 steps=200\n",
      "  Ep 39/100: reward=-121.01 steps=200\n",
      "  Ep 40/100: reward=-304.06 steps=200\n",
      "  Ep 41/100: reward=-121.81 steps=200\n",
      "  Ep 42/100: reward=-229.54 steps=200\n",
      "  Ep 43/100: reward=-121.89 steps=200\n",
      "  Ep 44/100: reward=-329.50 steps=200\n",
      "  Ep 45/100: reward=-126.22 steps=200\n",
      "  Ep 46/100: reward=-3.47 steps=200\n",
      "  Ep 47/100: reward=-2.20 steps=200\n",
      "  Ep 48/100: reward=-318.49 steps=200\n",
      "  Ep 49/100: reward=-2.06 steps=200\n",
      "  Ep 50/100: reward=-125.43 steps=200\n",
      "  Ep 51/100: reward=-331.68 steps=200\n",
      "  Ep 52/100: reward=-291.15 steps=200\n",
      "  Ep 53/100: reward=-122.07 steps=200\n",
      "  Ep 54/100: reward=-2.50 steps=200\n",
      "  Ep 55/100: reward=-118.93 steps=200\n",
      "  Ep 56/100: reward=-240.08 steps=200\n",
      "  Ep 57/100: reward=-330.42 steps=200\n",
      "  Ep 58/100: reward=-125.89 steps=200\n",
      "  Ep 59/100: reward=-121.31 steps=200\n",
      "  Ep 60/100: reward=-328.45 steps=200\n",
      "  Ep 61/100: reward=-243.00 steps=200\n",
      "  Ep 62/100: reward=-123.05 steps=200\n",
      "  Ep 63/100: reward=-120.14 steps=200\n",
      "  Ep 64/100: reward=-120.18 steps=200\n",
      "  Ep 65/100: reward=-120.03 steps=200\n",
      "  Ep 66/100: reward=-120.45 steps=200\n",
      "  Ep 67/100: reward=-125.89 steps=200\n",
      "  Ep 68/100: reward=-118.57 steps=200\n",
      "  Ep 69/100: reward=-234.94 steps=200\n",
      "  Ep 70/100: reward=-229.57 steps=200\n",
      "  Ep 71/100: reward=-124.50 steps=200\n",
      "  Ep 72/100: reward=-2.74 steps=200\n",
      "  Ep 73/100: reward=-4.06 steps=200\n",
      "  Ep 74/100: reward=-120.48 steps=200\n",
      "  Ep 75/100: reward=-235.46 steps=200\n",
      "  Ep 76/100: reward=-2.11 steps=200\n",
      "  Ep 77/100: reward=-3.20 steps=200\n",
      "  Ep 78/100: reward=-123.60 steps=200\n",
      "  Ep 79/100: reward=-237.81 steps=200\n",
      "  Ep 80/100: reward=-248.56 steps=200\n",
      "  Ep 81/100: reward=-123.30 steps=200\n",
      "  Ep 82/100: reward=-121.24 steps=200\n",
      "  Ep 83/100: reward=-125.11 steps=200\n",
      "  Ep 84/100: reward=-121.64 steps=200\n",
      "  Ep 85/100: reward=-126.15 steps=200\n",
      "  Ep 86/100: reward=-120.19 steps=200\n",
      "  Ep 87/100: reward=-124.68 steps=200\n",
      "  Ep 88/100: reward=-121.54 steps=200\n",
      "  Ep 89/100: reward=-252.02 steps=200\n",
      "  Ep 90/100: reward=-2.61 steps=200\n",
      "  Ep 91/100: reward=-124.44 steps=200\n",
      "  Ep 92/100: reward=-117.52 steps=200\n",
      "  Ep 93/100: reward=-128.39 steps=200\n",
      "  Ep 94/100: reward=-120.18 steps=200\n",
      "  Ep 95/100: reward=-120.18 steps=200\n",
      "  Ep 96/100: reward=-119.26 steps=200\n",
      "  Ep 97/100: reward=-119.83 steps=200\n",
      "  Ep 98/100: reward=-118.32 steps=200\n",
      "  Ep 99/100: reward=-119.58 steps=200\n",
      "  Ep 100/100: reward=-123.46 steps=200\n",
      "Finished evaluations. avg=-143.72 best=-2.06 worst=-336.96\n",
      "Recording random episode index 13 for model Pendulum_DQN_mem-10000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_mem-10000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_mem-10000\\Pendulum_DQN_mem-10000-episode-0.mp4\n",
      "Recording complete. Total reward: -233.87, Steps:200, Duration: 0.98 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>episode_reward</td><td>▂▆▅██▁▆▅▃▁▆▅▃▃█▅▃▆▅█▁█▆▅▆▆▆▆▅▆▃▃▅▆▅▆▆▆▆▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-143.71739</td></tr><tr><td>best_reward</td><td>-2.06468</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-123.45729</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.98489</td></tr><tr><td>recorded_episode_index</td><td>13</td></tr><tr><td>recorded_episode_reward</td><td>-233.87296</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-336.96325</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_mem-10000_1763011451</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qf3p83m8' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qf3p83m8</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072411-qf3p83m8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Pendulum_DQN_mem-50000.pth ===\n",
      "W&B run: test_Pendulum_DQN_mem-50000_1763011458\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_072418-q6obwjog</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/q6obwjog' target=\"_blank\">test_Pendulum_DQN_mem-50000_1763011458</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/q6obwjog' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/q6obwjog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-298.96 steps=200\n",
      "  Ep 2/100: reward=-243.20 steps=200\n",
      "  Ep 3/100: reward=-117.87 steps=200\n",
      "  Ep 4/100: reward=-113.90 steps=200\n",
      "  Ep 5/100: reward=-126.69 steps=200\n",
      "  Ep 6/100: reward=-115.57 steps=200\n",
      "  Ep 7/100: reward=-123.60 steps=200\n",
      "  Ep 8/100: reward=-121.27 steps=200\n",
      "  Ep 9/100: reward=-123.31 steps=200\n",
      "  Ep 10/100: reward=-255.99 steps=200\n",
      "  Ep 11/100: reward=-2.12 steps=200\n",
      "  Ep 12/100: reward=-0.99 steps=200\n",
      "  Ep 13/100: reward=-124.85 steps=200\n",
      "  Ep 14/100: reward=-121.44 steps=200\n",
      "  Ep 15/100: reward=-118.77 steps=200\n",
      "  Ep 16/100: reward=-233.24 steps=200\n",
      "  Ep 17/100: reward=-0.69 steps=200\n",
      "  Ep 18/100: reward=-123.56 steps=200\n",
      "  Ep 19/100: reward=-122.59 steps=200\n",
      "  Ep 20/100: reward=-122.39 steps=200\n",
      "  Ep 21/100: reward=-124.96 steps=200\n",
      "  Ep 22/100: reward=-113.67 steps=200\n",
      "  Ep 23/100: reward=-123.48 steps=200\n",
      "  Ep 24/100: reward=-122.48 steps=200\n",
      "  Ep 25/100: reward=-120.82 steps=200\n",
      "  Ep 26/100: reward=-124.67 steps=200\n",
      "  Ep 27/100: reward=-121.37 steps=200\n",
      "  Ep 28/100: reward=-126.42 steps=200\n",
      "  Ep 29/100: reward=-227.70 steps=200\n",
      "  Ep 30/100: reward=-243.96 steps=200\n",
      "  Ep 31/100: reward=-226.26 steps=200\n",
      "  Ep 32/100: reward=-117.50 steps=200\n",
      "  Ep 33/100: reward=-123.86 steps=200\n",
      "  Ep 34/100: reward=-118.20 steps=200\n",
      "  Ep 35/100: reward=-246.11 steps=200\n",
      "  Ep 36/100: reward=-116.58 steps=200\n",
      "  Ep 37/100: reward=-0.52 steps=200\n",
      "  Ep 38/100: reward=-233.90 steps=200\n",
      "  Ep 39/100: reward=-116.59 steps=200\n",
      "  Ep 40/100: reward=-122.61 steps=200\n",
      "  Ep 41/100: reward=-117.63 steps=200\n",
      "  Ep 42/100: reward=-232.36 steps=200\n",
      "  Ep 43/100: reward=-123.26 steps=200\n",
      "  Ep 44/100: reward=-123.59 steps=200\n",
      "  Ep 45/100: reward=-124.91 steps=200\n",
      "  Ep 46/100: reward=-227.23 steps=200\n",
      "  Ep 47/100: reward=-122.42 steps=200\n",
      "  Ep 48/100: reward=-234.66 steps=200\n",
      "  Ep 49/100: reward=-236.81 steps=200\n",
      "  Ep 50/100: reward=-225.16 steps=200\n",
      "  Ep 51/100: reward=-113.83 steps=200\n",
      "  Ep 52/100: reward=-120.01 steps=200\n",
      "  Ep 53/100: reward=-226.29 steps=200\n",
      "  Ep 54/100: reward=-331.43 steps=200\n",
      "  Ep 55/100: reward=-1.15 steps=200\n",
      "  Ep 56/100: reward=-123.43 steps=200\n",
      "  Ep 57/100: reward=-116.71 steps=200\n",
      "  Ep 58/100: reward=-1.20 steps=200\n",
      "  Ep 59/100: reward=-116.52 steps=200\n",
      "  Ep 60/100: reward=-121.30 steps=200\n",
      "  Ep 61/100: reward=-0.51 steps=200\n",
      "  Ep 62/100: reward=-123.57 steps=200\n",
      "  Ep 63/100: reward=-250.76 steps=200\n",
      "  Ep 64/100: reward=-120.00 steps=200\n",
      "  Ep 65/100: reward=-0.82 steps=200\n",
      "  Ep 66/100: reward=-117.59 steps=200\n",
      "  Ep 67/100: reward=-319.57 steps=200\n",
      "  Ep 68/100: reward=-228.19 steps=200\n",
      "  Ep 69/100: reward=-2.66 steps=200\n",
      "  Ep 70/100: reward=-124.69 steps=200\n",
      "  Ep 71/100: reward=-114.41 steps=200\n",
      "  Ep 72/100: reward=-120.72 steps=200\n",
      "  Ep 73/100: reward=-236.33 steps=200\n",
      "  Ep 74/100: reward=-114.27 steps=200\n",
      "  Ep 75/100: reward=-122.60 steps=200\n",
      "  Ep 76/100: reward=-0.56 steps=200\n",
      "  Ep 77/100: reward=-0.60 steps=200\n",
      "  Ep 78/100: reward=-119.91 steps=200\n",
      "  Ep 79/100: reward=-119.69 steps=200\n",
      "  Ep 80/100: reward=-4.26 steps=200\n",
      "  Ep 81/100: reward=-125.91 steps=200\n",
      "  Ep 82/100: reward=-122.67 steps=200\n",
      "  Ep 83/100: reward=-2.26 steps=200\n",
      "  Ep 84/100: reward=-322.93 steps=200\n",
      "  Ep 85/100: reward=-117.82 steps=200\n",
      "  Ep 86/100: reward=-223.26 steps=200\n",
      "  Ep 87/100: reward=-235.74 steps=200\n",
      "  Ep 88/100: reward=-125.96 steps=200\n",
      "  Ep 89/100: reward=-243.70 steps=200\n",
      "  Ep 90/100: reward=-224.70 steps=200\n",
      "  Ep 91/100: reward=-123.71 steps=200\n",
      "  Ep 92/100: reward=-119.95 steps=200\n",
      "  Ep 93/100: reward=-123.36 steps=200\n",
      "  Ep 94/100: reward=-0.69 steps=200\n",
      "  Ep 95/100: reward=-228.07 steps=200\n",
      "  Ep 96/100: reward=-244.13 steps=200\n",
      "  Ep 97/100: reward=-122.50 steps=200\n",
      "  Ep 98/100: reward=-230.36 steps=200\n",
      "  Ep 99/100: reward=-122.91 steps=200\n",
      "  Ep 100/100: reward=-117.06 steps=200\n",
      "Finished evaluations. avg=-139.42 best=-0.51 worst=-331.43\n",
      "Recording random episode index 30 for model Pendulum_DQN_mem-50000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Pendulum-v1\\Pendulum_DQN_mem-50000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Pendulum-v1/Pendulum_DQN_mem-50000\\Pendulum_DQN_mem-50000-episode-0.mp4\n",
      "Recording complete. Total reward: -119.04, Steps:200, Duration: 0.98 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▁▅▅█▅▅▅▅▅▅▃▅▂▅▅▃▅▂▃▅▅█▅▅█▃▅▅▅█▅▅▅▂▅▅▅▅█▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-139.41974</td></tr><tr><td>best_reward</td><td>-0.51078</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-117.05921</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.97972</td></tr><tr><td>recorded_episode_index</td><td>30</td></tr><tr><td>recorded_episode_reward</td><td>-119.03702</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-331.42985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Pendulum_DQN_mem-50000_1763011458</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/q6obwjog' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/q6obwjog</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_072418-q6obwjog\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_models_folder_with_wandb(\n",
    "    env_name=\"Pendulum-v1\",\n",
    "    num_discrete_actions=5,\n",
    "    models_folder=\"./pendulum Models\",\n",
    "    num_tests=100,\n",
    "    wandb_project=\"RL A2 tests\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ff216f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_batch-128.pth ===\n",
      "W&B run: test_Acrobot_DQN_batch-128_1763008229\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▅▇▄▇▅▄█▇▇▅▇▇▆▇▇▆▇▇▇▆▇▇▇▇▇▄▅▆▇▇▇▁▆▄▄▄██▇█</td></tr><tr><td>steps</td><td>█▂▅▃▅▂▂▂▂█▂▂▄▂▄▂▂▅▁▂▁▃▂▂▄▅▄▃▂▄▂▃▅▂▅▂▂▄▂▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-78.77</td></tr><tr><td>best_reward</td><td>-62</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-63</td></tr><tr><td>steps</td><td>64</td></tr><tr><td>worst_reward</td><td>-130</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_batch-64_1763007876</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/cifbyyva' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/cifbyyva</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_062436-cifbyyva\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063029-ujgd5zi7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ujgd5zi7' target=\"_blank\">test_Acrobot_DQN_batch-128_1763008229</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ujgd5zi7' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ujgd5zi7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-95.00 steps=96\n",
      "  Ep 2/100: reward=-97.00 steps=98\n",
      "  Ep 3/100: reward=-83.00 steps=84\n",
      "  Ep 4/100: reward=-107.00 steps=108\n",
      "  Ep 5/100: reward=-129.00 steps=130\n",
      "  Ep 6/100: reward=-106.00 steps=107\n",
      "  Ep 7/100: reward=-107.00 steps=108\n",
      "  Ep 8/100: reward=-107.00 steps=108\n",
      "  Ep 9/100: reward=-97.00 steps=98\n",
      "  Ep 10/100: reward=-106.00 steps=107\n",
      "  Ep 11/100: reward=-160.00 steps=161\n",
      "  Ep 12/100: reward=-110.00 steps=111\n",
      "  Ep 13/100: reward=-92.00 steps=93\n",
      "  Ep 14/100: reward=-104.00 steps=105\n",
      "  Ep 15/100: reward=-127.00 steps=128\n",
      "  Ep 16/100: reward=-83.00 steps=84\n",
      "  Ep 17/100: reward=-127.00 steps=128\n",
      "  Ep 18/100: reward=-131.00 steps=132\n",
      "  Ep 19/100: reward=-109.00 steps=110\n",
      "  Ep 20/100: reward=-238.00 steps=239\n",
      "  Ep 21/100: reward=-78.00 steps=79\n",
      "  Ep 22/100: reward=-105.00 steps=106\n",
      "  Ep 23/100: reward=-90.00 steps=91\n",
      "  Ep 24/100: reward=-86.00 steps=87\n",
      "  Ep 25/100: reward=-157.00 steps=158\n",
      "  Ep 26/100: reward=-87.00 steps=88\n",
      "  Ep 27/100: reward=-107.00 steps=108\n",
      "  Ep 28/100: reward=-115.00 steps=116\n",
      "  Ep 29/100: reward=-80.00 steps=81\n",
      "  Ep 30/100: reward=-78.00 steps=79\n",
      "  Ep 31/100: reward=-93.00 steps=94\n",
      "  Ep 32/100: reward=-101.00 steps=102\n",
      "  Ep 33/100: reward=-124.00 steps=125\n",
      "  Ep 34/100: reward=-500.00 steps=500\n",
      "  Ep 35/100: reward=-113.00 steps=114\n",
      "  Ep 36/100: reward=-94.00 steps=95\n",
      "  Ep 37/100: reward=-118.00 steps=119\n",
      "  Ep 38/100: reward=-121.00 steps=122\n",
      "  Ep 39/100: reward=-249.00 steps=250\n",
      "  Ep 40/100: reward=-168.00 steps=169\n",
      "  Ep 41/100: reward=-278.00 steps=279\n",
      "  Ep 42/100: reward=-93.00 steps=94\n",
      "  Ep 43/100: reward=-78.00 steps=79\n",
      "  Ep 44/100: reward=-117.00 steps=118\n",
      "  Ep 45/100: reward=-84.00 steps=85\n",
      "  Ep 46/100: reward=-127.00 steps=128\n",
      "  Ep 47/100: reward=-108.00 steps=109\n",
      "  Ep 48/100: reward=-148.00 steps=149\n",
      "  Ep 49/100: reward=-112.00 steps=113\n",
      "  Ep 50/100: reward=-95.00 steps=96\n",
      "  Ep 51/100: reward=-87.00 steps=88\n",
      "  Ep 52/100: reward=-111.00 steps=112\n",
      "  Ep 53/100: reward=-87.00 steps=88\n",
      "  Ep 54/100: reward=-119.00 steps=120\n",
      "  Ep 55/100: reward=-500.00 steps=500\n",
      "  Ep 56/100: reward=-86.00 steps=87\n",
      "  Ep 57/100: reward=-81.00 steps=82\n",
      "  Ep 58/100: reward=-108.00 steps=109\n",
      "  Ep 59/100: reward=-96.00 steps=97\n",
      "  Ep 60/100: reward=-91.00 steps=92\n",
      "  Ep 61/100: reward=-108.00 steps=109\n",
      "  Ep 62/100: reward=-104.00 steps=105\n",
      "  Ep 63/100: reward=-97.00 steps=98\n",
      "  Ep 64/100: reward=-139.00 steps=140\n",
      "  Ep 65/100: reward=-87.00 steps=88\n",
      "  Ep 66/100: reward=-105.00 steps=106\n",
      "  Ep 67/100: reward=-126.00 steps=127\n",
      "  Ep 68/100: reward=-90.00 steps=91\n",
      "  Ep 69/100: reward=-80.00 steps=81\n",
      "  Ep 70/100: reward=-145.00 steps=146\n",
      "  Ep 71/100: reward=-96.00 steps=97\n",
      "  Ep 72/100: reward=-100.00 steps=101\n",
      "  Ep 73/100: reward=-168.00 steps=169\n",
      "  Ep 74/100: reward=-101.00 steps=102\n",
      "  Ep 75/100: reward=-108.00 steps=109\n",
      "  Ep 76/100: reward=-107.00 steps=108\n",
      "  Ep 77/100: reward=-113.00 steps=114\n",
      "  Ep 78/100: reward=-114.00 steps=115\n",
      "  Ep 79/100: reward=-245.00 steps=246\n",
      "  Ep 80/100: reward=-99.00 steps=100\n",
      "  Ep 81/100: reward=-133.00 steps=134\n",
      "  Ep 82/100: reward=-500.00 steps=500\n",
      "  Ep 83/100: reward=-183.00 steps=184\n",
      "  Ep 84/100: reward=-103.00 steps=104\n",
      "  Ep 85/100: reward=-218.00 steps=219\n",
      "  Ep 86/100: reward=-102.00 steps=103\n",
      "  Ep 87/100: reward=-90.00 steps=91\n",
      "  Ep 88/100: reward=-109.00 steps=110\n",
      "  Ep 89/100: reward=-109.00 steps=110\n",
      "  Ep 90/100: reward=-97.00 steps=98\n",
      "  Ep 91/100: reward=-144.00 steps=145\n",
      "  Ep 92/100: reward=-190.00 steps=191\n",
      "  Ep 93/100: reward=-106.00 steps=107\n",
      "  Ep 94/100: reward=-92.00 steps=93\n",
      "  Ep 95/100: reward=-110.00 steps=111\n",
      "  Ep 96/100: reward=-111.00 steps=112\n",
      "  Ep 97/100: reward=-104.00 steps=105\n",
      "  Ep 98/100: reward=-101.00 steps=102\n",
      "  Ep 99/100: reward=-211.00 steps=212\n",
      "  Ep 100/100: reward=-104.00 steps=105\n",
      "Finished evaluations. avg=-128.34 best=-78.00 worst=-500.00\n",
      "Recording random episode index 27 for model Acrobot_DQN_batch-128.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_batch-128 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_batch-128\\Acrobot_DQN_batch-128-episode-0.mp4\n",
      "Recording complete. Total reward: -85.00, Steps:86, Duration: 0.50 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>███▇███▇██▇██▇█▇▁▇▇▅█▇▇▇████▇▇▇▅▁▆▆██▇██</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▁▂▂▂▄▂▂▁▃▇▁▁▂▂▁▂▂▃▂▄█▂▂▁▂▁▁▂▁▃▄▂▂▅▂▂▅▂▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-128.34</td></tr><tr><td>best_reward</td><td>-78</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-104</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.50332</td></tr><tr><td>recorded_episode_index</td><td>27</td></tr><tr><td>recorded_episode_reward</td><td>-85</td></tr><tr><td>recorded_episode_steps</td><td>86</td></tr><tr><td>steps</td><td>105</td></tr><tr><td>worst_reward</td><td>-500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_batch-128_1763008229</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ujgd5zi7' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ujgd5zi7</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063029-ujgd5zi7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_batch-256.pth ===\n",
      "W&B run: test_Acrobot_DQN_batch-256_1763008237\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063037-2td3jhf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2td3jhf4' target=\"_blank\">test_Acrobot_DQN_batch-256_1763008237</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2td3jhf4' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2td3jhf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-113.00 steps=114\n",
      "  Ep 2/100: reward=-97.00 steps=98\n",
      "  Ep 3/100: reward=-88.00 steps=89\n",
      "  Ep 4/100: reward=-70.00 steps=71\n",
      "  Ep 5/100: reward=-86.00 steps=87\n",
      "  Ep 6/100: reward=-74.00 steps=75\n",
      "  Ep 7/100: reward=-204.00 steps=205\n",
      "  Ep 8/100: reward=-122.00 steps=123\n",
      "  Ep 9/100: reward=-118.00 steps=119\n",
      "  Ep 10/100: reward=-88.00 steps=89\n",
      "  Ep 11/100: reward=-75.00 steps=76\n",
      "  Ep 12/100: reward=-94.00 steps=95\n",
      "  Ep 13/100: reward=-93.00 steps=94\n",
      "  Ep 14/100: reward=-83.00 steps=84\n",
      "  Ep 15/100: reward=-82.00 steps=83\n",
      "  Ep 16/100: reward=-89.00 steps=90\n",
      "  Ep 17/100: reward=-88.00 steps=89\n",
      "  Ep 18/100: reward=-101.00 steps=102\n",
      "  Ep 19/100: reward=-236.00 steps=237\n",
      "  Ep 20/100: reward=-88.00 steps=89\n",
      "  Ep 21/100: reward=-67.00 steps=68\n",
      "  Ep 22/100: reward=-62.00 steps=63\n",
      "  Ep 23/100: reward=-100.00 steps=101\n",
      "  Ep 24/100: reward=-89.00 steps=90\n",
      "  Ep 25/100: reward=-107.00 steps=108\n",
      "  Ep 26/100: reward=-79.00 steps=80\n",
      "  Ep 27/100: reward=-82.00 steps=83\n",
      "  Ep 28/100: reward=-69.00 steps=70\n",
      "  Ep 29/100: reward=-94.00 steps=95\n",
      "  Ep 30/100: reward=-85.00 steps=86\n",
      "  Ep 31/100: reward=-113.00 steps=114\n",
      "  Ep 32/100: reward=-86.00 steps=87\n",
      "  Ep 33/100: reward=-92.00 steps=93\n",
      "  Ep 34/100: reward=-93.00 steps=94\n",
      "  Ep 35/100: reward=-82.00 steps=83\n",
      "  Ep 36/100: reward=-126.00 steps=127\n",
      "  Ep 37/100: reward=-90.00 steps=91\n",
      "  Ep 38/100: reward=-99.00 steps=100\n",
      "  Ep 39/100: reward=-91.00 steps=92\n",
      "  Ep 40/100: reward=-86.00 steps=87\n",
      "  Ep 41/100: reward=-75.00 steps=76\n",
      "  Ep 42/100: reward=-97.00 steps=98\n",
      "  Ep 43/100: reward=-87.00 steps=88\n",
      "  Ep 44/100: reward=-91.00 steps=92\n",
      "  Ep 45/100: reward=-69.00 steps=70\n",
      "  Ep 46/100: reward=-86.00 steps=87\n",
      "  Ep 47/100: reward=-228.00 steps=229\n",
      "  Ep 48/100: reward=-80.00 steps=81\n",
      "  Ep 49/100: reward=-86.00 steps=87\n",
      "  Ep 50/100: reward=-79.00 steps=80\n",
      "  Ep 51/100: reward=-68.00 steps=69\n",
      "  Ep 52/100: reward=-72.00 steps=73\n",
      "  Ep 53/100: reward=-68.00 steps=69\n",
      "  Ep 54/100: reward=-71.00 steps=72\n",
      "  Ep 55/100: reward=-73.00 steps=74\n",
      "  Ep 56/100: reward=-115.00 steps=116\n",
      "  Ep 57/100: reward=-67.00 steps=68\n",
      "  Ep 58/100: reward=-100.00 steps=101\n",
      "  Ep 59/100: reward=-73.00 steps=74\n",
      "  Ep 60/100: reward=-74.00 steps=75\n",
      "  Ep 61/100: reward=-73.00 steps=74\n",
      "  Ep 62/100: reward=-100.00 steps=101\n",
      "  Ep 63/100: reward=-97.00 steps=98\n",
      "  Ep 64/100: reward=-91.00 steps=92\n",
      "  Ep 65/100: reward=-67.00 steps=68\n",
      "  Ep 66/100: reward=-96.00 steps=97\n",
      "  Ep 67/100: reward=-86.00 steps=87\n",
      "  Ep 68/100: reward=-113.00 steps=114\n",
      "  Ep 69/100: reward=-88.00 steps=89\n",
      "  Ep 70/100: reward=-76.00 steps=77\n",
      "  Ep 71/100: reward=-80.00 steps=81\n",
      "  Ep 72/100: reward=-90.00 steps=91\n",
      "  Ep 73/100: reward=-73.00 steps=74\n",
      "  Ep 74/100: reward=-96.00 steps=97\n",
      "  Ep 75/100: reward=-90.00 steps=91\n",
      "  Ep 76/100: reward=-93.00 steps=94\n",
      "  Ep 77/100: reward=-67.00 steps=68\n",
      "  Ep 78/100: reward=-67.00 steps=68\n",
      "  Ep 79/100: reward=-61.00 steps=62\n",
      "  Ep 80/100: reward=-105.00 steps=106\n",
      "  Ep 81/100: reward=-87.00 steps=88\n",
      "  Ep 82/100: reward=-74.00 steps=75\n",
      "  Ep 83/100: reward=-71.00 steps=72\n",
      "  Ep 84/100: reward=-115.00 steps=116\n",
      "  Ep 85/100: reward=-150.00 steps=151\n",
      "  Ep 86/100: reward=-127.00 steps=128\n",
      "  Ep 87/100: reward=-92.00 steps=93\n",
      "  Ep 88/100: reward=-86.00 steps=87\n",
      "  Ep 89/100: reward=-93.00 steps=94\n",
      "  Ep 90/100: reward=-70.00 steps=71\n",
      "  Ep 91/100: reward=-68.00 steps=69\n",
      "  Ep 92/100: reward=-102.00 steps=103\n",
      "  Ep 93/100: reward=-74.00 steps=75\n",
      "  Ep 94/100: reward=-88.00 steps=89\n",
      "  Ep 95/100: reward=-74.00 steps=75\n",
      "  Ep 96/100: reward=-112.00 steps=113\n",
      "  Ep 97/100: reward=-73.00 steps=74\n",
      "  Ep 98/100: reward=-118.00 steps=119\n",
      "  Ep 99/100: reward=-121.00 steps=122\n",
      "  Ep 100/100: reward=-159.00 steps=160\n",
      "Finished evaluations. avg=-93.03 best=-61.00 worst=-236.00\n",
      "Recording random episode index 51 for model Acrobot_DQN_batch-256.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_batch-256 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_batch-256\\Acrobot_DQN_batch-256-episode-0.mp4\n",
      "Recording complete. Total reward: -98.00, Steps:99, Duration: 0.51 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>episode_reward</td><td>▆▇█▆▇▇▇▁█▇▆██▇▇▇▇▇█▇████▇▇▇█▇▇▇█▆▅▇█▇██▄</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▃▂▇▃▂▂▂█▁▃▂▂▂▃▂▂▁▂▂▂▂▁▁▂▁▂▁▂▂▁▂▁▄▂▂▂▂▁▃▅</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-93.03</td></tr><tr><td>best_reward</td><td>-61</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-159</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.50961</td></tr><tr><td>recorded_episode_index</td><td>51</td></tr><tr><td>recorded_episode_reward</td><td>-98</td></tr><tr><td>recorded_episode_steps</td><td>99</td></tr><tr><td>steps</td><td>160</td></tr><tr><td>worst_reward</td><td>-236</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_batch-256_1763008237</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2td3jhf4' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/2td3jhf4</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063037-2td3jhf4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_batch-64.pth ===\n",
      "W&B run: test_Acrobot_DQN_batch-64_1763008242\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063042-vu917loo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/vu917loo' target=\"_blank\">test_Acrobot_DQN_batch-64_1763008242</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/vu917loo' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/vu917loo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-73.00 steps=74\n",
      "  Ep 2/100: reward=-74.00 steps=75\n",
      "  Ep 3/100: reward=-71.00 steps=72\n",
      "  Ep 4/100: reward=-102.00 steps=103\n",
      "  Ep 5/100: reward=-90.00 steps=91\n",
      "  Ep 6/100: reward=-85.00 steps=86\n",
      "  Ep 7/100: reward=-77.00 steps=78\n",
      "  Ep 8/100: reward=-74.00 steps=75\n",
      "  Ep 9/100: reward=-72.00 steps=73\n",
      "  Ep 10/100: reward=-70.00 steps=71\n",
      "  Ep 11/100: reward=-81.00 steps=82\n",
      "  Ep 12/100: reward=-74.00 steps=75\n",
      "  Ep 13/100: reward=-70.00 steps=71\n",
      "  Ep 14/100: reward=-71.00 steps=72\n",
      "  Ep 15/100: reward=-70.00 steps=71\n",
      "  Ep 16/100: reward=-95.00 steps=96\n",
      "  Ep 17/100: reward=-107.00 steps=108\n",
      "  Ep 18/100: reward=-84.00 steps=85\n",
      "  Ep 19/100: reward=-71.00 steps=72\n",
      "  Ep 20/100: reward=-74.00 steps=75\n",
      "  Ep 21/100: reward=-71.00 steps=72\n",
      "  Ep 22/100: reward=-87.00 steps=88\n",
      "  Ep 23/100: reward=-86.00 steps=87\n",
      "  Ep 24/100: reward=-85.00 steps=86\n",
      "  Ep 25/100: reward=-70.00 steps=71\n",
      "  Ep 26/100: reward=-95.00 steps=96\n",
      "  Ep 27/100: reward=-81.00 steps=82\n",
      "  Ep 28/100: reward=-83.00 steps=84\n",
      "  Ep 29/100: reward=-104.00 steps=105\n",
      "  Ep 30/100: reward=-70.00 steps=71\n",
      "  Ep 31/100: reward=-84.00 steps=85\n",
      "  Ep 32/100: reward=-80.00 steps=81\n",
      "  Ep 33/100: reward=-71.00 steps=72\n",
      "  Ep 34/100: reward=-71.00 steps=72\n",
      "  Ep 35/100: reward=-74.00 steps=75\n",
      "  Ep 36/100: reward=-98.00 steps=99\n",
      "  Ep 37/100: reward=-74.00 steps=75\n",
      "  Ep 38/100: reward=-111.00 steps=112\n",
      "  Ep 39/100: reward=-74.00 steps=75\n",
      "  Ep 40/100: reward=-70.00 steps=71\n",
      "  Ep 41/100: reward=-75.00 steps=76\n",
      "  Ep 42/100: reward=-70.00 steps=71\n",
      "  Ep 43/100: reward=-75.00 steps=76\n",
      "  Ep 44/100: reward=-86.00 steps=87\n",
      "  Ep 45/100: reward=-71.00 steps=72\n",
      "  Ep 46/100: reward=-74.00 steps=75\n",
      "  Ep 47/100: reward=-71.00 steps=72\n",
      "  Ep 48/100: reward=-92.00 steps=93\n",
      "  Ep 49/100: reward=-80.00 steps=81\n",
      "  Ep 50/100: reward=-63.00 steps=64\n",
      "  Ep 51/100: reward=-74.00 steps=75\n",
      "  Ep 52/100: reward=-93.00 steps=94\n",
      "  Ep 53/100: reward=-78.00 steps=79\n",
      "  Ep 54/100: reward=-74.00 steps=75\n",
      "  Ep 55/100: reward=-71.00 steps=72\n",
      "  Ep 56/100: reward=-96.00 steps=97\n",
      "  Ep 57/100: reward=-72.00 steps=73\n",
      "  Ep 58/100: reward=-128.00 steps=129\n",
      "  Ep 59/100: reward=-70.00 steps=71\n",
      "  Ep 60/100: reward=-71.00 steps=72\n",
      "  Ep 61/100: reward=-94.00 steps=95\n",
      "  Ep 62/100: reward=-200.00 steps=201\n",
      "  Ep 63/100: reward=-69.00 steps=70\n",
      "  Ep 64/100: reward=-128.00 steps=129\n",
      "  Ep 65/100: reward=-71.00 steps=72\n",
      "  Ep 66/100: reward=-65.00 steps=66\n",
      "  Ep 67/100: reward=-71.00 steps=72\n",
      "  Ep 68/100: reward=-73.00 steps=74\n",
      "  Ep 69/100: reward=-71.00 steps=72\n",
      "  Ep 70/100: reward=-74.00 steps=75\n",
      "  Ep 71/100: reward=-70.00 steps=71\n",
      "  Ep 72/100: reward=-76.00 steps=77\n",
      "  Ep 73/100: reward=-71.00 steps=72\n",
      "  Ep 74/100: reward=-88.00 steps=89\n",
      "  Ep 75/100: reward=-91.00 steps=92\n",
      "  Ep 76/100: reward=-77.00 steps=78\n",
      "  Ep 77/100: reward=-70.00 steps=71\n",
      "  Ep 78/100: reward=-99.00 steps=100\n",
      "  Ep 79/100: reward=-83.00 steps=84\n",
      "  Ep 80/100: reward=-71.00 steps=72\n",
      "  Ep 81/100: reward=-87.00 steps=88\n",
      "  Ep 82/100: reward=-70.00 steps=71\n",
      "  Ep 83/100: reward=-71.00 steps=72\n",
      "  Ep 84/100: reward=-81.00 steps=82\n",
      "  Ep 85/100: reward=-70.00 steps=71\n",
      "  Ep 86/100: reward=-75.00 steps=76\n",
      "  Ep 87/100: reward=-74.00 steps=75\n",
      "  Ep 88/100: reward=-80.00 steps=81\n",
      "  Ep 89/100: reward=-62.00 steps=63\n",
      "  Ep 90/100: reward=-70.00 steps=71\n",
      "  Ep 91/100: reward=-132.00 steps=133\n",
      "  Ep 92/100: reward=-65.00 steps=66\n",
      "  Ep 93/100: reward=-72.00 steps=73\n",
      "  Ep 94/100: reward=-96.00 steps=97\n",
      "  Ep 95/100: reward=-70.00 steps=71\n",
      "  Ep 96/100: reward=-69.00 steps=70\n",
      "  Ep 97/100: reward=-71.00 steps=72\n",
      "  Ep 98/100: reward=-71.00 steps=72\n",
      "  Ep 99/100: reward=-90.00 steps=91\n",
      "  Ep 100/100: reward=-71.00 steps=72\n",
      "Finished evaluations. avg=-80.67 best=-62.00 worst=-200.00\n",
      "Recording random episode index 10 for model Acrobot_DQN_batch-64.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_batch-64 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_batch-64\\Acrobot_DQN_batch-64-episode-0.mp4\n",
      "Recording complete. Total reward: -70.00, Steps:71, Duration: 0.45 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▇▇▇▇▇▇▇████▇▇█▇▆▇▇█▇▇█▇█▁██▇█▇█▆▇█▇▇██▇▇</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▂▂▁▁▃▃▂▁▂▃▂▂▃▂▁▂▃▂▂▁▂▁▁▁█▁▁▂▂▃▂▁▁▁▂▂▅▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-80.67</td></tr><tr><td>best_reward</td><td>-62</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-71</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.4468</td></tr><tr><td>recorded_episode_index</td><td>10</td></tr><tr><td>recorded_episode_reward</td><td>-70</td></tr><tr><td>recorded_episode_steps</td><td>71</td></tr><tr><td>steps</td><td>72</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_batch-64_1763008242</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/vu917loo' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/vu917loo</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063042-vu917loo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_eps-1000.pth ===\n",
      "W&B run: test_Acrobot_DQN_eps-1000_1763008251\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063051-9rkpuxct</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9rkpuxct' target=\"_blank\">test_Acrobot_DQN_eps-1000_1763008251</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9rkpuxct' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9rkpuxct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-72.00 steps=73\n",
      "  Ep 2/100: reward=-88.00 steps=89\n",
      "  Ep 3/100: reward=-78.00 steps=79\n",
      "  Ep 4/100: reward=-102.00 steps=103\n",
      "  Ep 5/100: reward=-70.00 steps=71\n",
      "  Ep 6/100: reward=-75.00 steps=76\n",
      "  Ep 7/100: reward=-70.00 steps=71\n",
      "  Ep 8/100: reward=-107.00 steps=108\n",
      "  Ep 9/100: reward=-97.00 steps=98\n",
      "  Ep 10/100: reward=-84.00 steps=85\n",
      "  Ep 11/100: reward=-89.00 steps=90\n",
      "  Ep 12/100: reward=-86.00 steps=87\n",
      "  Ep 13/100: reward=-122.00 steps=123\n",
      "  Ep 14/100: reward=-77.00 steps=78\n",
      "  Ep 15/100: reward=-78.00 steps=79\n",
      "  Ep 16/100: reward=-62.00 steps=63\n",
      "  Ep 17/100: reward=-108.00 steps=109\n",
      "  Ep 18/100: reward=-63.00 steps=64\n",
      "  Ep 19/100: reward=-78.00 steps=79\n",
      "  Ep 20/100: reward=-76.00 steps=77\n",
      "  Ep 21/100: reward=-69.00 steps=70\n",
      "  Ep 22/100: reward=-75.00 steps=76\n",
      "  Ep 23/100: reward=-78.00 steps=79\n",
      "  Ep 24/100: reward=-120.00 steps=121\n",
      "  Ep 25/100: reward=-73.00 steps=74\n",
      "  Ep 26/100: reward=-136.00 steps=137\n",
      "  Ep 27/100: reward=-86.00 steps=87\n",
      "  Ep 28/100: reward=-71.00 steps=72\n",
      "  Ep 29/100: reward=-83.00 steps=84\n",
      "  Ep 30/100: reward=-75.00 steps=76\n",
      "  Ep 31/100: reward=-80.00 steps=81\n",
      "  Ep 32/100: reward=-63.00 steps=64\n",
      "  Ep 33/100: reward=-63.00 steps=64\n",
      "  Ep 34/100: reward=-88.00 steps=89\n",
      "  Ep 35/100: reward=-74.00 steps=75\n",
      "  Ep 36/100: reward=-78.00 steps=79\n",
      "  Ep 37/100: reward=-74.00 steps=75\n",
      "  Ep 38/100: reward=-78.00 steps=79\n",
      "  Ep 39/100: reward=-93.00 steps=94\n",
      "  Ep 40/100: reward=-86.00 steps=87\n",
      "  Ep 41/100: reward=-70.00 steps=71\n",
      "  Ep 42/100: reward=-70.00 steps=71\n",
      "  Ep 43/100: reward=-63.00 steps=64\n",
      "  Ep 44/100: reward=-101.00 steps=102\n",
      "  Ep 45/100: reward=-131.00 steps=132\n",
      "  Ep 46/100: reward=-62.00 steps=63\n",
      "  Ep 47/100: reward=-86.00 steps=87\n",
      "  Ep 48/100: reward=-85.00 steps=86\n",
      "  Ep 49/100: reward=-90.00 steps=91\n",
      "  Ep 50/100: reward=-63.00 steps=64\n",
      "  Ep 51/100: reward=-63.00 steps=64\n",
      "  Ep 52/100: reward=-93.00 steps=94\n",
      "  Ep 53/100: reward=-77.00 steps=78\n",
      "  Ep 54/100: reward=-69.00 steps=70\n",
      "  Ep 55/100: reward=-145.00 steps=146\n",
      "  Ep 56/100: reward=-83.00 steps=84\n",
      "  Ep 57/100: reward=-62.00 steps=63\n",
      "  Ep 58/100: reward=-102.00 steps=103\n",
      "  Ep 59/100: reward=-95.00 steps=96\n",
      "  Ep 60/100: reward=-92.00 steps=93\n",
      "  Ep 61/100: reward=-84.00 steps=85\n",
      "  Ep 62/100: reward=-85.00 steps=86\n",
      "  Ep 63/100: reward=-85.00 steps=86\n",
      "  Ep 64/100: reward=-86.00 steps=87\n",
      "  Ep 65/100: reward=-74.00 steps=75\n",
      "  Ep 66/100: reward=-83.00 steps=84\n",
      "  Ep 67/100: reward=-87.00 steps=88\n",
      "  Ep 68/100: reward=-71.00 steps=72\n",
      "  Ep 69/100: reward=-93.00 steps=94\n",
      "  Ep 70/100: reward=-89.00 steps=90\n",
      "  Ep 71/100: reward=-91.00 steps=92\n",
      "  Ep 72/100: reward=-87.00 steps=88\n",
      "  Ep 73/100: reward=-96.00 steps=97\n",
      "  Ep 74/100: reward=-76.00 steps=77\n",
      "  Ep 75/100: reward=-70.00 steps=71\n",
      "  Ep 76/100: reward=-93.00 steps=94\n",
      "  Ep 77/100: reward=-82.00 steps=83\n",
      "  Ep 78/100: reward=-83.00 steps=84\n",
      "  Ep 79/100: reward=-78.00 steps=79\n",
      "  Ep 80/100: reward=-93.00 steps=94\n",
      "  Ep 81/100: reward=-91.00 steps=92\n",
      "  Ep 82/100: reward=-70.00 steps=71\n",
      "  Ep 83/100: reward=-125.00 steps=126\n",
      "  Ep 84/100: reward=-97.00 steps=98\n",
      "  Ep 85/100: reward=-71.00 steps=72\n",
      "  Ep 86/100: reward=-63.00 steps=64\n",
      "  Ep 87/100: reward=-94.00 steps=95\n",
      "  Ep 88/100: reward=-76.00 steps=77\n",
      "  Ep 89/100: reward=-75.00 steps=76\n",
      "  Ep 90/100: reward=-71.00 steps=72\n",
      "  Ep 91/100: reward=-77.00 steps=78\n",
      "  Ep 92/100: reward=-77.00 steps=78\n",
      "  Ep 93/100: reward=-62.00 steps=63\n",
      "  Ep 94/100: reward=-82.00 steps=83\n",
      "  Ep 95/100: reward=-70.00 steps=71\n",
      "  Ep 96/100: reward=-75.00 steps=76\n",
      "  Ep 97/100: reward=-95.00 steps=96\n",
      "  Ep 98/100: reward=-95.00 steps=96\n",
      "  Ep 99/100: reward=-70.00 steps=71\n",
      "  Ep 100/100: reward=-94.00 steps=95\n",
      "Finished evaluations. avg=-83.43 best=-62.00 worst=-145.00\n",
      "Recording random episode index 98 for model Acrobot_DQN_eps-1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_eps-1000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_eps-1000\\Acrobot_DQN_eps-1000-episode-0.mp4\n",
      "Recording complete. Total reward: -76.00, Steps:77, Duration: 0.47 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>episode_reward</td><td>▇▆▇▇▇▄█▇▇▆▇▆▇▇█▆██▇▁▆▆▆▅▆▇▇▅▆▅▅▇▇▇▇█▇▇▅▇</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▄▂▆▅▄▃▃█▂▄▁▁▃▂▂▂▄▄▁▆▅▄▄▄▂▅▅▄▃▅▄▃▅▂▃▁▃▂▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-83.43</td></tr><tr><td>best_reward</td><td>-62</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-94</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.4731</td></tr><tr><td>recorded_episode_index</td><td>98</td></tr><tr><td>recorded_episode_reward</td><td>-76</td></tr><tr><td>recorded_episode_steps</td><td>77</td></tr><tr><td>steps</td><td>95</td></tr><tr><td>worst_reward</td><td>-145</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_eps-1000_1763008251</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9rkpuxct' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9rkpuxct</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063051-9rkpuxct\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_eps-2500.pth ===\n",
      "W&B run: test_Acrobot_DQN_eps-2500_1763008256\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063056-ax2jb3k1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ax2jb3k1' target=\"_blank\">test_Acrobot_DQN_eps-2500_1763008256</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ax2jb3k1' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ax2jb3k1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-75.00 steps=76\n",
      "  Ep 2/100: reward=-102.00 steps=103\n",
      "  Ep 3/100: reward=-63.00 steps=64\n",
      "  Ep 4/100: reward=-89.00 steps=90\n",
      "  Ep 5/100: reward=-74.00 steps=75\n",
      "  Ep 6/100: reward=-73.00 steps=74\n",
      "  Ep 7/100: reward=-75.00 steps=76\n",
      "  Ep 8/100: reward=-96.00 steps=97\n",
      "  Ep 9/100: reward=-84.00 steps=85\n",
      "  Ep 10/100: reward=-77.00 steps=78\n",
      "  Ep 11/100: reward=-74.00 steps=75\n",
      "  Ep 12/100: reward=-70.00 steps=71\n",
      "  Ep 13/100: reward=-70.00 steps=71\n",
      "  Ep 14/100: reward=-93.00 steps=94\n",
      "  Ep 15/100: reward=-93.00 steps=94\n",
      "  Ep 16/100: reward=-101.00 steps=102\n",
      "  Ep 17/100: reward=-90.00 steps=91\n",
      "  Ep 18/100: reward=-76.00 steps=77\n",
      "  Ep 19/100: reward=-89.00 steps=90\n",
      "  Ep 20/100: reward=-95.00 steps=96\n",
      "  Ep 21/100: reward=-80.00 steps=81\n",
      "  Ep 22/100: reward=-93.00 steps=94\n",
      "  Ep 23/100: reward=-70.00 steps=71\n",
      "  Ep 24/100: reward=-92.00 steps=93\n",
      "  Ep 25/100: reward=-122.00 steps=123\n",
      "  Ep 26/100: reward=-78.00 steps=79\n",
      "  Ep 27/100: reward=-69.00 steps=70\n",
      "  Ep 28/100: reward=-73.00 steps=74\n",
      "  Ep 29/100: reward=-77.00 steps=78\n",
      "  Ep 30/100: reward=-98.00 steps=99\n",
      "  Ep 31/100: reward=-106.00 steps=107\n",
      "  Ep 32/100: reward=-62.00 steps=63\n",
      "  Ep 33/100: reward=-79.00 steps=80\n",
      "  Ep 34/100: reward=-77.00 steps=78\n",
      "  Ep 35/100: reward=-114.00 steps=115\n",
      "  Ep 36/100: reward=-74.00 steps=75\n",
      "  Ep 37/100: reward=-63.00 steps=64\n",
      "  Ep 38/100: reward=-73.00 steps=74\n",
      "  Ep 39/100: reward=-74.00 steps=75\n",
      "  Ep 40/100: reward=-106.00 steps=107\n",
      "  Ep 41/100: reward=-77.00 steps=78\n",
      "  Ep 42/100: reward=-126.00 steps=127\n",
      "  Ep 43/100: reward=-78.00 steps=79\n",
      "  Ep 44/100: reward=-86.00 steps=87\n",
      "  Ep 45/100: reward=-74.00 steps=75\n",
      "  Ep 46/100: reward=-91.00 steps=92\n",
      "  Ep 47/100: reward=-70.00 steps=71\n",
      "  Ep 48/100: reward=-79.00 steps=80\n",
      "  Ep 49/100: reward=-99.00 steps=100\n",
      "  Ep 50/100: reward=-92.00 steps=93\n",
      "  Ep 51/100: reward=-94.00 steps=95\n",
      "  Ep 52/100: reward=-94.00 steps=95\n",
      "  Ep 53/100: reward=-95.00 steps=96\n",
      "  Ep 54/100: reward=-79.00 steps=80\n",
      "  Ep 55/100: reward=-104.00 steps=105\n",
      "  Ep 56/100: reward=-74.00 steps=75\n",
      "  Ep 57/100: reward=-74.00 steps=75\n",
      "  Ep 58/100: reward=-99.00 steps=100\n",
      "  Ep 59/100: reward=-79.00 steps=80\n",
      "  Ep 60/100: reward=-75.00 steps=76\n",
      "  Ep 61/100: reward=-81.00 steps=82\n",
      "  Ep 62/100: reward=-103.00 steps=104\n",
      "  Ep 63/100: reward=-86.00 steps=87\n",
      "  Ep 64/100: reward=-112.00 steps=113\n",
      "  Ep 65/100: reward=-75.00 steps=76\n",
      "  Ep 66/100: reward=-89.00 steps=90\n",
      "  Ep 67/100: reward=-63.00 steps=64\n",
      "  Ep 68/100: reward=-91.00 steps=92\n",
      "  Ep 69/100: reward=-91.00 steps=92\n",
      "  Ep 70/100: reward=-70.00 steps=71\n",
      "  Ep 71/100: reward=-74.00 steps=75\n",
      "  Ep 72/100: reward=-84.00 steps=85\n",
      "  Ep 73/100: reward=-77.00 steps=78\n",
      "  Ep 74/100: reward=-94.00 steps=95\n",
      "  Ep 75/100: reward=-112.00 steps=113\n",
      "  Ep 76/100: reward=-96.00 steps=97\n",
      "  Ep 77/100: reward=-85.00 steps=86\n",
      "  Ep 78/100: reward=-63.00 steps=64\n",
      "  Ep 79/100: reward=-73.00 steps=74\n",
      "  Ep 80/100: reward=-82.00 steps=83\n",
      "  Ep 81/100: reward=-78.00 steps=79\n",
      "  Ep 82/100: reward=-74.00 steps=75\n",
      "  Ep 83/100: reward=-87.00 steps=88\n",
      "  Ep 84/100: reward=-76.00 steps=77\n",
      "  Ep 85/100: reward=-78.00 steps=79\n",
      "  Ep 86/100: reward=-70.00 steps=71\n",
      "  Ep 87/100: reward=-82.00 steps=83\n",
      "  Ep 88/100: reward=-75.00 steps=76\n",
      "  Ep 89/100: reward=-69.00 steps=70\n",
      "  Ep 90/100: reward=-143.00 steps=144\n",
      "  Ep 91/100: reward=-86.00 steps=87\n",
      "  Ep 92/100: reward=-74.00 steps=75\n",
      "  Ep 93/100: reward=-108.00 steps=109\n",
      "  Ep 94/100: reward=-71.00 steps=72\n",
      "  Ep 95/100: reward=-74.00 steps=75\n",
      "  Ep 96/100: reward=-108.00 steps=109\n",
      "  Ep 97/100: reward=-117.00 steps=118\n",
      "  Ep 98/100: reward=-93.00 steps=94\n",
      "  Ep 99/100: reward=-74.00 steps=75\n",
      "  Ep 100/100: reward=-74.00 steps=75\n",
      "Finished evaluations. avg=-84.90 best=-62.00 worst=-143.00\n",
      "Recording random episode index 22 for model Acrobot_DQN_eps-2500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_eps-2500 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_eps-2500\\Acrobot_DQN_eps-2500-episode-0.mp4\n",
      "Recording complete. Total reward: -104.00, Steps:105, Duration: 0.55 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▆█▇▇▇▄▆▄▆▄▁▇▇▆▄█▆▇▆▆▅▄▅▄▄▆▆▃▅▆▅▇▆▇▆▇▅▇▃▂</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▁▃▄▃▂▄▂▃▃▂▆▂▁▂▁▂▅▃▂▄▄▂▅▂▅▃▂▄▄▃▂▄▃▂▂▂█▂▆</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-84.9</td></tr><tr><td>best_reward</td><td>-62</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-74</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.55412</td></tr><tr><td>recorded_episode_index</td><td>22</td></tr><tr><td>recorded_episode_reward</td><td>-104</td></tr><tr><td>recorded_episode_steps</td><td>105</td></tr><tr><td>steps</td><td>75</td></tr><tr><td>worst_reward</td><td>-143</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_eps-2500_1763008256</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ax2jb3k1' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ax2jb3k1</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063056-ax2jb3k1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_eps-500.pth ===\n",
      "W&B run: test_Acrobot_DQN_eps-500_1763008262\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063102-sr8h2usm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/sr8h2usm' target=\"_blank\">test_Acrobot_DQN_eps-500_1763008262</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/sr8h2usm' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/sr8h2usm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-76.00 steps=77\n",
      "  Ep 2/100: reward=-71.00 steps=72\n",
      "  Ep 3/100: reward=-71.00 steps=72\n",
      "  Ep 4/100: reward=-71.00 steps=72\n",
      "  Ep 5/100: reward=-63.00 steps=64\n",
      "  Ep 6/100: reward=-70.00 steps=71\n",
      "  Ep 7/100: reward=-70.00 steps=71\n",
      "  Ep 8/100: reward=-64.00 steps=65\n",
      "  Ep 9/100: reward=-77.00 steps=78\n",
      "  Ep 10/100: reward=-87.00 steps=88\n",
      "  Ep 11/100: reward=-65.00 steps=66\n",
      "  Ep 12/100: reward=-71.00 steps=72\n",
      "  Ep 13/100: reward=-85.00 steps=86\n",
      "  Ep 14/100: reward=-71.00 steps=72\n",
      "  Ep 15/100: reward=-64.00 steps=65\n",
      "  Ep 16/100: reward=-63.00 steps=64\n",
      "  Ep 17/100: reward=-70.00 steps=71\n",
      "  Ep 18/100: reward=-70.00 steps=71\n",
      "  Ep 19/100: reward=-88.00 steps=89\n",
      "  Ep 20/100: reward=-76.00 steps=77\n",
      "  Ep 21/100: reward=-92.00 steps=93\n",
      "  Ep 22/100: reward=-96.00 steps=97\n",
      "  Ep 23/100: reward=-63.00 steps=64\n",
      "  Ep 24/100: reward=-70.00 steps=71\n",
      "  Ep 25/100: reward=-72.00 steps=73\n",
      "  Ep 26/100: reward=-76.00 steps=77\n",
      "  Ep 27/100: reward=-119.00 steps=120\n",
      "  Ep 28/100: reward=-104.00 steps=105\n",
      "  Ep 29/100: reward=-75.00 steps=76\n",
      "  Ep 30/100: reward=-71.00 steps=72\n",
      "  Ep 31/100: reward=-70.00 steps=71\n",
      "  Ep 32/100: reward=-71.00 steps=72\n",
      "  Ep 33/100: reward=-70.00 steps=71\n",
      "  Ep 34/100: reward=-83.00 steps=84\n",
      "  Ep 35/100: reward=-68.00 steps=69\n",
      "  Ep 36/100: reward=-94.00 steps=95\n",
      "  Ep 37/100: reward=-71.00 steps=72\n",
      "  Ep 38/100: reward=-73.00 steps=74\n",
      "  Ep 39/100: reward=-85.00 steps=86\n",
      "  Ep 40/100: reward=-62.00 steps=63\n",
      "  Ep 41/100: reward=-74.00 steps=75\n",
      "  Ep 42/100: reward=-62.00 steps=63\n",
      "  Ep 43/100: reward=-70.00 steps=71\n",
      "  Ep 44/100: reward=-71.00 steps=72\n",
      "  Ep 45/100: reward=-70.00 steps=71\n",
      "  Ep 46/100: reward=-74.00 steps=75\n",
      "  Ep 47/100: reward=-86.00 steps=87\n",
      "  Ep 48/100: reward=-73.00 steps=74\n",
      "  Ep 49/100: reward=-70.00 steps=71\n",
      "  Ep 50/100: reward=-63.00 steps=64\n",
      "  Ep 51/100: reward=-64.00 steps=65\n",
      "  Ep 52/100: reward=-70.00 steps=71\n",
      "  Ep 53/100: reward=-76.00 steps=77\n",
      "  Ep 54/100: reward=-75.00 steps=76\n",
      "  Ep 55/100: reward=-74.00 steps=75\n",
      "  Ep 56/100: reward=-104.00 steps=105\n",
      "  Ep 57/100: reward=-80.00 steps=81\n",
      "  Ep 58/100: reward=-77.00 steps=78\n",
      "  Ep 59/100: reward=-70.00 steps=71\n",
      "  Ep 60/100: reward=-97.00 steps=98\n",
      "  Ep 61/100: reward=-76.00 steps=77\n",
      "  Ep 62/100: reward=-91.00 steps=92\n",
      "  Ep 63/100: reward=-75.00 steps=76\n",
      "  Ep 64/100: reward=-73.00 steps=74\n",
      "  Ep 65/100: reward=-72.00 steps=73\n",
      "  Ep 66/100: reward=-63.00 steps=64\n",
      "  Ep 67/100: reward=-69.00 steps=70\n",
      "  Ep 68/100: reward=-71.00 steps=72\n",
      "  Ep 69/100: reward=-70.00 steps=71\n",
      "  Ep 70/100: reward=-84.00 steps=85\n",
      "  Ep 71/100: reward=-94.00 steps=95\n",
      "  Ep 72/100: reward=-70.00 steps=71\n",
      "  Ep 73/100: reward=-111.00 steps=112\n",
      "  Ep 74/100: reward=-78.00 steps=79\n",
      "  Ep 75/100: reward=-63.00 steps=64\n",
      "  Ep 76/100: reward=-83.00 steps=84\n",
      "  Ep 77/100: reward=-70.00 steps=71\n",
      "  Ep 78/100: reward=-70.00 steps=71\n",
      "  Ep 79/100: reward=-69.00 steps=70\n",
      "  Ep 80/100: reward=-70.00 steps=71\n",
      "  Ep 81/100: reward=-71.00 steps=72\n",
      "  Ep 82/100: reward=-113.00 steps=114\n",
      "  Ep 83/100: reward=-94.00 steps=95\n",
      "  Ep 84/100: reward=-68.00 steps=69\n",
      "  Ep 85/100: reward=-78.00 steps=79\n",
      "  Ep 86/100: reward=-63.00 steps=64\n",
      "  Ep 87/100: reward=-76.00 steps=77\n",
      "  Ep 88/100: reward=-73.00 steps=74\n",
      "  Ep 89/100: reward=-71.00 steps=72\n",
      "  Ep 90/100: reward=-70.00 steps=71\n",
      "  Ep 91/100: reward=-71.00 steps=72\n",
      "  Ep 92/100: reward=-64.00 steps=65\n",
      "  Ep 93/100: reward=-70.00 steps=71\n",
      "  Ep 94/100: reward=-71.00 steps=72\n",
      "  Ep 95/100: reward=-71.00 steps=72\n",
      "  Ep 96/100: reward=-70.00 steps=71\n",
      "  Ep 97/100: reward=-69.00 steps=70\n",
      "  Ep 98/100: reward=-79.00 steps=80\n",
      "  Ep 99/100: reward=-249.00 steps=250\n",
      "  Ep 100/100: reward=-77.00 steps=78\n",
      "Finished evaluations. avg=-77.18 best=-62.00 worst=-249.00\n",
      "Recording random episode index 18 for model Acrobot_DQN_eps-500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_eps-500 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_eps-500\\Acrobot_DQN_eps-500-episode-0.mp4\n",
      "Recording complete. Total reward: -68.00, Steps:69, Duration: 0.43 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▇▇█▄█▇▄▆▄▃▇▇▇▃▅█▇▅▆▇▆▇▃▆▄▇▇▅▇▁▇▇▇▇▇▇█▇▇▆</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▂▂▁▂▂▅▅▂▂█▂▂▂▄▂▁▂▂▂▁▂▂▆▃▅▅▃▂▂▅▂▂▂▂▅▂▁▂▃</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-77.18</td></tr><tr><td>best_reward</td><td>-62</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-77</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.43433</td></tr><tr><td>recorded_episode_index</td><td>18</td></tr><tr><td>recorded_episode_reward</td><td>-68</td></tr><tr><td>recorded_episode_steps</td><td>69</td></tr><tr><td>steps</td><td>78</td></tr><tr><td>worst_reward</td><td>-249</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_eps-500_1763008262</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/sr8h2usm' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/sr8h2usm</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063102-sr8h2usm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_eps-5000.pth ===\n",
      "W&B run: test_Acrobot_DQN_eps-5000_1763008266\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063106-yqbwdguw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/yqbwdguw' target=\"_blank\">test_Acrobot_DQN_eps-5000_1763008266</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/yqbwdguw' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/yqbwdguw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-71.00 steps=72\n",
      "  Ep 2/100: reward=-71.00 steps=72\n",
      "  Ep 3/100: reward=-61.00 steps=62\n",
      "  Ep 4/100: reward=-70.00 steps=71\n",
      "  Ep 5/100: reward=-63.00 steps=64\n",
      "  Ep 6/100: reward=-63.00 steps=64\n",
      "  Ep 7/100: reward=-74.00 steps=75\n",
      "  Ep 8/100: reward=-62.00 steps=63\n",
      "  Ep 9/100: reward=-135.00 steps=136\n",
      "  Ep 10/100: reward=-71.00 steps=72\n",
      "  Ep 11/100: reward=-62.00 steps=63\n",
      "  Ep 12/100: reward=-111.00 steps=112\n",
      "  Ep 13/100: reward=-63.00 steps=64\n",
      "  Ep 14/100: reward=-62.00 steps=63\n",
      "  Ep 15/100: reward=-73.00 steps=74\n",
      "  Ep 16/100: reward=-71.00 steps=72\n",
      "  Ep 17/100: reward=-62.00 steps=63\n",
      "  Ep 18/100: reward=-62.00 steps=63\n",
      "  Ep 19/100: reward=-62.00 steps=63\n",
      "  Ep 20/100: reward=-62.00 steps=63\n",
      "  Ep 21/100: reward=-62.00 steps=63\n",
      "  Ep 22/100: reward=-63.00 steps=64\n",
      "  Ep 23/100: reward=-70.00 steps=71\n",
      "  Ep 24/100: reward=-62.00 steps=63\n",
      "  Ep 25/100: reward=-88.00 steps=89\n",
      "  Ep 26/100: reward=-61.00 steps=62\n",
      "  Ep 27/100: reward=-63.00 steps=64\n",
      "  Ep 28/100: reward=-62.00 steps=63\n",
      "  Ep 29/100: reward=-62.00 steps=63\n",
      "  Ep 30/100: reward=-63.00 steps=64\n",
      "  Ep 31/100: reward=-86.00 steps=87\n",
      "  Ep 32/100: reward=-62.00 steps=63\n",
      "  Ep 33/100: reward=-70.00 steps=71\n",
      "  Ep 34/100: reward=-87.00 steps=88\n",
      "  Ep 35/100: reward=-62.00 steps=63\n",
      "  Ep 36/100: reward=-86.00 steps=87\n",
      "  Ep 37/100: reward=-61.00 steps=62\n",
      "  Ep 38/100: reward=-70.00 steps=71\n",
      "  Ep 39/100: reward=-78.00 steps=79\n",
      "  Ep 40/100: reward=-63.00 steps=64\n",
      "  Ep 41/100: reward=-62.00 steps=63\n",
      "  Ep 42/100: reward=-62.00 steps=63\n",
      "  Ep 43/100: reward=-62.00 steps=63\n",
      "  Ep 44/100: reward=-62.00 steps=63\n",
      "  Ep 45/100: reward=-62.00 steps=63\n",
      "  Ep 46/100: reward=-62.00 steps=63\n",
      "  Ep 47/100: reward=-63.00 steps=64\n",
      "  Ep 48/100: reward=-62.00 steps=63\n",
      "  Ep 49/100: reward=-62.00 steps=63\n",
      "  Ep 50/100: reward=-70.00 steps=71\n",
      "  Ep 51/100: reward=-74.00 steps=75\n",
      "  Ep 52/100: reward=-93.00 steps=94\n",
      "  Ep 53/100: reward=-75.00 steps=76\n",
      "  Ep 54/100: reward=-64.00 steps=65\n",
      "  Ep 55/100: reward=-63.00 steps=64\n",
      "  Ep 56/100: reward=-71.00 steps=72\n",
      "  Ep 57/100: reward=-85.00 steps=86\n",
      "  Ep 58/100: reward=-63.00 steps=64\n",
      "  Ep 59/100: reward=-62.00 steps=63\n",
      "  Ep 60/100: reward=-62.00 steps=63\n",
      "  Ep 61/100: reward=-63.00 steps=64\n",
      "  Ep 62/100: reward=-71.00 steps=72\n",
      "  Ep 63/100: reward=-62.00 steps=63\n",
      "  Ep 64/100: reward=-63.00 steps=64\n",
      "  Ep 65/100: reward=-125.00 steps=126\n",
      "  Ep 66/100: reward=-62.00 steps=63\n",
      "  Ep 67/100: reward=-85.00 steps=86\n",
      "  Ep 68/100: reward=-62.00 steps=63\n",
      "  Ep 69/100: reward=-62.00 steps=63\n",
      "  Ep 70/100: reward=-82.00 steps=83\n",
      "  Ep 71/100: reward=-62.00 steps=63\n",
      "  Ep 72/100: reward=-100.00 steps=101\n",
      "  Ep 73/100: reward=-62.00 steps=63\n",
      "  Ep 74/100: reward=-62.00 steps=63\n",
      "  Ep 75/100: reward=-93.00 steps=94\n",
      "  Ep 76/100: reward=-71.00 steps=72\n",
      "  Ep 77/100: reward=-86.00 steps=87\n",
      "  Ep 78/100: reward=-87.00 steps=88\n",
      "  Ep 79/100: reward=-78.00 steps=79\n",
      "  Ep 80/100: reward=-87.00 steps=88\n",
      "  Ep 81/100: reward=-62.00 steps=63\n",
      "  Ep 82/100: reward=-63.00 steps=64\n",
      "  Ep 83/100: reward=-161.00 steps=162\n",
      "  Ep 84/100: reward=-63.00 steps=64\n",
      "  Ep 85/100: reward=-87.00 steps=88\n",
      "  Ep 86/100: reward=-63.00 steps=64\n",
      "  Ep 87/100: reward=-86.00 steps=87\n",
      "  Ep 88/100: reward=-63.00 steps=64\n",
      "  Ep 89/100: reward=-62.00 steps=63\n",
      "  Ep 90/100: reward=-62.00 steps=63\n",
      "  Ep 91/100: reward=-62.00 steps=63\n",
      "  Ep 92/100: reward=-70.00 steps=71\n",
      "  Ep 93/100: reward=-63.00 steps=64\n",
      "  Ep 94/100: reward=-62.00 steps=63\n",
      "  Ep 95/100: reward=-63.00 steps=64\n",
      "  Ep 96/100: reward=-72.00 steps=73\n",
      "  Ep 97/100: reward=-62.00 steps=63\n",
      "  Ep 98/100: reward=-91.00 steps=92\n",
      "  Ep 99/100: reward=-62.00 steps=63\n",
      "  Ep 100/100: reward=-71.00 steps=72\n",
      "Finished evaluations. avg=-71.28 best=-61.00 worst=-161.00\n",
      "Recording random episode index 76 for model Acrobot_DQN_eps-5000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_eps-5000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_eps-5000\\Acrobot_DQN_eps-5000-episode-0.mp4\n",
      "Recording complete. Total reward: -62.00, Steps:63, Duration: 0.43 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>episode_reward</td><td>▆▆█▅█▅▆██▆▁██▂▆█████▅▇█▆██▂████▁▂████▅█▆</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▅▁▂▁▁▁▃▁▁▁▃▁▂▂▁▁▁▁▃▂▁▂▃▁▁▁▂▁▁▁▂▃▂█▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-71.28</td></tr><tr><td>best_reward</td><td>-61</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-71</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.43445</td></tr><tr><td>recorded_episode_index</td><td>76</td></tr><tr><td>recorded_episode_reward</td><td>-62</td></tr><tr><td>recorded_episode_steps</td><td>63</td></tr><tr><td>steps</td><td>72</td></tr><tr><td>worst_reward</td><td>-161</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_eps-5000_1763008266</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/yqbwdguw' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/yqbwdguw</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063106-yqbwdguw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_gamma-0.1.pth ===\n",
      "W&B run: test_Acrobot_DQN_gamma-0.1_1763008271\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063111-1ku0i5ea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1ku0i5ea' target=\"_blank\">test_Acrobot_DQN_gamma-0.1_1763008271</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1ku0i5ea' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1ku0i5ea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-500.00 steps=500\n",
      "  Ep 2/100: reward=-500.00 steps=500\n",
      "  Ep 3/100: reward=-500.00 steps=500\n",
      "  Ep 4/100: reward=-500.00 steps=500\n",
      "  Ep 5/100: reward=-500.00 steps=500\n",
      "  Ep 6/100: reward=-500.00 steps=500\n",
      "  Ep 7/100: reward=-500.00 steps=500\n",
      "  Ep 8/100: reward=-500.00 steps=500\n",
      "  Ep 9/100: reward=-500.00 steps=500\n",
      "  Ep 10/100: reward=-500.00 steps=500\n",
      "  Ep 11/100: reward=-500.00 steps=500\n",
      "  Ep 12/100: reward=-500.00 steps=500\n",
      "  Ep 13/100: reward=-500.00 steps=500\n",
      "  Ep 14/100: reward=-500.00 steps=500\n",
      "  Ep 15/100: reward=-500.00 steps=500\n",
      "  Ep 16/100: reward=-500.00 steps=500\n",
      "  Ep 17/100: reward=-500.00 steps=500\n",
      "  Ep 18/100: reward=-500.00 steps=500\n",
      "  Ep 19/100: reward=-500.00 steps=500\n",
      "  Ep 20/100: reward=-500.00 steps=500\n",
      "  Ep 21/100: reward=-500.00 steps=500\n",
      "  Ep 22/100: reward=-500.00 steps=500\n",
      "  Ep 23/100: reward=-500.00 steps=500\n",
      "  Ep 24/100: reward=-500.00 steps=500\n",
      "  Ep 25/100: reward=-500.00 steps=500\n",
      "  Ep 26/100: reward=-500.00 steps=500\n",
      "  Ep 27/100: reward=-500.00 steps=500\n",
      "  Ep 28/100: reward=-500.00 steps=500\n",
      "  Ep 29/100: reward=-500.00 steps=500\n",
      "  Ep 30/100: reward=-500.00 steps=500\n",
      "  Ep 31/100: reward=-500.00 steps=500\n",
      "  Ep 32/100: reward=-500.00 steps=500\n",
      "  Ep 33/100: reward=-500.00 steps=500\n",
      "  Ep 34/100: reward=-500.00 steps=500\n",
      "  Ep 35/100: reward=-500.00 steps=500\n",
      "  Ep 36/100: reward=-500.00 steps=500\n",
      "  Ep 37/100: reward=-500.00 steps=500\n",
      "  Ep 38/100: reward=-500.00 steps=500\n",
      "  Ep 39/100: reward=-500.00 steps=500\n",
      "  Ep 40/100: reward=-500.00 steps=500\n",
      "  Ep 41/100: reward=-500.00 steps=500\n",
      "  Ep 42/100: reward=-500.00 steps=500\n",
      "  Ep 43/100: reward=-500.00 steps=500\n",
      "  Ep 44/100: reward=-500.00 steps=500\n",
      "  Ep 45/100: reward=-500.00 steps=500\n",
      "  Ep 46/100: reward=-500.00 steps=500\n",
      "  Ep 47/100: reward=-500.00 steps=500\n",
      "  Ep 48/100: reward=-500.00 steps=500\n",
      "  Ep 49/100: reward=-500.00 steps=500\n",
      "  Ep 50/100: reward=-500.00 steps=500\n",
      "  Ep 51/100: reward=-500.00 steps=500\n",
      "  Ep 52/100: reward=-500.00 steps=500\n",
      "  Ep 53/100: reward=-500.00 steps=500\n",
      "  Ep 54/100: reward=-500.00 steps=500\n",
      "  Ep 55/100: reward=-500.00 steps=500\n",
      "  Ep 56/100: reward=-500.00 steps=500\n",
      "  Ep 57/100: reward=-500.00 steps=500\n",
      "  Ep 58/100: reward=-500.00 steps=500\n",
      "  Ep 59/100: reward=-500.00 steps=500\n",
      "  Ep 60/100: reward=-500.00 steps=500\n",
      "  Ep 61/100: reward=-500.00 steps=500\n",
      "  Ep 62/100: reward=-500.00 steps=500\n",
      "  Ep 63/100: reward=-500.00 steps=500\n",
      "  Ep 64/100: reward=-500.00 steps=500\n",
      "  Ep 65/100: reward=-500.00 steps=500\n",
      "  Ep 66/100: reward=-500.00 steps=500\n",
      "  Ep 67/100: reward=-500.00 steps=500\n",
      "  Ep 68/100: reward=-500.00 steps=500\n",
      "  Ep 69/100: reward=-500.00 steps=500\n",
      "  Ep 70/100: reward=-500.00 steps=500\n",
      "  Ep 71/100: reward=-500.00 steps=500\n",
      "  Ep 72/100: reward=-500.00 steps=500\n",
      "  Ep 73/100: reward=-500.00 steps=500\n",
      "  Ep 74/100: reward=-500.00 steps=500\n",
      "  Ep 75/100: reward=-500.00 steps=500\n",
      "  Ep 76/100: reward=-500.00 steps=500\n",
      "  Ep 77/100: reward=-500.00 steps=500\n",
      "  Ep 78/100: reward=-500.00 steps=500\n",
      "  Ep 79/100: reward=-500.00 steps=500\n",
      "  Ep 80/100: reward=-500.00 steps=500\n",
      "  Ep 81/100: reward=-500.00 steps=500\n",
      "  Ep 82/100: reward=-500.00 steps=500\n",
      "  Ep 83/100: reward=-500.00 steps=500\n",
      "  Ep 84/100: reward=-500.00 steps=500\n",
      "  Ep 85/100: reward=-500.00 steps=500\n",
      "  Ep 86/100: reward=-500.00 steps=500\n",
      "  Ep 87/100: reward=-500.00 steps=500\n",
      "  Ep 88/100: reward=-500.00 steps=500\n",
      "  Ep 89/100: reward=-500.00 steps=500\n",
      "  Ep 90/100: reward=-500.00 steps=500\n",
      "  Ep 91/100: reward=-500.00 steps=500\n",
      "  Ep 92/100: reward=-500.00 steps=500\n",
      "  Ep 93/100: reward=-500.00 steps=500\n",
      "  Ep 94/100: reward=-500.00 steps=500\n",
      "  Ep 95/100: reward=-500.00 steps=500\n",
      "  Ep 96/100: reward=-500.00 steps=500\n",
      "  Ep 97/100: reward=-500.00 steps=500\n",
      "  Ep 98/100: reward=-500.00 steps=500\n",
      "  Ep 99/100: reward=-500.00 steps=500\n",
      "  Ep 100/100: reward=-500.00 steps=500\n",
      "Finished evaluations. avg=-500.00 best=-500.00 worst=-500.00\n",
      "Recording random episode index 69 for model Acrobot_DQN_gamma-0.1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_gamma-0.1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_gamma-0.1\\Acrobot_DQN_gamma-0.1-episode-0.mp4\n",
      "Recording complete. Total reward: -500.00, Steps:500, Duration: 1.64 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-500</td></tr><tr><td>best_reward</td><td>-500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.6383</td></tr><tr><td>recorded_episode_index</td><td>69</td></tr><tr><td>recorded_episode_reward</td><td>-500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>-500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_gamma-0.1_1763008271</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1ku0i5ea' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/1ku0i5ea</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063111-1ku0i5ea\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_gamma-0.5.pth ===\n",
      "W&B run: test_Acrobot_DQN_gamma-0.5_1763008284\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063124-rrwl0d4v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/rrwl0d4v' target=\"_blank\">test_Acrobot_DQN_gamma-0.5_1763008284</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/rrwl0d4v' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/rrwl0d4v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-500.00 steps=500\n",
      "  Ep 2/100: reward=-500.00 steps=500\n",
      "  Ep 3/100: reward=-500.00 steps=500\n",
      "  Ep 4/100: reward=-500.00 steps=500\n",
      "  Ep 5/100: reward=-500.00 steps=500\n",
      "  Ep 6/100: reward=-500.00 steps=500\n",
      "  Ep 7/100: reward=-500.00 steps=500\n",
      "  Ep 8/100: reward=-500.00 steps=500\n",
      "  Ep 9/100: reward=-500.00 steps=500\n",
      "  Ep 10/100: reward=-500.00 steps=500\n",
      "  Ep 11/100: reward=-500.00 steps=500\n",
      "  Ep 12/100: reward=-500.00 steps=500\n",
      "  Ep 13/100: reward=-500.00 steps=500\n",
      "  Ep 14/100: reward=-500.00 steps=500\n",
      "  Ep 15/100: reward=-500.00 steps=500\n",
      "  Ep 16/100: reward=-500.00 steps=500\n",
      "  Ep 17/100: reward=-500.00 steps=500\n",
      "  Ep 18/100: reward=-500.00 steps=500\n",
      "  Ep 19/100: reward=-500.00 steps=500\n",
      "  Ep 20/100: reward=-500.00 steps=500\n",
      "  Ep 21/100: reward=-500.00 steps=500\n",
      "  Ep 22/100: reward=-500.00 steps=500\n",
      "  Ep 23/100: reward=-500.00 steps=500\n",
      "  Ep 24/100: reward=-500.00 steps=500\n",
      "  Ep 25/100: reward=-500.00 steps=500\n",
      "  Ep 26/100: reward=-500.00 steps=500\n",
      "  Ep 27/100: reward=-500.00 steps=500\n",
      "  Ep 28/100: reward=-500.00 steps=500\n",
      "  Ep 29/100: reward=-500.00 steps=500\n",
      "  Ep 30/100: reward=-500.00 steps=500\n",
      "  Ep 31/100: reward=-500.00 steps=500\n",
      "  Ep 32/100: reward=-500.00 steps=500\n",
      "  Ep 33/100: reward=-500.00 steps=500\n",
      "  Ep 34/100: reward=-500.00 steps=500\n",
      "  Ep 35/100: reward=-500.00 steps=500\n",
      "  Ep 36/100: reward=-500.00 steps=500\n",
      "  Ep 37/100: reward=-500.00 steps=500\n",
      "  Ep 38/100: reward=-500.00 steps=500\n",
      "  Ep 39/100: reward=-500.00 steps=500\n",
      "  Ep 40/100: reward=-500.00 steps=500\n",
      "  Ep 41/100: reward=-500.00 steps=500\n",
      "  Ep 42/100: reward=-500.00 steps=500\n",
      "  Ep 43/100: reward=-500.00 steps=500\n",
      "  Ep 44/100: reward=-500.00 steps=500\n",
      "  Ep 45/100: reward=-500.00 steps=500\n",
      "  Ep 46/100: reward=-500.00 steps=500\n",
      "  Ep 47/100: reward=-500.00 steps=500\n",
      "  Ep 48/100: reward=-500.00 steps=500\n",
      "  Ep 49/100: reward=-500.00 steps=500\n",
      "  Ep 50/100: reward=-500.00 steps=500\n",
      "  Ep 51/100: reward=-500.00 steps=500\n",
      "  Ep 52/100: reward=-500.00 steps=500\n",
      "  Ep 53/100: reward=-500.00 steps=500\n",
      "  Ep 54/100: reward=-500.00 steps=500\n",
      "  Ep 55/100: reward=-500.00 steps=500\n",
      "  Ep 56/100: reward=-500.00 steps=500\n",
      "  Ep 57/100: reward=-500.00 steps=500\n",
      "  Ep 58/100: reward=-500.00 steps=500\n",
      "  Ep 59/100: reward=-500.00 steps=500\n",
      "  Ep 60/100: reward=-500.00 steps=500\n",
      "  Ep 61/100: reward=-500.00 steps=500\n",
      "  Ep 62/100: reward=-500.00 steps=500\n",
      "  Ep 63/100: reward=-500.00 steps=500\n",
      "  Ep 64/100: reward=-500.00 steps=500\n",
      "  Ep 65/100: reward=-500.00 steps=500\n",
      "  Ep 66/100: reward=-500.00 steps=500\n",
      "  Ep 67/100: reward=-500.00 steps=500\n",
      "  Ep 68/100: reward=-500.00 steps=500\n",
      "  Ep 69/100: reward=-500.00 steps=500\n",
      "  Ep 70/100: reward=-500.00 steps=500\n",
      "  Ep 71/100: reward=-500.00 steps=500\n",
      "  Ep 72/100: reward=-500.00 steps=500\n",
      "  Ep 73/100: reward=-500.00 steps=500\n",
      "  Ep 74/100: reward=-500.00 steps=500\n",
      "  Ep 75/100: reward=-500.00 steps=500\n",
      "  Ep 76/100: reward=-500.00 steps=500\n",
      "  Ep 77/100: reward=-500.00 steps=500\n",
      "  Ep 78/100: reward=-500.00 steps=500\n",
      "  Ep 79/100: reward=-500.00 steps=500\n",
      "  Ep 80/100: reward=-500.00 steps=500\n",
      "  Ep 81/100: reward=-500.00 steps=500\n",
      "  Ep 82/100: reward=-500.00 steps=500\n",
      "  Ep 83/100: reward=-500.00 steps=500\n",
      "  Ep 84/100: reward=-500.00 steps=500\n",
      "  Ep 85/100: reward=-500.00 steps=500\n",
      "  Ep 86/100: reward=-500.00 steps=500\n",
      "  Ep 87/100: reward=-500.00 steps=500\n",
      "  Ep 88/100: reward=-500.00 steps=500\n",
      "  Ep 89/100: reward=-500.00 steps=500\n",
      "  Ep 90/100: reward=-500.00 steps=500\n",
      "  Ep 91/100: reward=-500.00 steps=500\n",
      "  Ep 92/100: reward=-500.00 steps=500\n",
      "  Ep 93/100: reward=-500.00 steps=500\n",
      "  Ep 94/100: reward=-500.00 steps=500\n",
      "  Ep 95/100: reward=-500.00 steps=500\n",
      "  Ep 96/100: reward=-500.00 steps=500\n",
      "  Ep 97/100: reward=-500.00 steps=500\n",
      "  Ep 98/100: reward=-500.00 steps=500\n",
      "  Ep 99/100: reward=-500.00 steps=500\n",
      "  Ep 100/100: reward=-500.00 steps=500\n",
      "Finished evaluations. avg=-500.00 best=-500.00 worst=-500.00\n",
      "Recording random episode index 91 for model Acrobot_DQN_gamma-0.5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_gamma-0.5 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_gamma-0.5\\Acrobot_DQN_gamma-0.5-episode-0.mp4\n",
      "Recording complete. Total reward: -500.00, Steps:500, Duration: 1.74 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-500</td></tr><tr><td>best_reward</td><td>-500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.74282</td></tr><tr><td>recorded_episode_index</td><td>91</td></tr><tr><td>recorded_episode_reward</td><td>-500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>-500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_gamma-0.5_1763008284</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/rrwl0d4v' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/rrwl0d4v</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063124-rrwl0d4v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_gamma-0.9.pth ===\n",
      "W&B run: test_Acrobot_DQN_gamma-0.9_1763008297\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063137-uhwzfho4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uhwzfho4' target=\"_blank\">test_Acrobot_DQN_gamma-0.9_1763008297</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uhwzfho4' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uhwzfho4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-500.00 steps=500\n",
      "  Ep 2/100: reward=-500.00 steps=500\n",
      "  Ep 3/100: reward=-500.00 steps=500\n",
      "  Ep 4/100: reward=-500.00 steps=500\n",
      "  Ep 5/100: reward=-500.00 steps=500\n",
      "  Ep 6/100: reward=-500.00 steps=500\n",
      "  Ep 7/100: reward=-500.00 steps=500\n",
      "  Ep 8/100: reward=-500.00 steps=500\n",
      "  Ep 9/100: reward=-500.00 steps=500\n",
      "  Ep 10/100: reward=-500.00 steps=500\n",
      "  Ep 11/100: reward=-500.00 steps=500\n",
      "  Ep 12/100: reward=-500.00 steps=500\n",
      "  Ep 13/100: reward=-500.00 steps=500\n",
      "  Ep 14/100: reward=-500.00 steps=500\n",
      "  Ep 15/100: reward=-325.00 steps=326\n",
      "  Ep 16/100: reward=-500.00 steps=500\n",
      "  Ep 17/100: reward=-500.00 steps=500\n",
      "  Ep 18/100: reward=-500.00 steps=500\n",
      "  Ep 19/100: reward=-500.00 steps=500\n",
      "  Ep 20/100: reward=-500.00 steps=500\n",
      "  Ep 21/100: reward=-500.00 steps=500\n",
      "  Ep 22/100: reward=-500.00 steps=500\n",
      "  Ep 23/100: reward=-500.00 steps=500\n",
      "  Ep 24/100: reward=-500.00 steps=500\n",
      "  Ep 25/100: reward=-500.00 steps=500\n",
      "  Ep 26/100: reward=-500.00 steps=500\n",
      "  Ep 27/100: reward=-500.00 steps=500\n",
      "  Ep 28/100: reward=-500.00 steps=500\n",
      "  Ep 29/100: reward=-500.00 steps=500\n",
      "  Ep 30/100: reward=-500.00 steps=500\n",
      "  Ep 31/100: reward=-500.00 steps=500\n",
      "  Ep 32/100: reward=-435.00 steps=436\n",
      "  Ep 33/100: reward=-500.00 steps=500\n",
      "  Ep 34/100: reward=-396.00 steps=397\n",
      "  Ep 35/100: reward=-500.00 steps=500\n",
      "  Ep 36/100: reward=-500.00 steps=500\n",
      "  Ep 37/100: reward=-500.00 steps=500\n",
      "  Ep 38/100: reward=-500.00 steps=500\n",
      "  Ep 39/100: reward=-500.00 steps=500\n",
      "  Ep 40/100: reward=-500.00 steps=500\n",
      "  Ep 41/100: reward=-500.00 steps=500\n",
      "  Ep 42/100: reward=-500.00 steps=500\n",
      "  Ep 43/100: reward=-500.00 steps=500\n",
      "  Ep 44/100: reward=-500.00 steps=500\n",
      "  Ep 45/100: reward=-500.00 steps=500\n",
      "  Ep 46/100: reward=-500.00 steps=500\n",
      "  Ep 47/100: reward=-500.00 steps=500\n",
      "  Ep 48/100: reward=-500.00 steps=500\n",
      "  Ep 49/100: reward=-500.00 steps=500\n",
      "  Ep 50/100: reward=-500.00 steps=500\n",
      "  Ep 51/100: reward=-500.00 steps=500\n",
      "  Ep 52/100: reward=-500.00 steps=500\n",
      "  Ep 53/100: reward=-500.00 steps=500\n",
      "  Ep 54/100: reward=-500.00 steps=500\n",
      "  Ep 55/100: reward=-500.00 steps=500\n",
      "  Ep 56/100: reward=-500.00 steps=500\n",
      "  Ep 57/100: reward=-500.00 steps=500\n",
      "  Ep 58/100: reward=-500.00 steps=500\n",
      "  Ep 59/100: reward=-500.00 steps=500\n",
      "  Ep 60/100: reward=-321.00 steps=322\n",
      "  Ep 61/100: reward=-500.00 steps=500\n",
      "  Ep 62/100: reward=-500.00 steps=500\n",
      "  Ep 63/100: reward=-308.00 steps=309\n",
      "  Ep 64/100: reward=-500.00 steps=500\n",
      "  Ep 65/100: reward=-385.00 steps=386\n",
      "  Ep 66/100: reward=-500.00 steps=500\n",
      "  Ep 67/100: reward=-500.00 steps=500\n",
      "  Ep 68/100: reward=-500.00 steps=500\n",
      "  Ep 69/100: reward=-500.00 steps=500\n",
      "  Ep 70/100: reward=-500.00 steps=500\n",
      "  Ep 71/100: reward=-500.00 steps=500\n",
      "  Ep 72/100: reward=-500.00 steps=500\n",
      "  Ep 73/100: reward=-500.00 steps=500\n",
      "  Ep 74/100: reward=-500.00 steps=500\n",
      "  Ep 75/100: reward=-500.00 steps=500\n",
      "  Ep 76/100: reward=-500.00 steps=500\n",
      "  Ep 77/100: reward=-500.00 steps=500\n",
      "  Ep 78/100: reward=-500.00 steps=500\n",
      "  Ep 79/100: reward=-500.00 steps=500\n",
      "  Ep 80/100: reward=-500.00 steps=500\n",
      "  Ep 81/100: reward=-500.00 steps=500\n",
      "  Ep 82/100: reward=-500.00 steps=500\n",
      "  Ep 83/100: reward=-500.00 steps=500\n",
      "  Ep 84/100: reward=-347.00 steps=348\n",
      "  Ep 85/100: reward=-500.00 steps=500\n",
      "  Ep 86/100: reward=-500.00 steps=500\n",
      "  Ep 87/100: reward=-500.00 steps=500\n",
      "  Ep 88/100: reward=-500.00 steps=500\n",
      "  Ep 89/100: reward=-500.00 steps=500\n",
      "  Ep 90/100: reward=-500.00 steps=500\n",
      "  Ep 91/100: reward=-500.00 steps=500\n",
      "  Ep 92/100: reward=-500.00 steps=500\n",
      "  Ep 93/100: reward=-500.00 steps=500\n",
      "  Ep 94/100: reward=-500.00 steps=500\n",
      "  Ep 95/100: reward=-500.00 steps=500\n",
      "  Ep 96/100: reward=-500.00 steps=500\n",
      "  Ep 97/100: reward=-500.00 steps=500\n",
      "  Ep 98/100: reward=-500.00 steps=500\n",
      "  Ep 99/100: reward=-500.00 steps=500\n",
      "  Ep 100/100: reward=-500.00 steps=500\n",
      "Finished evaluations. avg=-490.17 best=-308.00 worst=-500.00\n",
      "Recording random episode index 17 for model Acrobot_DQN_gamma-0.9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_gamma-0.9 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_gamma-0.9\\Acrobot_DQN_gamma-0.9-episode-0.mp4\n",
      "Recording complete. Total reward: -500.00, Steps:500, Duration: 1.54 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>██████████████▄██████████▁██████████████</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-490.17</td></tr><tr><td>best_reward</td><td>-308</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.53883</td></tr><tr><td>recorded_episode_index</td><td>17</td></tr><tr><td>recorded_episode_reward</td><td>-500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>-500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_gamma-0.9_1763008297</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uhwzfho4' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uhwzfho4</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063137-uhwzfho4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_gamma-0.99.pth ===\n",
      "W&B run: test_Acrobot_DQN_gamma-0.99_1763008310\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063150-qo1vrz4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qo1vrz4i' target=\"_blank\">test_Acrobot_DQN_gamma-0.99_1763008310</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qo1vrz4i' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qo1vrz4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-70.00 steps=71\n",
      "  Ep 2/100: reward=-69.00 steps=70\n",
      "  Ep 3/100: reward=-68.00 steps=69\n",
      "  Ep 4/100: reward=-88.00 steps=89\n",
      "  Ep 5/100: reward=-74.00 steps=75\n",
      "  Ep 6/100: reward=-83.00 steps=84\n",
      "  Ep 7/100: reward=-133.00 steps=134\n",
      "  Ep 8/100: reward=-78.00 steps=79\n",
      "  Ep 9/100: reward=-87.00 steps=88\n",
      "  Ep 10/100: reward=-89.00 steps=90\n",
      "  Ep 11/100: reward=-94.00 steps=95\n",
      "  Ep 12/100: reward=-86.00 steps=87\n",
      "  Ep 13/100: reward=-110.00 steps=111\n",
      "  Ep 14/100: reward=-78.00 steps=79\n",
      "  Ep 15/100: reward=-89.00 steps=90\n",
      "  Ep 16/100: reward=-89.00 steps=90\n",
      "  Ep 17/100: reward=-78.00 steps=79\n",
      "  Ep 18/100: reward=-78.00 steps=79\n",
      "  Ep 19/100: reward=-80.00 steps=81\n",
      "  Ep 20/100: reward=-70.00 steps=71\n",
      "  Ep 21/100: reward=-79.00 steps=80\n",
      "  Ep 22/100: reward=-80.00 steps=81\n",
      "  Ep 23/100: reward=-62.00 steps=63\n",
      "  Ep 24/100: reward=-98.00 steps=99\n",
      "  Ep 25/100: reward=-107.00 steps=108\n",
      "  Ep 26/100: reward=-96.00 steps=97\n",
      "  Ep 27/100: reward=-86.00 steps=87\n",
      "  Ep 28/100: reward=-74.00 steps=75\n",
      "  Ep 29/100: reward=-97.00 steps=98\n",
      "  Ep 30/100: reward=-75.00 steps=76\n",
      "  Ep 31/100: reward=-71.00 steps=72\n",
      "  Ep 32/100: reward=-95.00 steps=96\n",
      "  Ep 33/100: reward=-126.00 steps=127\n",
      "  Ep 34/100: reward=-70.00 steps=71\n",
      "  Ep 35/100: reward=-116.00 steps=117\n",
      "  Ep 36/100: reward=-123.00 steps=124\n",
      "  Ep 37/100: reward=-91.00 steps=92\n",
      "  Ep 38/100: reward=-69.00 steps=70\n",
      "  Ep 39/100: reward=-96.00 steps=97\n",
      "  Ep 40/100: reward=-93.00 steps=94\n",
      "  Ep 41/100: reward=-70.00 steps=71\n",
      "  Ep 42/100: reward=-95.00 steps=96\n",
      "  Ep 43/100: reward=-69.00 steps=70\n",
      "  Ep 44/100: reward=-103.00 steps=104\n",
      "  Ep 45/100: reward=-77.00 steps=78\n",
      "  Ep 46/100: reward=-103.00 steps=104\n",
      "  Ep 47/100: reward=-82.00 steps=83\n",
      "  Ep 48/100: reward=-264.00 steps=265\n",
      "  Ep 49/100: reward=-94.00 steps=95\n",
      "  Ep 50/100: reward=-87.00 steps=88\n",
      "  Ep 51/100: reward=-105.00 steps=106\n",
      "  Ep 52/100: reward=-75.00 steps=76\n",
      "  Ep 53/100: reward=-114.00 steps=115\n",
      "  Ep 54/100: reward=-69.00 steps=70\n",
      "  Ep 55/100: reward=-80.00 steps=81\n",
      "  Ep 56/100: reward=-62.00 steps=63\n",
      "  Ep 57/100: reward=-63.00 steps=64\n",
      "  Ep 58/100: reward=-94.00 steps=95\n",
      "  Ep 59/100: reward=-111.00 steps=112\n",
      "  Ep 60/100: reward=-92.00 steps=93\n",
      "  Ep 61/100: reward=-70.00 steps=71\n",
      "  Ep 62/100: reward=-70.00 steps=71\n",
      "  Ep 63/100: reward=-113.00 steps=114\n",
      "  Ep 64/100: reward=-70.00 steps=71\n",
      "  Ep 65/100: reward=-69.00 steps=70\n",
      "  Ep 66/100: reward=-85.00 steps=86\n",
      "  Ep 67/100: reward=-62.00 steps=63\n",
      "  Ep 68/100: reward=-71.00 steps=72\n",
      "  Ep 69/100: reward=-63.00 steps=64\n",
      "  Ep 70/100: reward=-76.00 steps=77\n",
      "  Ep 71/100: reward=-84.00 steps=85\n",
      "  Ep 72/100: reward=-89.00 steps=90\n",
      "  Ep 73/100: reward=-110.00 steps=111\n",
      "  Ep 74/100: reward=-106.00 steps=107\n",
      "  Ep 75/100: reward=-204.00 steps=205\n",
      "  Ep 76/100: reward=-87.00 steps=88\n",
      "  Ep 77/100: reward=-88.00 steps=89\n",
      "  Ep 78/100: reward=-92.00 steps=93\n",
      "  Ep 79/100: reward=-92.00 steps=93\n",
      "  Ep 80/100: reward=-63.00 steps=64\n",
      "  Ep 81/100: reward=-94.00 steps=95\n",
      "  Ep 82/100: reward=-68.00 steps=69\n",
      "  Ep 83/100: reward=-98.00 steps=99\n",
      "  Ep 84/100: reward=-101.00 steps=102\n",
      "  Ep 85/100: reward=-62.00 steps=63\n",
      "  Ep 86/100: reward=-63.00 steps=64\n",
      "  Ep 87/100: reward=-69.00 steps=70\n",
      "  Ep 88/100: reward=-80.00 steps=81\n",
      "  Ep 89/100: reward=-69.00 steps=70\n",
      "  Ep 90/100: reward=-162.00 steps=163\n",
      "  Ep 91/100: reward=-87.00 steps=88\n",
      "  Ep 92/100: reward=-70.00 steps=71\n",
      "  Ep 93/100: reward=-100.00 steps=101\n",
      "  Ep 94/100: reward=-79.00 steps=80\n",
      "  Ep 95/100: reward=-106.00 steps=107\n",
      "  Ep 96/100: reward=-61.00 steps=62\n",
      "  Ep 97/100: reward=-73.00 steps=74\n",
      "  Ep 98/100: reward=-101.00 steps=102\n",
      "  Ep 99/100: reward=-74.00 steps=75\n",
      "  Ep 100/100: reward=-74.00 steps=75\n",
      "Finished evaluations. avg=-88.59 best=-61.00 worst=-264.00\n",
      "Recording random episode index 76 for model Acrobot_DQN_gamma-0.99.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_gamma-0.99 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_gamma-0.99\\Acrobot_DQN_gamma-0.99-episode-0.mp4\n",
      "Recording complete. Total reward: -76.00, Steps:77, Duration: 0.45 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▅▇▅▄▅▅█▄▃▄▂▁▅▇▄▆▆▅▃▂▂▅▇▇▇▇▆▅▅▅▄█▇▆▇▅▇▄▆▇</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▃▂▂▂▂▁▄▃▂▂▅▅▃▃▁▂▄▂▂▅▁▁▃▄▃▁▃▂▃▃▃▃▃▁▄▁█▂▄▄</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-88.59</td></tr><tr><td>best_reward</td><td>-61</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-74</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.45198</td></tr><tr><td>recorded_episode_index</td><td>76</td></tr><tr><td>recorded_episode_reward</td><td>-76</td></tr><tr><td>recorded_episode_steps</td><td>77</td></tr><tr><td>steps</td><td>75</td></tr><tr><td>worst_reward</td><td>-264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_gamma-0.99_1763008310</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qo1vrz4i' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/qo1vrz4i</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063150-qo1vrz4i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_lr-0.0003.pth ===\n",
      "W&B run: test_Acrobot_DQN_lr-0.0003_1763008315\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063155-m8etnqg8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m8etnqg8' target=\"_blank\">test_Acrobot_DQN_lr-0.0003_1763008315</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m8etnqg8' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m8etnqg8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-102.00 steps=103\n",
      "  Ep 2/100: reward=-82.00 steps=83\n",
      "  Ep 3/100: reward=-73.00 steps=74\n",
      "  Ep 4/100: reward=-305.00 steps=306\n",
      "  Ep 5/100: reward=-74.00 steps=75\n",
      "  Ep 6/100: reward=-93.00 steps=94\n",
      "  Ep 7/100: reward=-71.00 steps=72\n",
      "  Ep 8/100: reward=-81.00 steps=82\n",
      "  Ep 9/100: reward=-78.00 steps=79\n",
      "  Ep 10/100: reward=-87.00 steps=88\n",
      "  Ep 11/100: reward=-88.00 steps=89\n",
      "  Ep 12/100: reward=-79.00 steps=80\n",
      "  Ep 13/100: reward=-81.00 steps=82\n",
      "  Ep 14/100: reward=-70.00 steps=71\n",
      "  Ep 15/100: reward=-69.00 steps=70\n",
      "  Ep 16/100: reward=-77.00 steps=78\n",
      "  Ep 17/100: reward=-76.00 steps=77\n",
      "  Ep 18/100: reward=-71.00 steps=72\n",
      "  Ep 19/100: reward=-82.00 steps=83\n",
      "  Ep 20/100: reward=-82.00 steps=83\n",
      "  Ep 21/100: reward=-159.00 steps=160\n",
      "  Ep 22/100: reward=-80.00 steps=81\n",
      "  Ep 23/100: reward=-93.00 steps=94\n",
      "  Ep 24/100: reward=-180.00 steps=181\n",
      "  Ep 25/100: reward=-87.00 steps=88\n",
      "  Ep 26/100: reward=-77.00 steps=78\n",
      "  Ep 27/100: reward=-70.00 steps=71\n",
      "  Ep 28/100: reward=-87.00 steps=88\n",
      "  Ep 29/100: reward=-88.00 steps=89\n",
      "  Ep 30/100: reward=-97.00 steps=98\n",
      "  Ep 31/100: reward=-120.00 steps=121\n",
      "  Ep 32/100: reward=-71.00 steps=72\n",
      "  Ep 33/100: reward=-89.00 steps=90\n",
      "  Ep 34/100: reward=-80.00 steps=81\n",
      "  Ep 35/100: reward=-75.00 steps=76\n",
      "  Ep 36/100: reward=-70.00 steps=71\n",
      "  Ep 37/100: reward=-93.00 steps=94\n",
      "  Ep 38/100: reward=-87.00 steps=88\n",
      "  Ep 39/100: reward=-73.00 steps=74\n",
      "  Ep 40/100: reward=-101.00 steps=102\n",
      "  Ep 41/100: reward=-77.00 steps=78\n",
      "  Ep 42/100: reward=-73.00 steps=74\n",
      "  Ep 43/100: reward=-80.00 steps=81\n",
      "  Ep 44/100: reward=-74.00 steps=75\n",
      "  Ep 45/100: reward=-81.00 steps=82\n",
      "  Ep 46/100: reward=-81.00 steps=82\n",
      "  Ep 47/100: reward=-94.00 steps=95\n",
      "  Ep 48/100: reward=-75.00 steps=76\n",
      "  Ep 49/100: reward=-73.00 steps=74\n",
      "  Ep 50/100: reward=-77.00 steps=78\n",
      "  Ep 51/100: reward=-81.00 steps=82\n",
      "  Ep 52/100: reward=-70.00 steps=71\n",
      "  Ep 53/100: reward=-69.00 steps=70\n",
      "  Ep 54/100: reward=-91.00 steps=92\n",
      "  Ep 55/100: reward=-89.00 steps=90\n",
      "  Ep 56/100: reward=-81.00 steps=82\n",
      "  Ep 57/100: reward=-73.00 steps=74\n",
      "  Ep 58/100: reward=-74.00 steps=75\n",
      "  Ep 59/100: reward=-74.00 steps=75\n",
      "  Ep 60/100: reward=-83.00 steps=84\n",
      "  Ep 61/100: reward=-87.00 steps=88\n",
      "  Ep 62/100: reward=-117.00 steps=118\n",
      "  Ep 63/100: reward=-80.00 steps=81\n",
      "  Ep 64/100: reward=-108.00 steps=109\n",
      "  Ep 65/100: reward=-79.00 steps=80\n",
      "  Ep 66/100: reward=-89.00 steps=90\n",
      "  Ep 67/100: reward=-108.00 steps=109\n",
      "  Ep 68/100: reward=-75.00 steps=76\n",
      "  Ep 69/100: reward=-130.00 steps=131\n",
      "  Ep 70/100: reward=-86.00 steps=87\n",
      "  Ep 71/100: reward=-70.00 steps=71\n",
      "  Ep 72/100: reward=-82.00 steps=83\n",
      "  Ep 73/100: reward=-89.00 steps=90\n",
      "  Ep 74/100: reward=-115.00 steps=116\n",
      "  Ep 75/100: reward=-95.00 steps=96\n",
      "  Ep 76/100: reward=-95.00 steps=96\n",
      "  Ep 77/100: reward=-81.00 steps=82\n",
      "  Ep 78/100: reward=-81.00 steps=82\n",
      "  Ep 79/100: reward=-80.00 steps=81\n",
      "  Ep 80/100: reward=-75.00 steps=76\n",
      "  Ep 81/100: reward=-81.00 steps=82\n",
      "  Ep 82/100: reward=-73.00 steps=74\n",
      "  Ep 83/100: reward=-104.00 steps=105\n",
      "  Ep 84/100: reward=-70.00 steps=71\n",
      "  Ep 85/100: reward=-74.00 steps=75\n",
      "  Ep 86/100: reward=-146.00 steps=147\n",
      "  Ep 87/100: reward=-70.00 steps=71\n",
      "  Ep 88/100: reward=-81.00 steps=82\n",
      "  Ep 89/100: reward=-84.00 steps=85\n",
      "  Ep 90/100: reward=-95.00 steps=96\n",
      "  Ep 91/100: reward=-75.00 steps=76\n",
      "  Ep 92/100: reward=-77.00 steps=78\n",
      "  Ep 93/100: reward=-70.00 steps=71\n",
      "  Ep 94/100: reward=-77.00 steps=78\n",
      "  Ep 95/100: reward=-82.00 steps=83\n",
      "  Ep 96/100: reward=-85.00 steps=86\n",
      "  Ep 97/100: reward=-74.00 steps=75\n",
      "  Ep 98/100: reward=-73.00 steps=74\n",
      "  Ep 99/100: reward=-76.00 steps=77\n",
      "  Ep 100/100: reward=-87.00 steps=88\n",
      "Finished evaluations. avg=-87.44 best=-69.00 worst=-305.00\n",
      "Recording random episode index 46 for model Acrobot_DQN_lr-0.0003.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_lr-0.0003 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_lr-0.0003\\Acrobot_DQN_lr-0.0003-episode-0.mp4\n",
      "Recording complete. Total reward: -73.00, Steps:74, Duration: 0.44 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▅▇█▆▇▇▇█▁▇▇▄█▇█▇██▇██▆▇▇▄▇▆▆▆▆▅██▂▇▇█▇█▇</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>█▁▁▂▂▁▁▁▁▄▁▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▂▂▁▂▁▂▂▂▂▁▁▂▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-87.44</td></tr><tr><td>best_reward</td><td>-69</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-87</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.43732</td></tr><tr><td>recorded_episode_index</td><td>46</td></tr><tr><td>recorded_episode_reward</td><td>-73</td></tr><tr><td>recorded_episode_steps</td><td>74</td></tr><tr><td>steps</td><td>88</td></tr><tr><td>worst_reward</td><td>-305</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_lr-0.0003_1763008315</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m8etnqg8' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m8etnqg8</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063155-m8etnqg8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_lr-0.001.pth ===\n",
      "W&B run: test_Acrobot_DQN_lr-0.001_1763008319\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063159-wvs7ktin</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/wvs7ktin' target=\"_blank\">test_Acrobot_DQN_lr-0.001_1763008319</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/wvs7ktin' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/wvs7ktin</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-73.00 steps=74\n",
      "  Ep 2/100: reward=-75.00 steps=76\n",
      "  Ep 3/100: reward=-69.00 steps=70\n",
      "  Ep 4/100: reward=-78.00 steps=79\n",
      "  Ep 5/100: reward=-99.00 steps=100\n",
      "  Ep 6/100: reward=-69.00 steps=70\n",
      "  Ep 7/100: reward=-91.00 steps=92\n",
      "  Ep 8/100: reward=-68.00 steps=69\n",
      "  Ep 9/100: reward=-71.00 steps=72\n",
      "  Ep 10/100: reward=-80.00 steps=81\n",
      "  Ep 11/100: reward=-75.00 steps=76\n",
      "  Ep 12/100: reward=-77.00 steps=78\n",
      "  Ep 13/100: reward=-62.00 steps=63\n",
      "  Ep 14/100: reward=-81.00 steps=82\n",
      "  Ep 15/100: reward=-77.00 steps=78\n",
      "  Ep 16/100: reward=-76.00 steps=77\n",
      "  Ep 17/100: reward=-95.00 steps=96\n",
      "  Ep 18/100: reward=-81.00 steps=82\n",
      "  Ep 19/100: reward=-118.00 steps=119\n",
      "  Ep 20/100: reward=-69.00 steps=70\n",
      "  Ep 21/100: reward=-86.00 steps=87\n",
      "  Ep 22/100: reward=-72.00 steps=73\n",
      "  Ep 23/100: reward=-71.00 steps=72\n",
      "  Ep 24/100: reward=-96.00 steps=97\n",
      "  Ep 25/100: reward=-77.00 steps=78\n",
      "  Ep 26/100: reward=-83.00 steps=84\n",
      "  Ep 27/100: reward=-74.00 steps=75\n",
      "  Ep 28/100: reward=-82.00 steps=83\n",
      "  Ep 29/100: reward=-78.00 steps=79\n",
      "  Ep 30/100: reward=-70.00 steps=71\n",
      "  Ep 31/100: reward=-73.00 steps=74\n",
      "  Ep 32/100: reward=-92.00 steps=93\n",
      "  Ep 33/100: reward=-78.00 steps=79\n",
      "  Ep 34/100: reward=-84.00 steps=85\n",
      "  Ep 35/100: reward=-76.00 steps=77\n",
      "  Ep 36/100: reward=-69.00 steps=70\n",
      "  Ep 37/100: reward=-76.00 steps=77\n",
      "  Ep 38/100: reward=-76.00 steps=77\n",
      "  Ep 39/100: reward=-96.00 steps=97\n",
      "  Ep 40/100: reward=-85.00 steps=86\n",
      "  Ep 41/100: reward=-98.00 steps=99\n",
      "  Ep 42/100: reward=-84.00 steps=85\n",
      "  Ep 43/100: reward=-94.00 steps=95\n",
      "  Ep 44/100: reward=-71.00 steps=72\n",
      "  Ep 45/100: reward=-86.00 steps=87\n",
      "  Ep 46/100: reward=-87.00 steps=88\n",
      "  Ep 47/100: reward=-71.00 steps=72\n",
      "  Ep 48/100: reward=-70.00 steps=71\n",
      "  Ep 49/100: reward=-78.00 steps=79\n",
      "  Ep 50/100: reward=-92.00 steps=93\n",
      "  Ep 51/100: reward=-71.00 steps=72\n",
      "  Ep 52/100: reward=-158.00 steps=159\n",
      "  Ep 53/100: reward=-118.00 steps=119\n",
      "  Ep 54/100: reward=-62.00 steps=63\n",
      "  Ep 55/100: reward=-101.00 steps=102\n",
      "  Ep 56/100: reward=-77.00 steps=78\n",
      "  Ep 57/100: reward=-70.00 steps=71\n",
      "  Ep 58/100: reward=-119.00 steps=120\n",
      "  Ep 59/100: reward=-85.00 steps=86\n",
      "  Ep 60/100: reward=-74.00 steps=75\n",
      "  Ep 61/100: reward=-62.00 steps=63\n",
      "  Ep 62/100: reward=-71.00 steps=72\n",
      "  Ep 63/100: reward=-89.00 steps=90\n",
      "  Ep 64/100: reward=-94.00 steps=95\n",
      "  Ep 65/100: reward=-78.00 steps=79\n",
      "  Ep 66/100: reward=-62.00 steps=63\n",
      "  Ep 67/100: reward=-92.00 steps=93\n",
      "  Ep 68/100: reward=-70.00 steps=71\n",
      "  Ep 69/100: reward=-83.00 steps=84\n",
      "  Ep 70/100: reward=-98.00 steps=99\n",
      "  Ep 71/100: reward=-103.00 steps=104\n",
      "  Ep 72/100: reward=-108.00 steps=109\n",
      "  Ep 73/100: reward=-71.00 steps=72\n",
      "  Ep 74/100: reward=-78.00 steps=79\n",
      "  Ep 75/100: reward=-74.00 steps=75\n",
      "  Ep 76/100: reward=-61.00 steps=62\n",
      "  Ep 77/100: reward=-105.00 steps=106\n",
      "  Ep 78/100: reward=-75.00 steps=76\n",
      "  Ep 79/100: reward=-86.00 steps=87\n",
      "  Ep 80/100: reward=-75.00 steps=76\n",
      "  Ep 81/100: reward=-87.00 steps=88\n",
      "  Ep 82/100: reward=-63.00 steps=64\n",
      "  Ep 83/100: reward=-74.00 steps=75\n",
      "  Ep 84/100: reward=-81.00 steps=82\n",
      "  Ep 85/100: reward=-87.00 steps=88\n",
      "  Ep 86/100: reward=-62.00 steps=63\n",
      "  Ep 87/100: reward=-69.00 steps=70\n",
      "  Ep 88/100: reward=-81.00 steps=82\n",
      "  Ep 89/100: reward=-83.00 steps=84\n",
      "  Ep 90/100: reward=-72.00 steps=73\n",
      "  Ep 91/100: reward=-77.00 steps=78\n",
      "  Ep 92/100: reward=-80.00 steps=81\n",
      "  Ep 93/100: reward=-88.00 steps=89\n",
      "  Ep 94/100: reward=-84.00 steps=85\n",
      "  Ep 95/100: reward=-101.00 steps=102\n",
      "  Ep 96/100: reward=-74.00 steps=75\n",
      "  Ep 97/100: reward=-71.00 steps=72\n",
      "  Ep 98/100: reward=-102.00 steps=103\n",
      "  Ep 99/100: reward=-74.00 steps=75\n",
      "  Ep 100/100: reward=-107.00 steps=108\n",
      "Finished evaluations. avg=-81.96 best=-61.00 worst=-158.00\n",
      "Recording random episode index 35 for model Acrobot_DQN_lr-0.001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_lr-0.001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_lr-0.001\\Acrobot_DQN_lr-0.001-episode-0.mp4\n",
      "Recording complete. Total reward: -74.00, Steps:75, Duration: 0.45 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇████</td></tr><tr><td>episode_reward</td><td>▇▇▇▇▇▆▆▇▆▇▇▇▇▇▆▆▆▇▁▄▄▆█▇▆█▅▅▇▆▆██▇▇▇▅▇▇▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▂▄▂▂▂▁▂▂▂▃▂▂▂▃▂▃▄▃▂▂▂▂▃█▄▅▁▁▃▄▂▁▄▂▂▂▂▂▄</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-81.96</td></tr><tr><td>best_reward</td><td>-61</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-107</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.44537</td></tr><tr><td>recorded_episode_index</td><td>35</td></tr><tr><td>recorded_episode_reward</td><td>-74</td></tr><tr><td>recorded_episode_steps</td><td>75</td></tr><tr><td>steps</td><td>108</td></tr><tr><td>worst_reward</td><td>-158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_lr-0.001_1763008319</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/wvs7ktin' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/wvs7ktin</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063159-wvs7ktin\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_lr-0.01.pth ===\n",
      "W&B run: test_Acrobot_DQN_lr-0.01_1763008324\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063204-hmjvup80</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hmjvup80' target=\"_blank\">test_Acrobot_DQN_lr-0.01_1763008324</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hmjvup80' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hmjvup80</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-71.00 steps=72\n",
      "  Ep 2/100: reward=-77.00 steps=78\n",
      "  Ep 3/100: reward=-63.00 steps=64\n",
      "  Ep 4/100: reward=-73.00 steps=74\n",
      "  Ep 5/100: reward=-70.00 steps=71\n",
      "  Ep 6/100: reward=-69.00 steps=70\n",
      "  Ep 7/100: reward=-72.00 steps=73\n",
      "  Ep 8/100: reward=-70.00 steps=71\n",
      "  Ep 9/100: reward=-90.00 steps=91\n",
      "  Ep 10/100: reward=-62.00 steps=63\n",
      "  Ep 11/100: reward=-88.00 steps=89\n",
      "  Ep 12/100: reward=-90.00 steps=91\n",
      "  Ep 13/100: reward=-69.00 steps=70\n",
      "  Ep 14/100: reward=-92.00 steps=93\n",
      "  Ep 15/100: reward=-62.00 steps=63\n",
      "  Ep 16/100: reward=-78.00 steps=79\n",
      "  Ep 17/100: reward=-83.00 steps=84\n",
      "  Ep 18/100: reward=-96.00 steps=97\n",
      "  Ep 19/100: reward=-87.00 steps=88\n",
      "  Ep 20/100: reward=-78.00 steps=79\n",
      "  Ep 21/100: reward=-90.00 steps=91\n",
      "  Ep 22/100: reward=-77.00 steps=78\n",
      "  Ep 23/100: reward=-69.00 steps=70\n",
      "  Ep 24/100: reward=-77.00 steps=78\n",
      "  Ep 25/100: reward=-85.00 steps=86\n",
      "  Ep 26/100: reward=-78.00 steps=79\n",
      "  Ep 27/100: reward=-83.00 steps=84\n",
      "  Ep 28/100: reward=-88.00 steps=89\n",
      "  Ep 29/100: reward=-77.00 steps=78\n",
      "  Ep 30/100: reward=-85.00 steps=86\n",
      "  Ep 31/100: reward=-84.00 steps=85\n",
      "  Ep 32/100: reward=-84.00 steps=85\n",
      "  Ep 33/100: reward=-88.00 steps=89\n",
      "  Ep 34/100: reward=-80.00 steps=81\n",
      "  Ep 35/100: reward=-94.00 steps=95\n",
      "  Ep 36/100: reward=-76.00 steps=77\n",
      "  Ep 37/100: reward=-96.00 steps=97\n",
      "  Ep 38/100: reward=-81.00 steps=82\n",
      "  Ep 39/100: reward=-86.00 steps=87\n",
      "  Ep 40/100: reward=-78.00 steps=79\n",
      "  Ep 41/100: reward=-79.00 steps=80\n",
      "  Ep 42/100: reward=-90.00 steps=91\n",
      "  Ep 43/100: reward=-92.00 steps=93\n",
      "  Ep 44/100: reward=-86.00 steps=87\n",
      "  Ep 45/100: reward=-74.00 steps=75\n",
      "  Ep 46/100: reward=-70.00 steps=71\n",
      "  Ep 47/100: reward=-82.00 steps=83\n",
      "  Ep 48/100: reward=-77.00 steps=78\n",
      "  Ep 49/100: reward=-72.00 steps=73\n",
      "  Ep 50/100: reward=-77.00 steps=78\n",
      "  Ep 51/100: reward=-70.00 steps=71\n",
      "  Ep 52/100: reward=-86.00 steps=87\n",
      "  Ep 53/100: reward=-77.00 steps=78\n",
      "  Ep 54/100: reward=-71.00 steps=72\n",
      "  Ep 55/100: reward=-72.00 steps=73\n",
      "  Ep 56/100: reward=-70.00 steps=71\n",
      "  Ep 57/100: reward=-69.00 steps=70\n",
      "  Ep 58/100: reward=-80.00 steps=81\n",
      "  Ep 59/100: reward=-87.00 steps=88\n",
      "  Ep 60/100: reward=-121.00 steps=122\n",
      "  Ep 61/100: reward=-80.00 steps=81\n",
      "  Ep 62/100: reward=-69.00 steps=70\n",
      "  Ep 63/100: reward=-62.00 steps=63\n",
      "  Ep 64/100: reward=-81.00 steps=82\n",
      "  Ep 65/100: reward=-74.00 steps=75\n",
      "  Ep 66/100: reward=-78.00 steps=79\n",
      "  Ep 67/100: reward=-87.00 steps=88\n",
      "  Ep 68/100: reward=-71.00 steps=72\n",
      "  Ep 69/100: reward=-78.00 steps=79\n",
      "  Ep 70/100: reward=-78.00 steps=79\n",
      "  Ep 71/100: reward=-70.00 steps=71\n",
      "  Ep 72/100: reward=-69.00 steps=70\n",
      "  Ep 73/100: reward=-62.00 steps=63\n",
      "  Ep 74/100: reward=-88.00 steps=89\n",
      "  Ep 75/100: reward=-86.00 steps=87\n",
      "  Ep 76/100: reward=-70.00 steps=71\n",
      "  Ep 77/100: reward=-81.00 steps=82\n",
      "  Ep 78/100: reward=-96.00 steps=97\n",
      "  Ep 79/100: reward=-87.00 steps=88\n",
      "  Ep 80/100: reward=-73.00 steps=74\n",
      "  Ep 81/100: reward=-70.00 steps=71\n",
      "  Ep 82/100: reward=-79.00 steps=80\n",
      "  Ep 83/100: reward=-69.00 steps=70\n",
      "  Ep 84/100: reward=-73.00 steps=74\n",
      "  Ep 85/100: reward=-69.00 steps=70\n",
      "  Ep 86/100: reward=-86.00 steps=87\n",
      "  Ep 87/100: reward=-84.00 steps=85\n",
      "  Ep 88/100: reward=-69.00 steps=70\n",
      "  Ep 89/100: reward=-84.00 steps=85\n",
      "  Ep 90/100: reward=-63.00 steps=64\n",
      "  Ep 91/100: reward=-83.00 steps=84\n",
      "  Ep 92/100: reward=-74.00 steps=75\n",
      "  Ep 93/100: reward=-79.00 steps=80\n",
      "  Ep 94/100: reward=-86.00 steps=87\n",
      "  Ep 95/100: reward=-69.00 steps=70\n",
      "  Ep 96/100: reward=-101.00 steps=102\n",
      "  Ep 97/100: reward=-69.00 steps=70\n",
      "  Ep 98/100: reward=-69.00 steps=70\n",
      "  Ep 99/100: reward=-69.00 steps=70\n",
      "  Ep 100/100: reward=-95.00 steps=96\n",
      "Finished evaluations. avg=-78.78 best=-62.00 worst=-121.00\n",
      "Recording random episode index 20 for model Acrobot_DQN_lr-0.01.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_lr-0.01 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_lr-0.01\\Acrobot_DQN_lr-0.01-episode-0.mp4\n",
      "Recording complete. Total reward: -74.00, Steps:75, Duration: 0.47 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▆▅█▆▆▃█▄▅▃▅▅▃▇▄▆▅▄▆▇▅▇█▆▄▅▇█▄▇▇▄▄█▆▄▇▁▇▂</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▃▁▂▂▄▄▁▃▅▄▃▃▅▃▃▃▃▃▂▂▂▂▄█▂▄▂▂▁▄▃▂▂▂▄▁▂▃▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-78.78</td></tr><tr><td>best_reward</td><td>-62</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-95</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.46611</td></tr><tr><td>recorded_episode_index</td><td>20</td></tr><tr><td>recorded_episode_reward</td><td>-74</td></tr><tr><td>recorded_episode_steps</td><td>75</td></tr><tr><td>steps</td><td>96</td></tr><tr><td>worst_reward</td><td>-121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_lr-0.01_1763008324</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hmjvup80' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/hmjvup80</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063204-hmjvup80\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_lr-1e-05.pth ===\n",
      "W&B run: test_Acrobot_DQN_lr-1e-05_1763008329\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063209-pqvugwch</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/pqvugwch' target=\"_blank\">test_Acrobot_DQN_lr-1e-05_1763008329</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/pqvugwch' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/pqvugwch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-500.00 steps=500\n",
      "  Ep 2/100: reward=-500.00 steps=500\n",
      "  Ep 3/100: reward=-500.00 steps=500\n",
      "  Ep 4/100: reward=-500.00 steps=500\n",
      "  Ep 5/100: reward=-500.00 steps=500\n",
      "  Ep 6/100: reward=-500.00 steps=500\n",
      "  Ep 7/100: reward=-500.00 steps=500\n",
      "  Ep 8/100: reward=-500.00 steps=500\n",
      "  Ep 9/100: reward=-500.00 steps=500\n",
      "  Ep 10/100: reward=-500.00 steps=500\n",
      "  Ep 11/100: reward=-500.00 steps=500\n",
      "  Ep 12/100: reward=-500.00 steps=500\n",
      "  Ep 13/100: reward=-500.00 steps=500\n",
      "  Ep 14/100: reward=-500.00 steps=500\n",
      "  Ep 15/100: reward=-500.00 steps=500\n",
      "  Ep 16/100: reward=-500.00 steps=500\n",
      "  Ep 17/100: reward=-500.00 steps=500\n",
      "  Ep 18/100: reward=-500.00 steps=500\n",
      "  Ep 19/100: reward=-500.00 steps=500\n",
      "  Ep 20/100: reward=-500.00 steps=500\n",
      "  Ep 21/100: reward=-500.00 steps=500\n",
      "  Ep 22/100: reward=-500.00 steps=500\n",
      "  Ep 23/100: reward=-500.00 steps=500\n",
      "  Ep 24/100: reward=-500.00 steps=500\n",
      "  Ep 25/100: reward=-500.00 steps=500\n",
      "  Ep 26/100: reward=-500.00 steps=500\n",
      "  Ep 27/100: reward=-500.00 steps=500\n",
      "  Ep 28/100: reward=-500.00 steps=500\n",
      "  Ep 29/100: reward=-500.00 steps=500\n",
      "  Ep 30/100: reward=-500.00 steps=500\n",
      "  Ep 31/100: reward=-500.00 steps=500\n",
      "  Ep 32/100: reward=-500.00 steps=500\n",
      "  Ep 33/100: reward=-500.00 steps=500\n",
      "  Ep 34/100: reward=-500.00 steps=500\n",
      "  Ep 35/100: reward=-500.00 steps=500\n",
      "  Ep 36/100: reward=-500.00 steps=500\n",
      "  Ep 37/100: reward=-500.00 steps=500\n",
      "  Ep 38/100: reward=-500.00 steps=500\n",
      "  Ep 39/100: reward=-500.00 steps=500\n",
      "  Ep 40/100: reward=-500.00 steps=500\n",
      "  Ep 41/100: reward=-500.00 steps=500\n",
      "  Ep 42/100: reward=-500.00 steps=500\n",
      "  Ep 43/100: reward=-500.00 steps=500\n",
      "  Ep 44/100: reward=-500.00 steps=500\n",
      "  Ep 45/100: reward=-500.00 steps=500\n",
      "  Ep 46/100: reward=-500.00 steps=500\n",
      "  Ep 47/100: reward=-500.00 steps=500\n",
      "  Ep 48/100: reward=-500.00 steps=500\n",
      "  Ep 49/100: reward=-500.00 steps=500\n",
      "  Ep 50/100: reward=-500.00 steps=500\n",
      "  Ep 51/100: reward=-500.00 steps=500\n",
      "  Ep 52/100: reward=-500.00 steps=500\n",
      "  Ep 53/100: reward=-500.00 steps=500\n",
      "  Ep 54/100: reward=-500.00 steps=500\n",
      "  Ep 55/100: reward=-500.00 steps=500\n",
      "  Ep 56/100: reward=-500.00 steps=500\n",
      "  Ep 57/100: reward=-500.00 steps=500\n",
      "  Ep 58/100: reward=-500.00 steps=500\n",
      "  Ep 59/100: reward=-500.00 steps=500\n",
      "  Ep 60/100: reward=-500.00 steps=500\n",
      "  Ep 61/100: reward=-500.00 steps=500\n",
      "  Ep 62/100: reward=-500.00 steps=500\n",
      "  Ep 63/100: reward=-500.00 steps=500\n",
      "  Ep 64/100: reward=-500.00 steps=500\n",
      "  Ep 65/100: reward=-500.00 steps=500\n",
      "  Ep 66/100: reward=-500.00 steps=500\n",
      "  Ep 67/100: reward=-500.00 steps=500\n",
      "  Ep 68/100: reward=-500.00 steps=500\n",
      "  Ep 69/100: reward=-500.00 steps=500\n",
      "  Ep 70/100: reward=-500.00 steps=500\n",
      "  Ep 71/100: reward=-500.00 steps=500\n",
      "  Ep 72/100: reward=-500.00 steps=500\n",
      "  Ep 73/100: reward=-500.00 steps=500\n",
      "  Ep 74/100: reward=-500.00 steps=500\n",
      "  Ep 75/100: reward=-500.00 steps=500\n",
      "  Ep 76/100: reward=-500.00 steps=500\n",
      "  Ep 77/100: reward=-500.00 steps=500\n",
      "  Ep 78/100: reward=-500.00 steps=500\n",
      "  Ep 79/100: reward=-500.00 steps=500\n",
      "  Ep 80/100: reward=-500.00 steps=500\n",
      "  Ep 81/100: reward=-500.00 steps=500\n",
      "  Ep 82/100: reward=-500.00 steps=500\n",
      "  Ep 83/100: reward=-500.00 steps=500\n",
      "  Ep 84/100: reward=-500.00 steps=500\n",
      "  Ep 85/100: reward=-500.00 steps=500\n",
      "  Ep 86/100: reward=-500.00 steps=500\n",
      "  Ep 87/100: reward=-500.00 steps=500\n",
      "  Ep 88/100: reward=-500.00 steps=500\n",
      "  Ep 89/100: reward=-500.00 steps=500\n",
      "  Ep 90/100: reward=-500.00 steps=500\n",
      "  Ep 91/100: reward=-500.00 steps=500\n",
      "  Ep 92/100: reward=-500.00 steps=500\n",
      "  Ep 93/100: reward=-500.00 steps=500\n",
      "  Ep 94/100: reward=-500.00 steps=500\n",
      "  Ep 95/100: reward=-500.00 steps=500\n",
      "  Ep 96/100: reward=-500.00 steps=500\n",
      "  Ep 97/100: reward=-500.00 steps=500\n",
      "  Ep 98/100: reward=-500.00 steps=500\n",
      "  Ep 99/100: reward=-500.00 steps=500\n",
      "  Ep 100/100: reward=-500.00 steps=500\n",
      "Finished evaluations. avg=-500.00 best=-500.00 worst=-500.00\n",
      "Recording random episode index 24 for model Acrobot_DQN_lr-1e-05.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_lr-1e-05 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_lr-1e-05\\Acrobot_DQN_lr-1e-05-episode-0.mp4\n",
      "Recording complete. Total reward: -500.00, Steps:500, Duration: 1.60 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-500</td></tr><tr><td>best_reward</td><td>-500</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-500</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.59776</td></tr><tr><td>recorded_episode_index</td><td>24</td></tr><tr><td>recorded_episode_reward</td><td>-500</td></tr><tr><td>recorded_episode_steps</td><td>500</td></tr><tr><td>steps</td><td>500</td></tr><tr><td>worst_reward</td><td>-500</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_lr-1e-05_1763008329</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/pqvugwch' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/pqvugwch</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063209-pqvugwch\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_mem-10000.pth ===\n",
      "W&B run: test_Acrobot_DQN_mem-10000_1763008342\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063222-jlvny5ms</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/jlvny5ms' target=\"_blank\">test_Acrobot_DQN_mem-10000_1763008342</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/jlvny5ms' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/jlvny5ms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-74.00 steps=75\n",
      "  Ep 2/100: reward=-74.00 steps=75\n",
      "  Ep 3/100: reward=-82.00 steps=83\n",
      "  Ep 4/100: reward=-73.00 steps=74\n",
      "  Ep 5/100: reward=-70.00 steps=71\n",
      "  Ep 6/100: reward=-75.00 steps=76\n",
      "  Ep 7/100: reward=-79.00 steps=80\n",
      "  Ep 8/100: reward=-94.00 steps=95\n",
      "  Ep 9/100: reward=-74.00 steps=75\n",
      "  Ep 10/100: reward=-89.00 steps=90\n",
      "  Ep 11/100: reward=-80.00 steps=81\n",
      "  Ep 12/100: reward=-76.00 steps=77\n",
      "  Ep 13/100: reward=-95.00 steps=96\n",
      "  Ep 14/100: reward=-62.00 steps=63\n",
      "  Ep 15/100: reward=-70.00 steps=71\n",
      "  Ep 16/100: reward=-73.00 steps=74\n",
      "  Ep 17/100: reward=-95.00 steps=96\n",
      "  Ep 18/100: reward=-78.00 steps=79\n",
      "  Ep 19/100: reward=-70.00 steps=71\n",
      "  Ep 20/100: reward=-70.00 steps=71\n",
      "  Ep 21/100: reward=-86.00 steps=87\n",
      "  Ep 22/100: reward=-71.00 steps=72\n",
      "  Ep 23/100: reward=-70.00 steps=71\n",
      "  Ep 24/100: reward=-75.00 steps=76\n",
      "  Ep 25/100: reward=-99.00 steps=100\n",
      "  Ep 26/100: reward=-136.00 steps=137\n",
      "  Ep 27/100: reward=-70.00 steps=71\n",
      "  Ep 28/100: reward=-78.00 steps=79\n",
      "  Ep 29/100: reward=-114.00 steps=115\n",
      "  Ep 30/100: reward=-70.00 steps=71\n",
      "  Ep 31/100: reward=-77.00 steps=78\n",
      "  Ep 32/100: reward=-62.00 steps=63\n",
      "  Ep 33/100: reward=-77.00 steps=78\n",
      "  Ep 34/100: reward=-78.00 steps=79\n",
      "  Ep 35/100: reward=-70.00 steps=71\n",
      "  Ep 36/100: reward=-74.00 steps=75\n",
      "  Ep 37/100: reward=-73.00 steps=74\n",
      "  Ep 38/100: reward=-74.00 steps=75\n",
      "  Ep 39/100: reward=-73.00 steps=74\n",
      "  Ep 40/100: reward=-71.00 steps=72\n",
      "  Ep 41/100: reward=-110.00 steps=111\n",
      "  Ep 42/100: reward=-72.00 steps=73\n",
      "  Ep 43/100: reward=-71.00 steps=72\n",
      "  Ep 44/100: reward=-94.00 steps=95\n",
      "  Ep 45/100: reward=-72.00 steps=73\n",
      "  Ep 46/100: reward=-69.00 steps=70\n",
      "  Ep 47/100: reward=-84.00 steps=85\n",
      "  Ep 48/100: reward=-74.00 steps=75\n",
      "  Ep 49/100: reward=-86.00 steps=87\n",
      "  Ep 50/100: reward=-74.00 steps=75\n",
      "  Ep 51/100: reward=-92.00 steps=93\n",
      "  Ep 52/100: reward=-98.00 steps=99\n",
      "  Ep 53/100: reward=-76.00 steps=77\n",
      "  Ep 54/100: reward=-82.00 steps=83\n",
      "  Ep 55/100: reward=-70.00 steps=71\n",
      "  Ep 56/100: reward=-99.00 steps=100\n",
      "  Ep 57/100: reward=-79.00 steps=80\n",
      "  Ep 58/100: reward=-74.00 steps=75\n",
      "  Ep 59/100: reward=-74.00 steps=75\n",
      "  Ep 60/100: reward=-80.00 steps=81\n",
      "  Ep 61/100: reward=-96.00 steps=97\n",
      "  Ep 62/100: reward=-76.00 steps=77\n",
      "  Ep 63/100: reward=-70.00 steps=71\n",
      "  Ep 64/100: reward=-115.00 steps=116\n",
      "  Ep 65/100: reward=-79.00 steps=80\n",
      "  Ep 66/100: reward=-78.00 steps=79\n",
      "  Ep 67/100: reward=-96.00 steps=97\n",
      "  Ep 68/100: reward=-74.00 steps=75\n",
      "  Ep 69/100: reward=-91.00 steps=92\n",
      "  Ep 70/100: reward=-69.00 steps=70\n",
      "  Ep 71/100: reward=-93.00 steps=94\n",
      "  Ep 72/100: reward=-73.00 steps=74\n",
      "  Ep 73/100: reward=-79.00 steps=80\n",
      "  Ep 74/100: reward=-74.00 steps=75\n",
      "  Ep 75/100: reward=-74.00 steps=75\n",
      "  Ep 76/100: reward=-72.00 steps=73\n",
      "  Ep 77/100: reward=-74.00 steps=75\n",
      "  Ep 78/100: reward=-78.00 steps=79\n",
      "  Ep 79/100: reward=-93.00 steps=94\n",
      "  Ep 80/100: reward=-78.00 steps=79\n",
      "  Ep 81/100: reward=-74.00 steps=75\n",
      "  Ep 82/100: reward=-69.00 steps=70\n",
      "  Ep 83/100: reward=-109.00 steps=110\n",
      "  Ep 84/100: reward=-89.00 steps=90\n",
      "  Ep 85/100: reward=-71.00 steps=72\n",
      "  Ep 86/100: reward=-68.00 steps=69\n",
      "  Ep 87/100: reward=-94.00 steps=95\n",
      "  Ep 88/100: reward=-74.00 steps=75\n",
      "  Ep 89/100: reward=-69.00 steps=70\n",
      "  Ep 90/100: reward=-70.00 steps=71\n",
      "  Ep 91/100: reward=-96.00 steps=97\n",
      "  Ep 92/100: reward=-77.00 steps=78\n",
      "  Ep 93/100: reward=-75.00 steps=76\n",
      "  Ep 94/100: reward=-73.00 steps=74\n",
      "  Ep 95/100: reward=-75.00 steps=76\n",
      "  Ep 96/100: reward=-99.00 steps=100\n",
      "  Ep 97/100: reward=-75.00 steps=76\n",
      "  Ep 98/100: reward=-71.00 steps=72\n",
      "  Ep 99/100: reward=-69.00 steps=70\n",
      "  Ep 100/100: reward=-74.00 steps=75\n",
      "Finished evaluations. avg=-79.84 best=-62.00 worst=-136.00\n",
      "Recording random episode index 51 for model Acrobot_DQN_mem-10000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_mem-10000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_mem-10000\\Acrobot_DQN_mem-10000-episode-0.mp4\n",
      "Recording complete. Total reward: -79.00, Steps:80, Duration: 0.45 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▇▇▇▇▆██▇▅▁▃▇▇▇▇▅█▇▇▇█▇▇▇█▇▆▇▇▇▄▆█▅▇▇▇▅▇▇</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▄▂▃▃▃▂▂▄▂▆▂█▂▃▁▃▃▃▂▇▂▃▆▄▂▂█▂▅▂▃▂▃▃▂▂▃▆▃▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-79.84</td></tr><tr><td>best_reward</td><td>-62</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-74</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.45154</td></tr><tr><td>recorded_episode_index</td><td>51</td></tr><tr><td>recorded_episode_reward</td><td>-79</td></tr><tr><td>recorded_episode_steps</td><td>80</td></tr><tr><td>steps</td><td>75</td></tr><tr><td>worst_reward</td><td>-136</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_mem-10000_1763008342</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/jlvny5ms' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/jlvny5ms</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063222-jlvny5ms\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_mem-500.pth ===\n",
      "W&B run: test_Acrobot_DQN_mem-500_1763008347\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063227-biccrd6e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/biccrd6e' target=\"_blank\">test_Acrobot_DQN_mem-500_1763008347</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/biccrd6e' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/biccrd6e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-74.00 steps=75\n",
      "  Ep 2/100: reward=-131.00 steps=132\n",
      "  Ep 3/100: reward=-70.00 steps=71\n",
      "  Ep 4/100: reward=-74.00 steps=75\n",
      "  Ep 5/100: reward=-89.00 steps=90\n",
      "  Ep 6/100: reward=-259.00 steps=260\n",
      "  Ep 7/100: reward=-63.00 steps=64\n",
      "  Ep 8/100: reward=-84.00 steps=85\n",
      "  Ep 9/100: reward=-62.00 steps=63\n",
      "  Ep 10/100: reward=-62.00 steps=63\n",
      "  Ep 11/100: reward=-72.00 steps=73\n",
      "  Ep 12/100: reward=-93.00 steps=94\n",
      "  Ep 13/100: reward=-69.00 steps=70\n",
      "  Ep 14/100: reward=-98.00 steps=99\n",
      "  Ep 15/100: reward=-61.00 steps=62\n",
      "  Ep 16/100: reward=-69.00 steps=70\n",
      "  Ep 17/100: reward=-76.00 steps=77\n",
      "  Ep 18/100: reward=-80.00 steps=81\n",
      "  Ep 19/100: reward=-76.00 steps=77\n",
      "  Ep 20/100: reward=-70.00 steps=71\n",
      "  Ep 21/100: reward=-68.00 steps=69\n",
      "  Ep 22/100: reward=-70.00 steps=71\n",
      "  Ep 23/100: reward=-82.00 steps=83\n",
      "  Ep 24/100: reward=-74.00 steps=75\n",
      "  Ep 25/100: reward=-61.00 steps=62\n",
      "  Ep 26/100: reward=-77.00 steps=78\n",
      "  Ep 27/100: reward=-393.00 steps=394\n",
      "  Ep 28/100: reward=-71.00 steps=72\n",
      "  Ep 29/100: reward=-69.00 steps=70\n",
      "  Ep 30/100: reward=-75.00 steps=76\n",
      "  Ep 31/100: reward=-68.00 steps=69\n",
      "  Ep 32/100: reward=-68.00 steps=69\n",
      "  Ep 33/100: reward=-77.00 steps=78\n",
      "  Ep 34/100: reward=-90.00 steps=91\n",
      "  Ep 35/100: reward=-69.00 steps=70\n",
      "  Ep 36/100: reward=-82.00 steps=83\n",
      "  Ep 37/100: reward=-91.00 steps=92\n",
      "  Ep 38/100: reward=-69.00 steps=70\n",
      "  Ep 39/100: reward=-80.00 steps=81\n",
      "  Ep 40/100: reward=-77.00 steps=78\n",
      "  Ep 41/100: reward=-83.00 steps=84\n",
      "  Ep 42/100: reward=-70.00 steps=71\n",
      "  Ep 43/100: reward=-90.00 steps=91\n",
      "  Ep 44/100: reward=-69.00 steps=70\n",
      "  Ep 45/100: reward=-78.00 steps=79\n",
      "  Ep 46/100: reward=-69.00 steps=70\n",
      "  Ep 47/100: reward=-69.00 steps=70\n",
      "  Ep 48/100: reward=-60.00 steps=61\n",
      "  Ep 49/100: reward=-129.00 steps=130\n",
      "  Ep 50/100: reward=-85.00 steps=86\n",
      "  Ep 51/100: reward=-80.00 steps=81\n",
      "  Ep 52/100: reward=-70.00 steps=71\n",
      "  Ep 53/100: reward=-78.00 steps=79\n",
      "  Ep 54/100: reward=-62.00 steps=63\n",
      "  Ep 55/100: reward=-96.00 steps=97\n",
      "  Ep 56/100: reward=-294.00 steps=295\n",
      "  Ep 57/100: reward=-61.00 steps=62\n",
      "  Ep 58/100: reward=-76.00 steps=77\n",
      "  Ep 59/100: reward=-83.00 steps=84\n",
      "  Ep 60/100: reward=-82.00 steps=83\n",
      "  Ep 61/100: reward=-98.00 steps=99\n",
      "  Ep 62/100: reward=-75.00 steps=76\n",
      "  Ep 63/100: reward=-70.00 steps=71\n",
      "  Ep 64/100: reward=-81.00 steps=82\n",
      "  Ep 65/100: reward=-77.00 steps=78\n",
      "  Ep 66/100: reward=-75.00 steps=76\n",
      "  Ep 67/100: reward=-84.00 steps=85\n",
      "  Ep 68/100: reward=-95.00 steps=96\n",
      "  Ep 69/100: reward=-74.00 steps=75\n",
      "  Ep 70/100: reward=-81.00 steps=82\n",
      "  Ep 71/100: reward=-74.00 steps=75\n",
      "  Ep 72/100: reward=-88.00 steps=89\n",
      "  Ep 73/100: reward=-76.00 steps=77\n",
      "  Ep 74/100: reward=-82.00 steps=83\n",
      "  Ep 75/100: reward=-63.00 steps=64\n",
      "  Ep 76/100: reward=-91.00 steps=92\n",
      "  Ep 77/100: reward=-86.00 steps=87\n",
      "  Ep 78/100: reward=-69.00 steps=70\n",
      "  Ep 79/100: reward=-93.00 steps=94\n",
      "  Ep 80/100: reward=-82.00 steps=83\n",
      "  Ep 81/100: reward=-62.00 steps=63\n",
      "  Ep 82/100: reward=-70.00 steps=71\n",
      "  Ep 83/100: reward=-132.00 steps=133\n",
      "  Ep 84/100: reward=-70.00 steps=71\n",
      "  Ep 85/100: reward=-92.00 steps=93\n",
      "  Ep 86/100: reward=-75.00 steps=76\n",
      "  Ep 87/100: reward=-85.00 steps=86\n",
      "  Ep 88/100: reward=-110.00 steps=111\n",
      "  Ep 89/100: reward=-69.00 steps=70\n",
      "  Ep 90/100: reward=-77.00 steps=78\n",
      "  Ep 91/100: reward=-76.00 steps=77\n",
      "  Ep 92/100: reward=-92.00 steps=93\n",
      "  Ep 93/100: reward=-69.00 steps=70\n",
      "  Ep 94/100: reward=-87.00 steps=88\n",
      "  Ep 95/100: reward=-82.00 steps=83\n",
      "  Ep 96/100: reward=-77.00 steps=78\n",
      "  Ep 97/100: reward=-68.00 steps=69\n",
      "  Ep 98/100: reward=-68.00 steps=69\n",
      "  Ep 99/100: reward=-69.00 steps=70\n",
      "  Ep 100/100: reward=-69.00 steps=70\n",
      "Finished evaluations. avg=-85.40 best=-60.00 worst=-393.00\n",
      "Recording random episode index 32 for model Acrobot_DQN_mem-500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_mem-500 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_mem-500\\Acrobot_DQN_mem-500-episode-0.mp4\n",
      "Recording complete. Total reward: -69.00, Steps:70, Duration: 0.44 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>▇▅▆▇▆█▇▇▅▇▇▅▇▆▇▁▆▆▆██▆▆▄▇▆▅▇▇▅▆█▇▇▇▆▃▇▅▇</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▇▁▂▂▁▁▁▁▁▁▁▂▂▁▂▁▂▁▂█▁▂▁▂▂▁▂▁▂▁▃▁▁▂▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-85.4</td></tr><tr><td>best_reward</td><td>-60</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-69</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.4352</td></tr><tr><td>recorded_episode_index</td><td>32</td></tr><tr><td>recorded_episode_reward</td><td>-69</td></tr><tr><td>recorded_episode_steps</td><td>70</td></tr><tr><td>steps</td><td>70</td></tr><tr><td>worst_reward</td><td>-393</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_mem-500_1763008347</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/biccrd6e' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/biccrd6e</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063227-biccrd6e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: Acrobot_DQN_mem-50000.pth ===\n",
      "W&B run: test_Acrobot_DQN_mem-50000_1763008352\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_063232-ecxhi26c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ecxhi26c' target=\"_blank\">test_Acrobot_DQN_mem-50000_1763008352</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ecxhi26c' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ecxhi26c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-71.00 steps=72\n",
      "  Ep 2/100: reward=-72.00 steps=73\n",
      "  Ep 3/100: reward=-91.00 steps=92\n",
      "  Ep 4/100: reward=-69.00 steps=70\n",
      "  Ep 5/100: reward=-71.00 steps=72\n",
      "  Ep 6/100: reward=-69.00 steps=70\n",
      "  Ep 7/100: reward=-107.00 steps=108\n",
      "  Ep 8/100: reward=-70.00 steps=71\n",
      "  Ep 9/100: reward=-71.00 steps=72\n",
      "  Ep 10/100: reward=-131.00 steps=132\n",
      "  Ep 11/100: reward=-83.00 steps=84\n",
      "  Ep 12/100: reward=-140.00 steps=141\n",
      "  Ep 13/100: reward=-74.00 steps=75\n",
      "  Ep 14/100: reward=-91.00 steps=92\n",
      "  Ep 15/100: reward=-71.00 steps=72\n",
      "  Ep 16/100: reward=-94.00 steps=95\n",
      "  Ep 17/100: reward=-62.00 steps=63\n",
      "  Ep 18/100: reward=-80.00 steps=81\n",
      "  Ep 19/100: reward=-89.00 steps=90\n",
      "  Ep 20/100: reward=-109.00 steps=110\n",
      "  Ep 21/100: reward=-126.00 steps=127\n",
      "  Ep 22/100: reward=-74.00 steps=75\n",
      "  Ep 23/100: reward=-63.00 steps=64\n",
      "  Ep 24/100: reward=-93.00 steps=94\n",
      "  Ep 25/100: reward=-79.00 steps=80\n",
      "  Ep 26/100: reward=-112.00 steps=113\n",
      "  Ep 27/100: reward=-124.00 steps=125\n",
      "  Ep 28/100: reward=-70.00 steps=71\n",
      "  Ep 29/100: reward=-72.00 steps=73\n",
      "  Ep 30/100: reward=-74.00 steps=75\n",
      "  Ep 31/100: reward=-97.00 steps=98\n",
      "  Ep 32/100: reward=-91.00 steps=92\n",
      "  Ep 33/100: reward=-74.00 steps=75\n",
      "  Ep 34/100: reward=-69.00 steps=70\n",
      "  Ep 35/100: reward=-77.00 steps=78\n",
      "  Ep 36/100: reward=-64.00 steps=65\n",
      "  Ep 37/100: reward=-101.00 steps=102\n",
      "  Ep 38/100: reward=-69.00 steps=70\n",
      "  Ep 39/100: reward=-63.00 steps=64\n",
      "  Ep 40/100: reward=-86.00 steps=87\n",
      "  Ep 41/100: reward=-72.00 steps=73\n",
      "  Ep 42/100: reward=-86.00 steps=87\n",
      "  Ep 43/100: reward=-74.00 steps=75\n",
      "  Ep 44/100: reward=-70.00 steps=71\n",
      "  Ep 45/100: reward=-75.00 steps=76\n",
      "  Ep 46/100: reward=-70.00 steps=71\n",
      "  Ep 47/100: reward=-86.00 steps=87\n",
      "  Ep 48/100: reward=-70.00 steps=71\n",
      "  Ep 49/100: reward=-89.00 steps=90\n",
      "  Ep 50/100: reward=-86.00 steps=87\n",
      "  Ep 51/100: reward=-63.00 steps=64\n",
      "  Ep 52/100: reward=-86.00 steps=87\n",
      "  Ep 53/100: reward=-64.00 steps=65\n",
      "  Ep 54/100: reward=-72.00 steps=73\n",
      "  Ep 55/100: reward=-69.00 steps=70\n",
      "  Ep 56/100: reward=-76.00 steps=77\n",
      "  Ep 57/100: reward=-76.00 steps=77\n",
      "  Ep 58/100: reward=-72.00 steps=73\n",
      "  Ep 59/100: reward=-71.00 steps=72\n",
      "  Ep 60/100: reward=-70.00 steps=71\n",
      "  Ep 61/100: reward=-75.00 steps=76\n",
      "  Ep 62/100: reward=-71.00 steps=72\n",
      "  Ep 63/100: reward=-71.00 steps=72\n",
      "  Ep 64/100: reward=-70.00 steps=71\n",
      "  Ep 65/100: reward=-90.00 steps=91\n",
      "  Ep 66/100: reward=-109.00 steps=110\n",
      "  Ep 67/100: reward=-86.00 steps=87\n",
      "  Ep 68/100: reward=-75.00 steps=76\n",
      "  Ep 69/100: reward=-63.00 steps=64\n",
      "  Ep 70/100: reward=-63.00 steps=64\n",
      "  Ep 71/100: reward=-71.00 steps=72\n",
      "  Ep 72/100: reward=-75.00 steps=76\n",
      "  Ep 73/100: reward=-84.00 steps=85\n",
      "  Ep 74/100: reward=-64.00 steps=65\n",
      "  Ep 75/100: reward=-87.00 steps=88\n",
      "  Ep 76/100: reward=-96.00 steps=97\n",
      "  Ep 77/100: reward=-72.00 steps=73\n",
      "  Ep 78/100: reward=-72.00 steps=73\n",
      "  Ep 79/100: reward=-63.00 steps=64\n",
      "  Ep 80/100: reward=-74.00 steps=75\n",
      "  Ep 81/100: reward=-79.00 steps=80\n",
      "  Ep 82/100: reward=-79.00 steps=80\n",
      "  Ep 83/100: reward=-71.00 steps=72\n",
      "  Ep 84/100: reward=-88.00 steps=89\n",
      "  Ep 85/100: reward=-96.00 steps=97\n",
      "  Ep 86/100: reward=-70.00 steps=71\n",
      "  Ep 87/100: reward=-79.00 steps=80\n",
      "  Ep 88/100: reward=-71.00 steps=72\n",
      "  Ep 89/100: reward=-97.00 steps=98\n",
      "  Ep 90/100: reward=-80.00 steps=81\n",
      "  Ep 91/100: reward=-71.00 steps=72\n",
      "  Ep 92/100: reward=-90.00 steps=91\n",
      "  Ep 93/100: reward=-98.00 steps=99\n",
      "  Ep 94/100: reward=-82.00 steps=83\n",
      "  Ep 95/100: reward=-135.00 steps=136\n",
      "  Ep 96/100: reward=-97.00 steps=98\n",
      "  Ep 97/100: reward=-70.00 steps=71\n",
      "  Ep 98/100: reward=-86.00 steps=87\n",
      "  Ep 99/100: reward=-72.00 steps=73\n",
      "  Ep 100/100: reward=-95.00 steps=96\n",
      "Finished evaluations. avg=-81.27 best=-62.00 worst=-140.00\n",
      "Recording random episode index 22 for model Acrobot_DQN_mem-50000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\Acrobot-v1\\Acrobot_DQN_mem-50000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/Acrobot-v1/Acrobot_DQN_mem-50000\\Acrobot_DQN_mem-50000-episode-0.mp4\n",
      "Recording complete. Total reward: -71.00, Steps:72, Duration: 0.44 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>episode_reward</td><td>▅▇▇▇▁▆▄▇█▅▄▇▅▇▅▇▇▇▇▆▇▇▇▇▇▇▆▄▆█▇█▇▇▇▇▅▇▇▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂▂▂▅▂▄▄▆▂▂▄▂▁▅▂▃▂▂▃▃▂▂▂▂▂▃▃▂▂▁▃▃▂▃▄▃▄█▂▄</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-81.27</td></tr><tr><td>best_reward</td><td>-62</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-95</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.44344</td></tr><tr><td>recorded_episode_index</td><td>22</td></tr><tr><td>recorded_episode_reward</td><td>-71</td></tr><tr><td>recorded_episode_steps</td><td>72</td></tr><tr><td>steps</td><td>96</td></tr><tr><td>worst_reward</td><td>-140</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_Acrobot_DQN_mem-50000_1763008352</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ecxhi26c' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ecxhi26c</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063232-ecxhi26c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_models_folder_with_wandb(\n",
    "    env_name=\"Acrobot-v1\",\n",
    "    num_discrete_actions=5,\n",
    "    models_folder=\"./Acrobot models\",\n",
    "    num_tests=100,\n",
    "    wandb_project=\"RL A2 tests\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7af17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_batch-128.pth ===\n",
      "W&B run: test_MountainCar_DQN_batch-128_1763010684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_reward</td><td>██▁████▁▁▇██▁███████▇██▇▇▇▇▁▇▇▁▁▇▇█▁▁██▆</td></tr><tr><td>steps</td><td>▁▁█▁▁▁▁█▁█▁▁▁▁▁▂▁█▁█▂▁▁▂▁▂▁▁▁▁█▁▂██▁███▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-158.97</td></tr><tr><td>best_reward</td><td>-140</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-153</td></tr><tr><td>steps</td><td>153</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_mem-10000 _1763008465</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/jgk63w8c' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/jgk63w8c</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_063425-jgk63w8c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071124-9ucjiws1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9ucjiws1' target=\"_blank\">test_MountainCar_DQN_batch-128_1763010684</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9ucjiws1' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9ucjiws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 21 for model MountainCar_DQN_batch-128.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_batch-128 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_batch-128\\MountainCar_DQN_batch-128-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 1.29 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.29392</td></tr><tr><td>recorded_episode_index</td><td>21</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_batch-128_1763010684</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9ucjiws1' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9ucjiws1</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071124-9ucjiws1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_batch-256.pth ===\n",
      "W&B run: test_MountainCar_DQN_batch-256_1763010693\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071133-ofbpxtng</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ofbpxtng' target=\"_blank\">test_MountainCar_DQN_batch-256_1763010693</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ofbpxtng' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ofbpxtng</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 20 for model MountainCar_DQN_batch-256.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_batch-256 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_batch-256\\MountainCar_DQN_batch-256-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 0.85 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.8468</td></tr><tr><td>recorded_episode_index</td><td>20</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_batch-256_1763010693</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ofbpxtng' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/ofbpxtng</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071133-ofbpxtng\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_batch-64.pth ===\n",
      "W&B run: test_MountainCar_DQN_batch-64_1763010700\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071140-oeqhrggc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/oeqhrggc' target=\"_blank\">test_MountainCar_DQN_batch-64_1763010700</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/oeqhrggc' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/oeqhrggc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 40 for model MountainCar_DQN_batch-64.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_batch-64 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_batch-64\\MountainCar_DQN_batch-64-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 0.78 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.7784</td></tr><tr><td>recorded_episode_index</td><td>40</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_batch-64_1763010700</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/oeqhrggc' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/oeqhrggc</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071140-oeqhrggc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_eps-1000.pth ===\n",
      "W&B run: test_MountainCar_DQN_eps-1000_1763010706\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071146-uzsvb2j4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uzsvb2j4' target=\"_blank\">test_MountainCar_DQN_eps-1000_1763010706</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uzsvb2j4' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uzsvb2j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-104.00 steps=104\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-136.00 steps=136\n",
      "  Ep 4/100: reward=-104.00 steps=104\n",
      "  Ep 5/100: reward=-142.00 steps=142\n",
      "  Ep 6/100: reward=-135.00 steps=135\n",
      "  Ep 7/100: reward=-91.00 steps=91\n",
      "  Ep 8/100: reward=-104.00 steps=104\n",
      "  Ep 9/100: reward=-143.00 steps=143\n",
      "  Ep 10/100: reward=-136.00 steps=136\n",
      "  Ep 11/100: reward=-104.00 steps=104\n",
      "  Ep 12/100: reward=-140.00 steps=140\n",
      "  Ep 13/100: reward=-137.00 steps=137\n",
      "  Ep 14/100: reward=-137.00 steps=137\n",
      "  Ep 15/100: reward=-145.00 steps=145\n",
      "  Ep 16/100: reward=-104.00 steps=104\n",
      "  Ep 17/100: reward=-136.00 steps=136\n",
      "  Ep 18/100: reward=-104.00 steps=104\n",
      "  Ep 19/100: reward=-162.00 steps=162\n",
      "  Ep 20/100: reward=-89.00 steps=89\n",
      "  Ep 21/100: reward=-104.00 steps=104\n",
      "  Ep 22/100: reward=-104.00 steps=104\n",
      "  Ep 23/100: reward=-137.00 steps=137\n",
      "  Ep 24/100: reward=-141.00 steps=141\n",
      "  Ep 25/100: reward=-104.00 steps=104\n",
      "  Ep 26/100: reward=-85.00 steps=85\n",
      "  Ep 27/100: reward=-91.00 steps=91\n",
      "  Ep 28/100: reward=-104.00 steps=104\n",
      "  Ep 29/100: reward=-136.00 steps=136\n",
      "  Ep 30/100: reward=-90.00 steps=90\n",
      "  Ep 31/100: reward=-87.00 steps=87\n",
      "  Ep 32/100: reward=-141.00 steps=141\n",
      "  Ep 33/100: reward=-137.00 steps=137\n",
      "  Ep 34/100: reward=-141.00 steps=141\n",
      "  Ep 35/100: reward=-105.00 steps=105\n",
      "  Ep 36/100: reward=-96.00 steps=96\n",
      "  Ep 37/100: reward=-104.00 steps=104\n",
      "  Ep 38/100: reward=-151.00 steps=151\n",
      "  Ep 39/100: reward=-135.00 steps=135\n",
      "  Ep 40/100: reward=-137.00 steps=137\n",
      "  Ep 41/100: reward=-104.00 steps=104\n",
      "  Ep 42/100: reward=-85.00 steps=85\n",
      "  Ep 43/100: reward=-98.00 steps=98\n",
      "  Ep 44/100: reward=-104.00 steps=104\n",
      "  Ep 45/100: reward=-135.00 steps=135\n",
      "  Ep 46/100: reward=-104.00 steps=104\n",
      "  Ep 47/100: reward=-104.00 steps=104\n",
      "  Ep 48/100: reward=-149.00 steps=149\n",
      "  Ep 49/100: reward=-90.00 steps=90\n",
      "  Ep 50/100: reward=-142.00 steps=142\n",
      "  Ep 51/100: reward=-136.00 steps=136\n",
      "  Ep 52/100: reward=-88.00 steps=88\n",
      "  Ep 53/100: reward=-85.00 steps=85\n",
      "  Ep 54/100: reward=-92.00 steps=92\n",
      "  Ep 55/100: reward=-105.00 steps=105\n",
      "  Ep 56/100: reward=-136.00 steps=136\n",
      "  Ep 57/100: reward=-97.00 steps=97\n",
      "  Ep 58/100: reward=-85.00 steps=85\n",
      "  Ep 59/100: reward=-84.00 steps=84\n",
      "  Ep 60/100: reward=-134.00 steps=134\n",
      "  Ep 61/100: reward=-137.00 steps=137\n",
      "  Ep 62/100: reward=-105.00 steps=105\n",
      "  Ep 63/100: reward=-90.00 steps=90\n",
      "  Ep 64/100: reward=-135.00 steps=135\n",
      "  Ep 65/100: reward=-91.00 steps=91\n",
      "  Ep 66/100: reward=-99.00 steps=99\n",
      "  Ep 67/100: reward=-104.00 steps=104\n",
      "  Ep 68/100: reward=-147.00 steps=147\n",
      "  Ep 69/100: reward=-134.00 steps=134\n",
      "  Ep 70/100: reward=-138.00 steps=138\n",
      "  Ep 71/100: reward=-137.00 steps=137\n",
      "  Ep 72/100: reward=-104.00 steps=104\n",
      "  Ep 73/100: reward=-137.00 steps=137\n",
      "  Ep 74/100: reward=-104.00 steps=104\n",
      "  Ep 75/100: reward=-136.00 steps=136\n",
      "  Ep 76/100: reward=-143.00 steps=143\n",
      "  Ep 77/100: reward=-86.00 steps=86\n",
      "  Ep 78/100: reward=-143.00 steps=143\n",
      "  Ep 79/100: reward=-136.00 steps=136\n",
      "  Ep 80/100: reward=-104.00 steps=104\n",
      "  Ep 81/100: reward=-136.00 steps=136\n",
      "  Ep 82/100: reward=-136.00 steps=136\n",
      "  Ep 83/100: reward=-135.00 steps=135\n",
      "  Ep 84/100: reward=-136.00 steps=136\n",
      "  Ep 85/100: reward=-85.00 steps=85\n",
      "  Ep 86/100: reward=-87.00 steps=87\n",
      "  Ep 87/100: reward=-145.00 steps=145\n",
      "  Ep 88/100: reward=-135.00 steps=135\n",
      "  Ep 89/100: reward=-158.00 steps=158\n",
      "  Ep 90/100: reward=-154.00 steps=154\n",
      "  Ep 91/100: reward=-134.00 steps=134\n",
      "  Ep 92/100: reward=-142.00 steps=142\n",
      "  Ep 93/100: reward=-137.00 steps=137\n",
      "  Ep 94/100: reward=-84.00 steps=84\n",
      "  Ep 95/100: reward=-135.00 steps=135\n",
      "  Ep 96/100: reward=-98.00 steps=98\n",
      "  Ep 97/100: reward=-89.00 steps=89\n",
      "  Ep 98/100: reward=-135.00 steps=135\n",
      "  Ep 99/100: reward=-135.00 steps=135\n",
      "  Ep 100/100: reward=-137.00 steps=137\n",
      "Finished evaluations. avg=-119.83 best=-84.00 worst=-200.00\n",
      "Recording random episode index 82 for model MountainCar_DQN_eps-1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_eps-1000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_eps-1000\\MountainCar_DQN_eps-1000-episode-0.mp4\n",
      "Recording complete. Total reward: -137.00, Steps:137, Duration: 0.64 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▇▁▇▅█▇▅▅▄▇▇▇▇▅▇▄▅▅▇▄▇█▅█▅▅▅▄▄▇██▄▅▄▅▅█▅▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▂█▅▂▄▁▄▁▄▂▄▂▄▂▅▄▁▁▂▁▁▄▁▂▄▄▁▄▂▄▄▁▁▅▅▄▁▂▄▄</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-119.83</td></tr><tr><td>best_reward</td><td>-84</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-137</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.63938</td></tr><tr><td>recorded_episode_index</td><td>82</td></tr><tr><td>recorded_episode_reward</td><td>-137</td></tr><tr><td>recorded_episode_steps</td><td>137</td></tr><tr><td>steps</td><td>137</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_eps-1000_1763010706</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uzsvb2j4' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/uzsvb2j4</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071146-uzsvb2j4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_eps-2500.pth ===\n",
      "W&B run: test_MountainCar_DQN_eps-2500_1763010712\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071152-4ifezcbu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4ifezcbu' target=\"_blank\">test_MountainCar_DQN_eps-2500_1763010712</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4ifezcbu' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4ifezcbu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-86.00 steps=86\n",
      "  Ep 2/100: reward=-184.00 steps=184\n",
      "  Ep 3/100: reward=-143.00 steps=143\n",
      "  Ep 4/100: reward=-170.00 steps=170\n",
      "  Ep 5/100: reward=-143.00 steps=143\n",
      "  Ep 6/100: reward=-143.00 steps=143\n",
      "  Ep 7/100: reward=-90.00 steps=90\n",
      "  Ep 8/100: reward=-172.00 steps=172\n",
      "  Ep 9/100: reward=-98.00 steps=98\n",
      "  Ep 10/100: reward=-143.00 steps=143\n",
      "  Ep 11/100: reward=-142.00 steps=142\n",
      "  Ep 12/100: reward=-143.00 steps=143\n",
      "  Ep 13/100: reward=-92.00 steps=92\n",
      "  Ep 14/100: reward=-94.00 steps=94\n",
      "  Ep 15/100: reward=-143.00 steps=143\n",
      "  Ep 16/100: reward=-93.00 steps=93\n",
      "  Ep 17/100: reward=-96.00 steps=96\n",
      "  Ep 18/100: reward=-103.00 steps=103\n",
      "  Ep 19/100: reward=-93.00 steps=93\n",
      "  Ep 20/100: reward=-143.00 steps=143\n",
      "  Ep 21/100: reward=-142.00 steps=142\n",
      "  Ep 22/100: reward=-143.00 steps=143\n",
      "  Ep 23/100: reward=-193.00 steps=193\n",
      "  Ep 24/100: reward=-143.00 steps=143\n",
      "  Ep 25/100: reward=-143.00 steps=143\n",
      "  Ep 26/100: reward=-143.00 steps=143\n",
      "  Ep 27/100: reward=-87.00 steps=87\n",
      "  Ep 28/100: reward=-143.00 steps=143\n",
      "  Ep 29/100: reward=-143.00 steps=143\n",
      "  Ep 30/100: reward=-91.00 steps=91\n",
      "  Ep 31/100: reward=-177.00 steps=177\n",
      "  Ep 32/100: reward=-143.00 steps=143\n",
      "  Ep 33/100: reward=-143.00 steps=143\n",
      "  Ep 34/100: reward=-143.00 steps=143\n",
      "  Ep 35/100: reward=-143.00 steps=143\n",
      "  Ep 36/100: reward=-84.00 steps=84\n",
      "  Ep 37/100: reward=-143.00 steps=143\n",
      "  Ep 38/100: reward=-143.00 steps=143\n",
      "  Ep 39/100: reward=-144.00 steps=144\n",
      "  Ep 40/100: reward=-143.00 steps=143\n",
      "  Ep 41/100: reward=-87.00 steps=87\n",
      "  Ep 42/100: reward=-88.00 steps=88\n",
      "  Ep 43/100: reward=-143.00 steps=143\n",
      "  Ep 44/100: reward=-172.00 steps=172\n",
      "  Ep 45/100: reward=-132.00 steps=132\n",
      "  Ep 46/100: reward=-90.00 steps=90\n",
      "  Ep 47/100: reward=-143.00 steps=143\n",
      "  Ep 48/100: reward=-86.00 steps=86\n",
      "  Ep 49/100: reward=-183.00 steps=183\n",
      "  Ep 50/100: reward=-144.00 steps=144\n",
      "  Ep 51/100: reward=-144.00 steps=144\n",
      "  Ep 52/100: reward=-97.00 steps=97\n",
      "  Ep 53/100: reward=-87.00 steps=87\n",
      "  Ep 54/100: reward=-93.00 steps=93\n",
      "  Ep 55/100: reward=-99.00 steps=99\n",
      "  Ep 56/100: reward=-143.00 steps=143\n",
      "  Ep 57/100: reward=-143.00 steps=143\n",
      "  Ep 58/100: reward=-101.00 steps=101\n",
      "  Ep 59/100: reward=-122.00 steps=122\n",
      "  Ep 60/100: reward=-105.00 steps=105\n",
      "  Ep 61/100: reward=-97.00 steps=97\n",
      "  Ep 62/100: reward=-143.00 steps=143\n",
      "  Ep 63/100: reward=-144.00 steps=144\n",
      "  Ep 64/100: reward=-143.00 steps=143\n",
      "  Ep 65/100: reward=-142.00 steps=142\n",
      "  Ep 66/100: reward=-87.00 steps=87\n",
      "  Ep 67/100: reward=-144.00 steps=144\n",
      "  Ep 68/100: reward=-182.00 steps=182\n",
      "  Ep 69/100: reward=-144.00 steps=144\n",
      "  Ep 70/100: reward=-134.00 steps=134\n",
      "  Ep 71/100: reward=-143.00 steps=143\n",
      "  Ep 72/100: reward=-143.00 steps=143\n",
      "  Ep 73/100: reward=-144.00 steps=144\n",
      "  Ep 74/100: reward=-143.00 steps=143\n",
      "  Ep 75/100: reward=-144.00 steps=144\n",
      "  Ep 76/100: reward=-177.00 steps=177\n",
      "  Ep 77/100: reward=-167.00 steps=167\n",
      "  Ep 78/100: reward=-143.00 steps=143\n",
      "  Ep 79/100: reward=-143.00 steps=143\n",
      "  Ep 80/100: reward=-97.00 steps=97\n",
      "  Ep 81/100: reward=-85.00 steps=85\n",
      "  Ep 82/100: reward=-84.00 steps=84\n",
      "  Ep 83/100: reward=-174.00 steps=174\n",
      "  Ep 84/100: reward=-96.00 steps=96\n",
      "  Ep 85/100: reward=-179.00 steps=179\n",
      "  Ep 86/100: reward=-92.00 steps=92\n",
      "  Ep 87/100: reward=-143.00 steps=143\n",
      "  Ep 88/100: reward=-143.00 steps=143\n",
      "  Ep 89/100: reward=-116.00 steps=116\n",
      "  Ep 90/100: reward=-86.00 steps=86\n",
      "  Ep 91/100: reward=-143.00 steps=143\n",
      "  Ep 92/100: reward=-83.00 steps=83\n",
      "  Ep 93/100: reward=-144.00 steps=144\n",
      "  Ep 94/100: reward=-144.00 steps=144\n",
      "  Ep 95/100: reward=-143.00 steps=143\n",
      "  Ep 96/100: reward=-143.00 steps=143\n",
      "  Ep 97/100: reward=-85.00 steps=85\n",
      "  Ep 98/100: reward=-144.00 steps=144\n",
      "  Ep 99/100: reward=-94.00 steps=94\n",
      "  Ep 100/100: reward=-142.00 steps=142\n",
      "Finished evaluations. avg=-129.60 best=-83.00 worst=-193.00\n",
      "Recording random episode index 53 for model MountainCar_DQN_eps-2500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_eps-2500 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_eps-2500\\MountainCar_DQN_eps-2500-episode-0.mp4\n",
      "Recording complete. Total reward: -144.00, Steps:144, Duration: 0.63 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▄▄█▂▄▄▇▇▄▄▄▇▁▄██▄▂█▄▇▇▇▄▄▄▄▄▄▄██▇▁▇▆▄▄▄▄</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▅▅▂▅▂▅▂▂▅▅▅▅▅▅▅▅▅▁▅▂▂▅▄▃▁▅▅▇▅▂▁█▂▅▅▁▅▅▅</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-129.6</td></tr><tr><td>best_reward</td><td>-83</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-142</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.63467</td></tr><tr><td>recorded_episode_index</td><td>53</td></tr><tr><td>recorded_episode_reward</td><td>-144</td></tr><tr><td>recorded_episode_steps</td><td>144</td></tr><tr><td>steps</td><td>142</td></tr><tr><td>worst_reward</td><td>-193</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_eps-2500_1763010712</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4ifezcbu' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4ifezcbu</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071152-4ifezcbu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_eps-500.pth ===\n",
      "W&B run: test_MountainCar_DQN_eps-500_1763010717\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071157-w35d35a9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/w35d35a9' target=\"_blank\">test_MountainCar_DQN_eps-500_1763010717</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/w35d35a9' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/w35d35a9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-143.00 steps=143\n",
      "  Ep 2/100: reward=-97.00 steps=97\n",
      "  Ep 3/100: reward=-138.00 steps=138\n",
      "  Ep 4/100: reward=-137.00 steps=137\n",
      "  Ep 5/100: reward=-138.00 steps=138\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-149.00 steps=149\n",
      "  Ep 9/100: reward=-143.00 steps=143\n",
      "  Ep 10/100: reward=-141.00 steps=141\n",
      "  Ep 11/100: reward=-144.00 steps=144\n",
      "  Ep 12/100: reward=-144.00 steps=144\n",
      "  Ep 13/100: reward=-149.00 steps=149\n",
      "  Ep 14/100: reward=-140.00 steps=140\n",
      "  Ep 15/100: reward=-150.00 steps=150\n",
      "  Ep 16/100: reward=-156.00 steps=156\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-139.00 steps=139\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-99.00 steps=99\n",
      "  Ep 21/100: reward=-144.00 steps=144\n",
      "  Ep 22/100: reward=-146.00 steps=146\n",
      "  Ep 23/100: reward=-137.00 steps=137\n",
      "  Ep 24/100: reward=-146.00 steps=146\n",
      "  Ep 25/100: reward=-151.00 steps=151\n",
      "  Ep 26/100: reward=-140.00 steps=140\n",
      "  Ep 27/100: reward=-139.00 steps=139\n",
      "  Ep 28/100: reward=-137.00 steps=137\n",
      "  Ep 29/100: reward=-144.00 steps=144\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-140.00 steps=140\n",
      "  Ep 32/100: reward=-149.00 steps=149\n",
      "  Ep 33/100: reward=-141.00 steps=141\n",
      "  Ep 34/100: reward=-139.00 steps=139\n",
      "  Ep 35/100: reward=-95.00 steps=95\n",
      "  Ep 36/100: reward=-137.00 steps=137\n",
      "  Ep 37/100: reward=-125.00 steps=125\n",
      "  Ep 38/100: reward=-141.00 steps=141\n",
      "  Ep 39/100: reward=-105.00 steps=105\n",
      "  Ep 40/100: reward=-137.00 steps=137\n",
      "  Ep 41/100: reward=-138.00 steps=138\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-144.00 steps=144\n",
      "  Ep 44/100: reward=-141.00 steps=141\n",
      "  Ep 45/100: reward=-140.00 steps=140\n",
      "  Ep 46/100: reward=-142.00 steps=142\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-153.00 steps=153\n",
      "  Ep 49/100: reward=-156.00 steps=156\n",
      "  Ep 50/100: reward=-138.00 steps=138\n",
      "  Ep 51/100: reward=-140.00 steps=140\n",
      "  Ep 52/100: reward=-149.00 steps=149\n",
      "  Ep 53/100: reward=-138.00 steps=138\n",
      "  Ep 54/100: reward=-137.00 steps=137\n",
      "  Ep 55/100: reward=-138.00 steps=138\n",
      "  Ep 56/100: reward=-144.00 steps=144\n",
      "  Ep 57/100: reward=-137.00 steps=137\n",
      "  Ep 58/100: reward=-143.00 steps=143\n",
      "  Ep 59/100: reward=-158.00 steps=158\n",
      "  Ep 60/100: reward=-144.00 steps=144\n",
      "  Ep 61/100: reward=-144.00 steps=144\n",
      "  Ep 62/100: reward=-156.00 steps=156\n",
      "  Ep 63/100: reward=-154.00 steps=154\n",
      "  Ep 64/100: reward=-139.00 steps=139\n",
      "  Ep 65/100: reward=-140.00 steps=140\n",
      "  Ep 66/100: reward=-170.00 steps=170\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-113.00 steps=113\n",
      "  Ep 70/100: reward=-141.00 steps=141\n",
      "  Ep 71/100: reward=-140.00 steps=140\n",
      "  Ep 72/100: reward=-141.00 steps=141\n",
      "  Ep 73/100: reward=-143.00 steps=143\n",
      "  Ep 74/100: reward=-138.00 steps=138\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-139.00 steps=139\n",
      "  Ep 77/100: reward=-141.00 steps=141\n",
      "  Ep 78/100: reward=-141.00 steps=141\n",
      "  Ep 79/100: reward=-137.00 steps=137\n",
      "  Ep 80/100: reward=-138.00 steps=138\n",
      "  Ep 81/100: reward=-98.00 steps=98\n",
      "  Ep 82/100: reward=-144.00 steps=144\n",
      "  Ep 83/100: reward=-141.00 steps=141\n",
      "  Ep 84/100: reward=-137.00 steps=137\n",
      "  Ep 85/100: reward=-137.00 steps=137\n",
      "  Ep 86/100: reward=-144.00 steps=144\n",
      "  Ep 87/100: reward=-140.00 steps=140\n",
      "  Ep 88/100: reward=-139.00 steps=139\n",
      "  Ep 89/100: reward=-137.00 steps=137\n",
      "  Ep 90/100: reward=-140.00 steps=140\n",
      "  Ep 91/100: reward=-139.00 steps=139\n",
      "  Ep 92/100: reward=-138.00 steps=138\n",
      "  Ep 93/100: reward=-140.00 steps=140\n",
      "  Ep 94/100: reward=-160.00 steps=160\n",
      "  Ep 95/100: reward=-112.00 steps=112\n",
      "  Ep 96/100: reward=-151.00 steps=151\n",
      "  Ep 97/100: reward=-141.00 steps=141\n",
      "  Ep 98/100: reward=-139.00 steps=139\n",
      "  Ep 99/100: reward=-143.00 steps=143\n",
      "  Ep 100/100: reward=-144.00 steps=144\n",
      "Finished evaluations. avg=-145.59 best=-95.00 worst=-200.00\n",
      "Recording random episode index 93 for model MountainCar_DQN_eps-500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_eps-500 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_eps-500\\MountainCar_DQN_eps-500-episode-0.mp4\n",
      "Recording complete. Total reward: -144.00, Steps:144, Duration: 0.65 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>episode_reward</td><td>▅▆▁▅▅▁▅▁▅▆▅▆▁▅▅▅▅█▁▅▁▄▆▅▆▆▅▅▄▅▃▅▆▅▅▅▆▄▇▅</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▄▄█▅▄▄▄▅▅█▄▄▅▄█▃▁▄▄▄▄█▄▄▄▄▅██▂▄▄█▄▁▄▄▄▄▄</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-145.59</td></tr><tr><td>best_reward</td><td>-95</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-144</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.64772</td></tr><tr><td>recorded_episode_index</td><td>93</td></tr><tr><td>recorded_episode_reward</td><td>-144</td></tr><tr><td>recorded_episode_steps</td><td>144</td></tr><tr><td>steps</td><td>144</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_eps-500_1763010717</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/w35d35a9' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/w35d35a9</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071157-w35d35a9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_eps-5000.pth ===\n",
      "W&B run: test_MountainCar_DQN_eps-5000_1763010723\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071203-4qltbfrg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4qltbfrg' target=\"_blank\">test_MountainCar_DQN_eps-5000_1763010723</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4qltbfrg' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4qltbfrg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-148.00 steps=148\n",
      "  Ep 2/100: reward=-141.00 steps=141\n",
      "  Ep 3/100: reward=-142.00 steps=142\n",
      "  Ep 4/100: reward=-151.00 steps=151\n",
      "  Ep 5/100: reward=-140.00 steps=140\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-148.00 steps=148\n",
      "  Ep 8/100: reward=-146.00 steps=146\n",
      "  Ep 9/100: reward=-157.00 steps=157\n",
      "  Ep 10/100: reward=-143.00 steps=143\n",
      "  Ep 11/100: reward=-173.00 steps=173\n",
      "  Ep 12/100: reward=-148.00 steps=148\n",
      "  Ep 13/100: reward=-143.00 steps=143\n",
      "  Ep 14/100: reward=-149.00 steps=149\n",
      "  Ep 15/100: reward=-142.00 steps=142\n",
      "  Ep 16/100: reward=-109.00 steps=109\n",
      "  Ep 17/100: reward=-116.00 steps=116\n",
      "  Ep 18/100: reward=-141.00 steps=141\n",
      "  Ep 19/100: reward=-91.00 steps=91\n",
      "  Ep 20/100: reward=-143.00 steps=143\n",
      "  Ep 21/100: reward=-144.00 steps=144\n",
      "  Ep 22/100: reward=-145.00 steps=145\n",
      "  Ep 23/100: reward=-149.00 steps=149\n",
      "  Ep 24/100: reward=-161.00 steps=161\n",
      "  Ep 25/100: reward=-141.00 steps=141\n",
      "  Ep 26/100: reward=-140.00 steps=140\n",
      "  Ep 27/100: reward=-142.00 steps=142\n",
      "  Ep 28/100: reward=-153.00 steps=153\n",
      "  Ep 29/100: reward=-147.00 steps=147\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-144.00 steps=144\n",
      "  Ep 32/100: reward=-141.00 steps=141\n",
      "  Ep 33/100: reward=-149.00 steps=149\n",
      "  Ep 34/100: reward=-95.00 steps=95\n",
      "  Ep 35/100: reward=-154.00 steps=154\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-147.00 steps=147\n",
      "  Ep 38/100: reward=-155.00 steps=155\n",
      "  Ep 39/100: reward=-143.00 steps=143\n",
      "  Ep 40/100: reward=-140.00 steps=140\n",
      "  Ep 41/100: reward=-99.00 steps=99\n",
      "  Ep 42/100: reward=-148.00 steps=148\n",
      "  Ep 43/100: reward=-143.00 steps=143\n",
      "  Ep 44/100: reward=-145.00 steps=145\n",
      "  Ep 45/100: reward=-141.00 steps=141\n",
      "  Ep 46/100: reward=-99.00 steps=99\n",
      "  Ep 47/100: reward=-149.00 steps=149\n",
      "  Ep 48/100: reward=-92.00 steps=92\n",
      "  Ep 49/100: reward=-142.00 steps=142\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-161.00 steps=161\n",
      "  Ep 52/100: reward=-146.00 steps=146\n",
      "  Ep 53/100: reward=-149.00 steps=149\n",
      "  Ep 54/100: reward=-135.00 steps=135\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-146.00 steps=146\n",
      "  Ep 57/100: reward=-150.00 steps=150\n",
      "  Ep 58/100: reward=-144.00 steps=144\n",
      "  Ep 59/100: reward=-165.00 steps=165\n",
      "  Ep 60/100: reward=-150.00 steps=150\n",
      "  Ep 61/100: reward=-137.00 steps=137\n",
      "  Ep 62/100: reward=-146.00 steps=146\n",
      "  Ep 63/100: reward=-145.00 steps=145\n",
      "  Ep 64/100: reward=-100.00 steps=100\n",
      "  Ep 65/100: reward=-139.00 steps=139\n",
      "  Ep 66/100: reward=-144.00 steps=144\n",
      "  Ep 67/100: reward=-96.00 steps=96\n",
      "  Ep 68/100: reward=-144.00 steps=144\n",
      "  Ep 69/100: reward=-153.00 steps=153\n",
      "  Ep 70/100: reward=-145.00 steps=145\n",
      "  Ep 71/100: reward=-142.00 steps=142\n",
      "  Ep 72/100: reward=-165.00 steps=165\n",
      "  Ep 73/100: reward=-145.00 steps=145\n",
      "  Ep 74/100: reward=-154.00 steps=154\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-143.00 steps=143\n",
      "  Ep 77/100: reward=-138.00 steps=138\n",
      "  Ep 78/100: reward=-144.00 steps=144\n",
      "  Ep 79/100: reward=-137.00 steps=137\n",
      "  Ep 80/100: reward=-165.00 steps=165\n",
      "  Ep 81/100: reward=-140.00 steps=140\n",
      "  Ep 82/100: reward=-143.00 steps=143\n",
      "  Ep 83/100: reward=-115.00 steps=115\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-136.00 steps=136\n",
      "  Ep 86/100: reward=-97.00 steps=97\n",
      "  Ep 87/100: reward=-141.00 steps=141\n",
      "  Ep 88/100: reward=-141.00 steps=141\n",
      "  Ep 89/100: reward=-141.00 steps=141\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-152.00 steps=152\n",
      "  Ep 92/100: reward=-146.00 steps=146\n",
      "  Ep 93/100: reward=-183.00 steps=183\n",
      "  Ep 94/100: reward=-146.00 steps=146\n",
      "  Ep 95/100: reward=-141.00 steps=141\n",
      "  Ep 96/100: reward=-156.00 steps=156\n",
      "  Ep 97/100: reward=-142.00 steps=142\n",
      "  Ep 98/100: reward=-141.00 steps=141\n",
      "  Ep 99/100: reward=-144.00 steps=144\n",
      "  Ep 100/100: reward=-111.00 steps=111\n",
      "Finished evaluations. avg=-145.63 best=-91.00 worst=-200.00\n",
      "Recording random episode index 77 for model MountainCar_DQN_eps-5000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_eps-5000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_eps-5000\\MountainCar_DQN_eps-5000-episode-0.mp4\n",
      "Recording complete. Total reward: -97.00, Steps:97, Duration: 0.58 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>episode_reward</td><td>▅▄▅▅▅▅▅▅▅▅▁▅▄▄▁▅▅▅▅▅▄▅▃▅▅▄▅▅▅▅▅▇▅█▅▁▄▅▅▇</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▅▄█▅▅▅▃▄▁▄▅▄▅█▅▅▅▄▂▄▅▄█▅▄▅▄▅▆▅▄▆▄█▄▄▇▅▅▂</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-145.63</td></tr><tr><td>best_reward</td><td>-91</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-111</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.57965</td></tr><tr><td>recorded_episode_index</td><td>77</td></tr><tr><td>recorded_episode_reward</td><td>-97</td></tr><tr><td>recorded_episode_steps</td><td>97</td></tr><tr><td>steps</td><td>111</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_eps-5000_1763010723</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4qltbfrg' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/4qltbfrg</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071203-4qltbfrg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_gamma-0.1.pth ===\n",
      "W&B run: test_MountainCar_DQN_gamma-0.1_1763010729\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071209-9362ux9f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9362ux9f' target=\"_blank\">test_MountainCar_DQN_gamma-0.1_1763010729</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9362ux9f' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9362ux9f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 5 for model MountainCar_DQN_gamma-0.1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_gamma-0.1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_gamma-0.1\\MountainCar_DQN_gamma-0.1-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 1.11 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.11301</td></tr><tr><td>recorded_episode_index</td><td>5</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_gamma-0.1_1763010729</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9362ux9f' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9362ux9f</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071209-9362ux9f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_gamma-0.5.pth ===\n",
      "W&B run: test_MountainCar_DQN_gamma-0.5_1763010735\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071215-9uon4h4m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9uon4h4m' target=\"_blank\">test_MountainCar_DQN_gamma-0.5_1763010735</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9uon4h4m' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9uon4h4m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 31 for model MountainCar_DQN_gamma-0.5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_gamma-0.5 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_gamma-0.5\\MountainCar_DQN_gamma-0.5-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 0.97 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.97475</td></tr><tr><td>recorded_episode_index</td><td>31</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_gamma-0.5_1763010735</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9uon4h4m' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/9uon4h4m</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071215-9uon4h4m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_gamma-0.9.pth ===\n",
      "W&B run: test_MountainCar_DQN_gamma-0.9_1763010743\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071223-7ov7gmzg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7ov7gmzg' target=\"_blank\">test_MountainCar_DQN_gamma-0.9_1763010743</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7ov7gmzg' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7ov7gmzg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 8 for model MountainCar_DQN_gamma-0.9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_gamma-0.9 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_gamma-0.9\\MountainCar_DQN_gamma-0.9-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 0.88 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.87952</td></tr><tr><td>recorded_episode_index</td><td>8</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_gamma-0.9_1763010743</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7ov7gmzg' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7ov7gmzg</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071223-7ov7gmzg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_gamma-0.99.pth ===\n",
      "W&B run: test_MountainCar_DQN_gamma-0.99_1763010750\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071230-7b0dcjok</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7b0dcjok' target=\"_blank\">test_MountainCar_DQN_gamma-0.99_1763010750</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7b0dcjok' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7b0dcjok</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 14 for model MountainCar_DQN_gamma-0.99.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_gamma-0.99 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_gamma-0.99\\MountainCar_DQN_gamma-0.99-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 0.89 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.88959</td></tr><tr><td>recorded_episode_index</td><td>14</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_gamma-0.99_1763010750</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7b0dcjok' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/7b0dcjok</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071230-7b0dcjok\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_lr-0.0003.pth ===\n",
      "W&B run: test_MountainCar_DQN_lr-0.0003_1763010756\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071236-72j61hjw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/72j61hjw' target=\"_blank\">test_MountainCar_DQN_lr-0.0003_1763010756</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/72j61hjw' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/72j61hjw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 68 for model MountainCar_DQN_lr-0.0003.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_lr-0.0003 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_lr-0.0003\\MountainCar_DQN_lr-0.0003-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 1.02 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.01714</td></tr><tr><td>recorded_episode_index</td><td>68</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_lr-0.0003_1763010756</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/72j61hjw' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/72j61hjw</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071236-72j61hjw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_lr-0.001.pth ===\n",
      "W&B run: test_MountainCar_DQN_lr-0.001_1763010763\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071243-m2eocvc9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m2eocvc9' target=\"_blank\">test_MountainCar_DQN_lr-0.001_1763010763</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m2eocvc9' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m2eocvc9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 12 for model MountainCar_DQN_lr-0.001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_lr-0.001 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_lr-0.001\\MountainCar_DQN_lr-0.001-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 0.89 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.89155</td></tr><tr><td>recorded_episode_index</td><td>12</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_lr-0.001_1763010763</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m2eocvc9' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/m2eocvc9</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071243-m2eocvc9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_lr-0.01.pth ===\n",
      "W&B run: test_MountainCar_DQN_lr-0.01_1763010769\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071249-6186v7sr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/6186v7sr' target=\"_blank\">test_MountainCar_DQN_lr-0.01_1763010769</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/6186v7sr' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/6186v7sr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 4 for model MountainCar_DQN_lr-0.01.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_lr-0.01 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_lr-0.01\\MountainCar_DQN_lr-0.01-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 0.91 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>0.91325</td></tr><tr><td>recorded_episode_index</td><td>4</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_lr-0.01_1763010769</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/6186v7sr' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/6186v7sr</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071249-6186v7sr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_lr-1e-05.pth ===\n",
      "W&B run: test_MountainCar_DQN_lr-1e-05_1763010775\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071255-8zdozas8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8zdozas8' target=\"_blank\">test_MountainCar_DQN_lr-1e-05_1763010775</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8zdozas8' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8zdozas8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-200.00 steps=200\n",
      "  Ep 2/100: reward=-200.00 steps=200\n",
      "  Ep 3/100: reward=-200.00 steps=200\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-200.00 steps=200\n",
      "  Ep 6/100: reward=-200.00 steps=200\n",
      "  Ep 7/100: reward=-200.00 steps=200\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-200.00 steps=200\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-200.00 steps=200\n",
      "  Ep 12/100: reward=-200.00 steps=200\n",
      "  Ep 13/100: reward=-200.00 steps=200\n",
      "  Ep 14/100: reward=-200.00 steps=200\n",
      "  Ep 15/100: reward=-200.00 steps=200\n",
      "  Ep 16/100: reward=-200.00 steps=200\n",
      "  Ep 17/100: reward=-200.00 steps=200\n",
      "  Ep 18/100: reward=-200.00 steps=200\n",
      "  Ep 19/100: reward=-200.00 steps=200\n",
      "  Ep 20/100: reward=-200.00 steps=200\n",
      "  Ep 21/100: reward=-200.00 steps=200\n",
      "  Ep 22/100: reward=-200.00 steps=200\n",
      "  Ep 23/100: reward=-200.00 steps=200\n",
      "  Ep 24/100: reward=-200.00 steps=200\n",
      "  Ep 25/100: reward=-200.00 steps=200\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-200.00 steps=200\n",
      "  Ep 28/100: reward=-200.00 steps=200\n",
      "  Ep 29/100: reward=-200.00 steps=200\n",
      "  Ep 30/100: reward=-200.00 steps=200\n",
      "  Ep 31/100: reward=-200.00 steps=200\n",
      "  Ep 32/100: reward=-200.00 steps=200\n",
      "  Ep 33/100: reward=-200.00 steps=200\n",
      "  Ep 34/100: reward=-200.00 steps=200\n",
      "  Ep 35/100: reward=-200.00 steps=200\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-200.00 steps=200\n",
      "  Ep 38/100: reward=-200.00 steps=200\n",
      "  Ep 39/100: reward=-200.00 steps=200\n",
      "  Ep 40/100: reward=-200.00 steps=200\n",
      "  Ep 41/100: reward=-200.00 steps=200\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-200.00 steps=200\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-200.00 steps=200\n",
      "  Ep 46/100: reward=-200.00 steps=200\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-200.00 steps=200\n",
      "  Ep 51/100: reward=-200.00 steps=200\n",
      "  Ep 52/100: reward=-200.00 steps=200\n",
      "  Ep 53/100: reward=-200.00 steps=200\n",
      "  Ep 54/100: reward=-200.00 steps=200\n",
      "  Ep 55/100: reward=-200.00 steps=200\n",
      "  Ep 56/100: reward=-200.00 steps=200\n",
      "  Ep 57/100: reward=-200.00 steps=200\n",
      "  Ep 58/100: reward=-200.00 steps=200\n",
      "  Ep 59/100: reward=-200.00 steps=200\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-200.00 steps=200\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-200.00 steps=200\n",
      "  Ep 64/100: reward=-200.00 steps=200\n",
      "  Ep 65/100: reward=-200.00 steps=200\n",
      "  Ep 66/100: reward=-200.00 steps=200\n",
      "  Ep 67/100: reward=-200.00 steps=200\n",
      "  Ep 68/100: reward=-200.00 steps=200\n",
      "  Ep 69/100: reward=-200.00 steps=200\n",
      "  Ep 70/100: reward=-200.00 steps=200\n",
      "  Ep 71/100: reward=-200.00 steps=200\n",
      "  Ep 72/100: reward=-200.00 steps=200\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-200.00 steps=200\n",
      "  Ep 75/100: reward=-200.00 steps=200\n",
      "  Ep 76/100: reward=-200.00 steps=200\n",
      "  Ep 77/100: reward=-200.00 steps=200\n",
      "  Ep 78/100: reward=-200.00 steps=200\n",
      "  Ep 79/100: reward=-200.00 steps=200\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-200.00 steps=200\n",
      "  Ep 82/100: reward=-200.00 steps=200\n",
      "  Ep 83/100: reward=-200.00 steps=200\n",
      "  Ep 84/100: reward=-200.00 steps=200\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-200.00 steps=200\n",
      "  Ep 88/100: reward=-200.00 steps=200\n",
      "  Ep 89/100: reward=-200.00 steps=200\n",
      "  Ep 90/100: reward=-200.00 steps=200\n",
      "  Ep 91/100: reward=-200.00 steps=200\n",
      "  Ep 92/100: reward=-200.00 steps=200\n",
      "  Ep 93/100: reward=-200.00 steps=200\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-200.00 steps=200\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-200.00 steps=200\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-200.00 steps=200\n",
      "  Ep 100/100: reward=-200.00 steps=200\n",
      "Finished evaluations. avg=-200.00 best=-200.00 worst=-200.00\n",
      "Recording random episode index 80 for model MountainCar_DQN_lr-1e-05.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_lr-1e-05 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded video: videos/MountainCar-v0/MountainCar_DQN_lr-1e-05\\MountainCar_DQN_lr-1e-05-episode-0.mp4\n",
      "Recording complete. Total reward: -200.00, Steps:200, Duration: 1.29 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▁</td></tr><tr><td>best_reward</td><td>▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recorded_episode_duration_sec</td><td>▁</td></tr><tr><td>recorded_episode_index</td><td>▁</td></tr><tr><td>recorded_episode_reward</td><td>▁</td></tr><tr><td>recorded_episode_steps</td><td>▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>worst_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-200</td></tr><tr><td>best_reward</td><td>-200</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_duration_sec</td><td>1.28645</td></tr><tr><td>recorded_episode_index</td><td>80</td></tr><tr><td>recorded_episode_reward</td><td>-200</td></tr><tr><td>recorded_episode_steps</td><td>200</td></tr><tr><td>steps</td><td>200</td></tr><tr><td>worst_reward</td><td>-200</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_MountainCar_DQN_lr-1e-05_1763010775</strong> at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8zdozas8' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/8zdozas8</a><br> View project at: <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251113_071255-8zdozas8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model: MountainCar_DQN_mem-10000 .pth ===\n",
      "W&B run: test_MountainCar_DQN_mem-10000 _1763010782\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\wandb\\run-20251113_071302-a1c5fnna</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/a1c5fnna' target=\"_blank\">test_MountainCar_DQN_mem-10000 _1763010782</a></strong> to <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/a1c5fnna' target=\"_blank\">https://wandb.ai/omarelshereef-cairo-university/RL%20A2%20tests/runs/a1c5fnna</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep 1/100: reward=-140.00 steps=140\n",
      "  Ep 2/100: reward=-143.00 steps=143\n",
      "  Ep 3/100: reward=-143.00 steps=143\n",
      "  Ep 4/100: reward=-200.00 steps=200\n",
      "  Ep 5/100: reward=-151.00 steps=151\n",
      "  Ep 6/100: reward=-145.00 steps=145\n",
      "  Ep 7/100: reward=-143.00 steps=143\n",
      "  Ep 8/100: reward=-200.00 steps=200\n",
      "  Ep 9/100: reward=-144.00 steps=144\n",
      "  Ep 10/100: reward=-200.00 steps=200\n",
      "  Ep 11/100: reward=-142.00 steps=142\n",
      "  Ep 12/100: reward=-141.00 steps=141\n",
      "  Ep 13/100: reward=-145.00 steps=145\n",
      "  Ep 14/100: reward=-140.00 steps=140\n",
      "  Ep 15/100: reward=-144.00 steps=144\n",
      "  Ep 16/100: reward=-140.00 steps=140\n",
      "  Ep 17/100: reward=-144.00 steps=144\n",
      "  Ep 18/100: reward=-151.00 steps=151\n",
      "  Ep 19/100: reward=-140.00 steps=140\n",
      "  Ep 20/100: reward=-140.00 steps=140\n",
      "  Ep 21/100: reward=-199.00 steps=199\n",
      "  Ep 22/100: reward=-140.00 steps=140\n",
      "  Ep 23/100: reward=-147.00 steps=147\n",
      "  Ep 24/100: reward=-149.00 steps=149\n",
      "  Ep 25/100: reward=-144.00 steps=144\n",
      "  Ep 26/100: reward=-200.00 steps=200\n",
      "  Ep 27/100: reward=-140.00 steps=140\n",
      "  Ep 28/100: reward=-140.00 steps=140\n",
      "  Ep 29/100: reward=-141.00 steps=141\n",
      "  Ep 30/100: reward=-141.00 steps=141\n",
      "  Ep 31/100: reward=-146.00 steps=146\n",
      "  Ep 32/100: reward=-142.00 steps=142\n",
      "  Ep 33/100: reward=-142.00 steps=142\n",
      "  Ep 34/100: reward=-142.00 steps=142\n",
      "  Ep 35/100: reward=-145.00 steps=145\n",
      "  Ep 36/100: reward=-200.00 steps=200\n",
      "  Ep 37/100: reward=-151.00 steps=151\n",
      "  Ep 38/100: reward=-149.00 steps=149\n",
      "  Ep 39/100: reward=-144.00 steps=144\n",
      "  Ep 40/100: reward=-140.00 steps=140\n",
      "  Ep 41/100: reward=-143.00 steps=143\n",
      "  Ep 42/100: reward=-200.00 steps=200\n",
      "  Ep 43/100: reward=-144.00 steps=144\n",
      "  Ep 44/100: reward=-200.00 steps=200\n",
      "  Ep 45/100: reward=-147.00 steps=147\n",
      "  Ep 46/100: reward=-141.00 steps=141\n",
      "  Ep 47/100: reward=-200.00 steps=200\n",
      "  Ep 48/100: reward=-200.00 steps=200\n",
      "  Ep 49/100: reward=-200.00 steps=200\n",
      "  Ep 50/100: reward=-199.00 steps=199\n",
      "  Ep 51/100: reward=-142.00 steps=142\n",
      "  Ep 52/100: reward=-143.00 steps=143\n",
      "  Ep 53/100: reward=-149.00 steps=149\n",
      "  Ep 54/100: reward=-198.00 steps=198\n",
      "  Ep 55/100: reward=-141.00 steps=141\n",
      "  Ep 56/100: reward=-142.00 steps=142\n",
      "  Ep 57/100: reward=-144.00 steps=144\n",
      "  Ep 58/100: reward=-143.00 steps=143\n",
      "  Ep 59/100: reward=-143.00 steps=143\n",
      "  Ep 60/100: reward=-200.00 steps=200\n",
      "  Ep 61/100: reward=-140.00 steps=140\n",
      "  Ep 62/100: reward=-200.00 steps=200\n",
      "  Ep 63/100: reward=-141.00 steps=141\n",
      "  Ep 64/100: reward=-143.00 steps=143\n",
      "  Ep 65/100: reward=-146.00 steps=146\n",
      "  Ep 66/100: reward=-143.00 steps=143\n",
      "  Ep 67/100: reward=-141.00 steps=141\n",
      "  Ep 68/100: reward=-147.00 steps=147\n",
      "  Ep 69/100: reward=-143.00 steps=143\n",
      "  Ep 70/100: reward=-141.00 steps=141\n",
      "  Ep 71/100: reward=-149.00 steps=149\n",
      "  Ep 72/100: reward=-140.00 steps=140\n",
      "  Ep 73/100: reward=-200.00 steps=200\n",
      "  Ep 74/100: reward=-141.00 steps=141\n",
      "  Ep 75/100: reward=-142.00 steps=142\n",
      "  Ep 76/100: reward=-143.00 steps=143\n",
      "  Ep 77/100: reward=-141.00 steps=141\n",
      "  Ep 78/100: reward=-140.00 steps=140\n",
      "  Ep 79/100: reward=-146.00 steps=146\n",
      "  Ep 80/100: reward=-200.00 steps=200\n",
      "  Ep 81/100: reward=-150.00 steps=150\n",
      "  Ep 82/100: reward=-140.00 steps=140\n",
      "  Ep 83/100: reward=-143.00 steps=143\n",
      "  Ep 84/100: reward=-142.00 steps=142\n",
      "  Ep 85/100: reward=-200.00 steps=200\n",
      "  Ep 86/100: reward=-200.00 steps=200\n",
      "  Ep 87/100: reward=-140.00 steps=140\n",
      "  Ep 88/100: reward=-145.00 steps=145\n",
      "  Ep 89/100: reward=-141.00 steps=141\n",
      "  Ep 90/100: reward=-199.00 steps=199\n",
      "  Ep 91/100: reward=-141.00 steps=141\n",
      "  Ep 92/100: reward=-140.00 steps=140\n",
      "  Ep 93/100: reward=-151.00 steps=151\n",
      "  Ep 94/100: reward=-200.00 steps=200\n",
      "  Ep 95/100: reward=-145.00 steps=145\n",
      "  Ep 96/100: reward=-200.00 steps=200\n",
      "  Ep 97/100: reward=-144.00 steps=144\n",
      "  Ep 98/100: reward=-200.00 steps=200\n",
      "  Ep 99/100: reward=-148.00 steps=148\n",
      "  Ep 100/100: reward=-143.00 steps=143\n",
      "Finished evaluations. avg=-156.36 best=-140.00 worst=-200.00\n",
      "Recording random episode index 5 for model MountainCar_DQN_mem-10000 .pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\materials for college\\Fall 2025-2026\\Reinforcement Learning\\assignments\\assignment 2\\videos\\MountainCar-v0\\MountainCar_DQN_mem-10000 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'videos/MountainCar-v0/MountainCar_DQN_mem-10000 '",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_models_folder_with_wandb\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMountainCar-v0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_discrete_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./MountainCar models\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_tests\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_project\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRL A2 tests\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mtest_models_folder_with_wandb\u001b[39m\u001b[34m(models_folder, env_name, num_tests, num_discrete_actions, wandb_project, seed)\u001b[39m\n\u001b[32m    122\u001b[39m video_env.close()\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Log video to W&B\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m vid_files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_folder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f.endswith(\u001b[33m\"\u001b[39m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vid_files:\n\u001b[32m    127\u001b[39m     vid_files = \u001b[38;5;28msorted\u001b[39m(vid_files, key=\u001b[38;5;28;01mlambda\u001b[39;00m p: os.path.getmtime(os.path.join(video_folder, p)), reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'videos/MountainCar-v0/MountainCar_DQN_mem-10000 '"
     ]
    }
   ],
   "source": [
    "test_models_folder_with_wandb(\n",
    "    env_name=\"MountainCar-v0\",\n",
    "    num_discrete_actions=5,\n",
    "    models_folder=\"./MountainCar models\",\n",
    "    num_tests=100,\n",
    "    wandb_project=\"RL A2 tests\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
