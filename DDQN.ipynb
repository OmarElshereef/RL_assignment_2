{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:12:04.406692Z",
     "iopub.status.busy": "2025-11-13T01:12:04.406355Z",
     "iopub.status.idle": "2025-11-13T01:13:29.249895Z",
     "shell.execute_reply": "2025-11-13T01:13:29.249173Z",
     "shell.execute_reply.started": "2025-11-13T01:12:04.406667Z"
    },
    "id": "spc3_-fdnX-m",
    "outputId": "8029e60b-b4ef-4f9f-8229-792f7035cb9d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install all required libraries\n",
    "!pip install gymnasium[classic] torch wandb numpy\n",
    "!apt-get install -y ffmpeg\n",
    "!pip install pyvirtualdisplay\n",
    "!apt-get install -y xvfb\n",
    "\n",
    "# We'll also need to set up the virtual display\n",
    "# You can put this at the top of your main training cells\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:13:29.251921Z",
     "iopub.status.busy": "2025-11-13T01:13:29.251703Z",
     "iopub.status.idle": "2025-11-13T01:13:41.569719Z",
     "shell.execute_reply": "2025-11-13T01:13:41.569132Z",
     "shell.execute_reply.started": "2025-11-13T01:13:29.251899Z"
    },
    "id": "7O6-IKdNnbVd",
    "outputId": "756b2b01-cc3a-4f0a-adcc-3de63ca65f20",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Run this cell and paste your W&B API key when prompted\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:13:41.570786Z",
     "iopub.status.busy": "2025-11-13T01:13:41.570403Z",
     "iopub.status.idle": "2025-11-13T01:13:49.767653Z",
     "shell.execute_reply": "2025-11-13T01:13:49.766954Z",
     "shell.execute_reply.started": "2025-11-13T01:13:41.570767Z"
    },
    "id": "l8g_w-RXndHE",
    "outputId": "79b91e51-50f7-4d81-d2c8-f036bfbd00a1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Run a quick test to ensure wandb is working, as requested in the assignment\n",
    "print(\"Running wandb Quickstart demo...\")\n",
    "run = wandb.init(\n",
    "    project=\"cmps458_assignment2_quicktest\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"epochs\": 5,\n",
    "    },\n",
    ")\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = random.random()\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "run.finish()\n",
    "print(\"Quickstart demo finished. Check your wandb project.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:13:49.768942Z",
     "iopub.status.busy": "2025-11-13T01:13:49.768555Z",
     "iopub.status.idle": "2025-11-13T01:13:49.774365Z",
     "shell.execute_reply": "2025-11-13T01:13:49.773778Z",
     "shell.execute_reply.started": "2025-11-13T01:13:49.768913Z"
    },
    "id": "cLec1G88njUM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# Define the 'Transition' structure for our experiences\n",
    "# This is a highly efficient way to store (s, a, r, s') tuples\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    A simple replay buffer, as described in the DQN paper and lecture.\n",
    "    Stores transitions and allows for random sampling of batches.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # Use a deque as the memory. It automatically handles max length.\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition (state, action, next_state, reward)\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Select a random batch of transitions for training\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:13:49.775468Z",
     "iopub.status.busy": "2025-11-13T01:13:49.775284Z",
     "iopub.status.idle": "2025-11-13T01:13:53.277866Z",
     "shell.execute_reply": "2025-11-13T01:13:53.277287Z",
     "shell.execute_reply.started": "2025-11-13T01:13:49.775452Z"
    },
    "id": "UY3Q_GSgnm6i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    The Deep Q-Network model.\n",
    "    It's a simple feed-forward neural network.\n",
    "    Input: State (s)\n",
    "    Output: Q-value for each possible action Q(s, a)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # Define the layers, matching the lecture slide\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "        Uses ReLU activation functions as shown in the lecture\n",
    "        \"\"\"\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        # The final layer returns raw Q-values (no activation)\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:13:53.279105Z",
     "iopub.status.busy": "2025-11-13T01:13:53.278625Z",
     "iopub.status.idle": "2025-11-13T01:13:53.328470Z",
     "shell.execute_reply": "2025-11-13T01:13:53.327775Z",
     "shell.execute_reply.started": "2025-11-13T01:13:53.279078Z"
    },
    "id": "JibtiJ9qnpCy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Use the GPU if available, otherwise use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def select_action(state, policy_net, n_actions, epsilon):\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy policy.\n",
    "    With probability (1-epsilon), it exploits (picks the best action).\n",
    "    With probability (epsilon), it explores (picks a random action).\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    if sample > epsilon:\n",
    "        # EXPLOITATION: Get the best action from the policy_net\n",
    "        with torch.no_grad():\n",
    "            # policy_net(state) returns Q-values for all actions\n",
    "            # .max(1)[1] gets the *index* of the max Q-value\n",
    "            # .view(1, 1) reshapes it to [[action]]\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # EXPLORATION: Pick a random action\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:13:53.330809Z",
     "iopub.status.busy": "2025-11-13T01:13:53.330520Z",
     "iopub.status.idle": "2025-11-13T01:13:54.299490Z",
     "shell.execute_reply": "2025-11-13T01:13:54.298729Z",
     "shell.execute_reply.started": "2025-11-13T01:13:53.330791Z"
    },
    "id": "L-mnhjisnrCt",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class DiscretizeActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wraps the Pendulum-v1 environment to discretize its continuous action space.\n",
    "    The continuous action is a torque in [-2.0, 2.0].\n",
    "    We will map 3 discrete actions {0, 1, 2} to {-2.0, 0.0, 2.0}.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, num_discrete_actions=3):\n",
    "        super().__init__(env)\n",
    "        self.num_actions = num_discrete_actions\n",
    "        # Redefine the action space as discrete\n",
    "        self.action_space = gym.spaces.Discrete(self.num_actions)\n",
    "        # Create a mapping from discrete action index to continuous torque value\n",
    "        self.action_map = np.linspace(\n",
    "            self.env.action_space.low[0],\n",
    "            self.env.action_space.high[0],\n",
    "            self.num_actions\n",
    "        )\n",
    "\n",
    "    def action(self, action_index):\n",
    "        # Map the discrete action index back to a continuous value\n",
    "        continuous_action = [self.action_map[action_index]]\n",
    "        return np.array(continuous_action, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:13:54.300414Z",
     "iopub.status.busy": "2025-11-13T01:13:54.300202Z",
     "iopub.status.idle": "2025-11-13T01:13:54.308159Z",
     "shell.execute_reply": "2025-11-13T01:13:54.307200Z",
     "shell.execute_reply.started": "2025-11-13T01:13:54.300395Z"
    },
    "id": "UDUbmHCsntL1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def optimize_model(is_ddqn, policy_net, target_net, optimizer, memory, batch_size, gamma, loss_fn):\n",
    "    \"\"\"\n",
    "    Performs one step of optimization (backpropagation).\n",
    "    Samples a batch, computes the loss, and updates the policy_net.\n",
    "    \"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return  # Not enough experiences in memory to sample a batch\n",
    "\n",
    "    # Sample a batch of transitions from replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    # Transpose the batch (see PyTorch tutorial for details)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Create tensors for states, actions, and rewards\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Identify non-final next states\n",
    "    non_final_mask = torch.tensor(tuple(s is not None for s in batch.next_state), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # 1. Compute Q_current: Q(s, a)\n",
    "    # We get the Q-values for *all* actions from the policy_net,\n",
    "    # then use .gather() to select only the Q-value for the *action we took*.\n",
    "    Q_current = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 2. Compute Q_target: R + gamma * max_a' Q(s', a')\n",
    "    # This is the \"TD Target\"\n",
    "    Q_target_next = torch.zeros(batch_size, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if is_ddqn:\n",
    "            # --- DDQN Update Logic ---\n",
    "            # 1. Get the *action* (a*) from the *policy_net*\n",
    "            a_star = policy_net(non_final_next_states).argmax(dim=1).unsqueeze(1)\n",
    "            # 2. Get the *value* of that action (a*) from the *target_net*\n",
    "            Q_target_next[non_final_mask] = target_net(non_final_next_states).gather(1, a_star).squeeze(1)\n",
    "        else:\n",
    "            # --- Standard DQN Update Logic ---\n",
    "            # Get the max Q-value for the next state from the *target_net*\n",
    "            Q_target_next[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Combine immediate reward + discounted future value\n",
    "    Q_target = (Q_target_next * gamma) + reward_batch\n",
    "\n",
    "    # 3. Compute Loss\n",
    "    # We use Smooth L1 Loss (Huber Loss) as recommended\n",
    "    loss = loss_fn(Q_current, Q_target.unsqueeze(1))\n",
    "\n",
    "    # 4. Optimize the model (Backpropagation)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Gradient Clipping (as mentioned in lecture)\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T01:13:54.309293Z",
     "iopub.status.busy": "2025-11-13T01:13:54.309067Z",
     "iopub.status.idle": "2025-11-13T01:13:54.331610Z",
     "shell.execute_reply": "2025-11-13T01:13:54.330810Z",
     "shell.execute_reply.started": "2025-11-13T01:13:54.309270Z"
    },
    "id": "edvLr2qnnvLb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from collections import namedtuple\n",
    "import itertools # Used for counting steps\n",
    "\n",
    "# --- Main Training Function (MODIFIED to accept all HPs) ---\n",
    "def run_experiment(\n",
    "    env_name,\n",
    "    is_ddqn,\n",
    "    num_episodes=1000,\n",
    "    # --- Hyperparameters to tune --...\"\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    eps_start=0.9,\n",
    "    eps_end=0.05,\n",
    "    eps_decay=1000,\n",
    "    tau=0.005,               # Soft update param\n",
    "    lr=1e-3,\n",
    "    memory_size=10000,\n",
    "    # --- Wandb tracking ---\n",
    "    wandb_run_name=None,\n",
    "    wandb_group=None\n",
    "):\n",
    "    \"\"\"\n",
    "    The main training script.\n",
    "    Initializes and runs the full training loop for one agent.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Initialize wandb\n",
    "    model_type = \"DDQN\" if is_ddqn else \"DQN\"\n",
    "\n",
    "    if wandb_run_name is None:\n",
    "        wandb_run_name = f\"{env_name}_{model_type}_{int(time.time())}\"\n",
    "\n",
    "    # All config parameters are logged to wandb\n",
    "    config = {\n",
    "        \"env_name\": env_name, \"model_type\": model_type, \"num_episodes\": num_episodes,\n",
    "        \"batch_size\": batch_size, \"gamma\": gamma, \"eps_start\": eps_start,\n",
    "        \"eps_end\": eps_end, \"eps_decay\": eps_decay, \"tau\": tau, \"lr\": lr,\n",
    "        \"memory_size\": memory_size,\n",
    "    }\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"cmps458_final_assignment2\",\n",
    "        name=wandb_run_name,\n",
    "        group=wandb_group,  # This groups runs in the wandb dashboard\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # 2. Setup Environment\n",
    "    if env_name == \"Pendulum-v1\":\n",
    "        env = gym.make(env_name)\n",
    "        env = DiscretizeActionWrapper(env, num_discrete_actions=5)\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    state, info = env.reset()\n",
    "    n_observations = len(state)\n",
    "\n",
    "    # 3. Initialize Models and Optimizer\n",
    "    policy_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net = DQN(n_observations, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "    memory = ReplayMemory(memory_size)\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    print(f\"--- Starting Training for {wandb_run_name} (Group: {wandb_group}) ---\")\n",
    "    print(f\"Config: {config}\")\n",
    "\n",
    "    # 4. Training Loop\n",
    "    steps_done = 0\n",
    "    \n",
    "    # --- ADDED: Start timer ---\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        episode_duration = 0\n",
    "        episode_reward = 0\n",
    "        for t in itertools.count():\n",
    "            epsilon = eps_end + (eps_start - eps_end) * math.exp(-1. * steps_done / eps_decay)\n",
    "            action = select_action(state, policy_net, n_actions, epsilon)\n",
    "            steps_done += 1\n",
    "\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            episode_duration += 1\n",
    "            episode_reward += reward\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            optimize_model(is_ddqn, policy_net, target_net, optimizer, memory, batch_size, gamma, loss_fn)\n",
    "\n",
    "            # Soft update of the target network's weights\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Log episode results to wandb\n",
    "        wandb.log({\"episode\": i_episode, \"duration\": episode_duration, \"episode_reward\": episode_reward, \"epsilon\": epsilon})\n",
    "\n",
    "    # --- ADDED: End timer and calculate duration ---\n",
    "    end_time = time.time()\n",
    "    training_duration_seconds = end_time - start_time\n",
    "\n",
    "    print(f\"--- Training Complete for {wandb_run_name} ---\")\n",
    "    print(f\"Total training time: {training_duration_seconds:.2f} seconds\") # --- ADDED ---\n",
    "\n",
    "    model_path = f\"{wandb_run_name}.pth\"\n",
    "    torch.save(policy_net.state_dict(), model_path)\n",
    "    \n",
    "    # --- ADDED: Log training time to wandb before finishing ---\n",
    "    wandb.log({\"training_time_seconds\": training_duration_seconds})\n",
    "    \n",
    "    run.finish()\n",
    "    return model_path, config\n",
    "\n",
    "\n",
    "# --- Test & Record Function (MODIFIED to accept wandb grouping) ---\n",
    "# --- Test & Record Function (CORRECTED) ---\n",
    "def test_and_record(\n",
    "    env_name,\n",
    "    model_type,\n",
    "    model_path,\n",
    "    num_tests=100,\n",
    "    wandb_run_name=None,\n",
    "    wandb_group=None,\n",
    "    config=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Tests a trained agent for 100 episodes and logs the duration.\n",
    "    Records a video of the first test episode.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Testing {model_type} on {env_name} ---\")\n",
    "\n",
    "    if wandb_run_name is None:\n",
    "        wandb_run_name = f\"test_{model_type}\"\n",
    "\n",
    "    if config is None:\n",
    "        config={\"env_name\": env_name, \"model_type\": model_type, \"num_tests\": num_tests}\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"cmps458_final_assignment2_tests\",\n",
    "        name=wandb_run_name,\n",
    "        group=wandb_group,\n",
    "        config=config,\n",
    "        reinit=True # Add reinit=True to allow this in a loop\n",
    "    )\n",
    "\n",
    "    # 2. Setup Environment\n",
    "    if env_name == \"Pendulum-v1\":\n",
    "        base_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        env = DiscretizeActionWrapper(base_env, num_discrete_actions=5)\n",
    "    else:\n",
    "        env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "    # 3. Wrap for Video Recording\n",
    "    video_folder = f\"videos/{wandb_run_name}\"\n",
    "    env = RecordVideo(\n",
    "        env, video_folder=video_folder,\n",
    "        episode_trigger=lambda e: e == 0,\n",
    "        name_prefix=f\"{model_type}\"\n",
    "    )\n",
    "\n",
    "    # 4. Load Model\n",
    "    n_actions = env.action_space.n\n",
    "    state, info = env.reset()\n",
    "    n_observations = len(state)\n",
    "\n",
    "    model = DQN(n_observations, n_actions).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # 5. Run 100 Tests\n",
    "    test_durations = []\n",
    "    test_rewards = []\n",
    "    for i_episode in range(num_tests):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        episode_duration = 0\n",
    "        episode_reward = 0\n",
    "        for t in itertools.count():\n",
    "            action = select_action(state, model, n_actions, epsilon=0.0) # Always greedy\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            episode_duration += 1\n",
    "            episode_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        wandb.log({\"test_episode\": i_episode, \"test_duration\": episode_duration, \"test_reward\": episode_reward})\n",
    "        test_durations.append(episode_duration)\n",
    "        test_rewards.append(episode_reward)\n",
    "\n",
    "    print(f\"Testing complete. Video saved in '{video_folder}'\")\n",
    "    avg_duration = sum(test_durations) / num_tests\n",
    "    avg_reward = sum(test_rewards) / num_tests\n",
    "    print(f\"Average duration: {avg_duration:.2f}, Average reward: {avg_reward:.2f}\")\n",
    "\n",
    "    wandb.log({\"avg_test_duration\": avg_duration, \"avg_test_reward\": avg_reward})\n",
    "\n",
    "    # --- THIS IS THE CRUCIAL FIX ---\n",
    "    # Close the environment to finalize the video recording.\n",
    "    env.close()\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-13T03:34:37.268Z",
     "iopub.execute_input": "2025-11-13T01:13:54.332752Z",
     "iopub.status.busy": "2025-11-13T01:13:54.332468Z"
    },
    "id": "keiNQVfeoHlW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "\n",
    "\n",
    "# --- CartPole-v1 Hyperparameter Sweep ---\n",
    "# 1. Test Learning Rates (lr)\n",
    "lr_tests = [1e-3, 1e-2, 3e-4, 1e-5]\n",
    "gamma_tests = [0.99, 0.9, 0.5, 0.1]\n",
    "eps_decay_tests = [1000, 500, 2500, 5000] # Fast vs. Patient exploration \n",
    "memory_tests = [10000, 500, 50000]\n",
    "batch_size_tests = [128, 64, 256] # Noisy/Fast vs. Stable/Slow\n",
    "\n",
    "environments = [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"]\n",
    "\n",
    "for environment in environments:\n",
    "    for lr in lr_tests:\n",
    "        if environment == \"CartPole-v1\":\n",
    "            continue;\n",
    "        group_name = f\"{environment}_LR_Test\"\n",
    "        run_name = f\"{environment}_DDQN_lr-{lr}\"\n",
    "\n",
    "        model_path, config = run_experiment(\n",
    "            env_name=environment, is_ddqn=True,\n",
    "            lr=lr, # <-- Variable we are testing\n",
    "            wandb_run_name=run_name, wandb_group=group_name\n",
    "        )\n",
    "        test_and_record(\n",
    "            environment, run_name, model_path,\n",
    "            wandb_run_name=f\"test_{run_name}\", wandb_group=group_name, config=config\n",
    "        )\n",
    "\n",
    "    # 2. Test Discount Factors (gamma)\n",
    "    for gamma in gamma_tests:\n",
    "        if environment == \"CartPole-v1\":\n",
    "            continue;\n",
    "        group_name = f\"{environment}_Gamma_Test\"\n",
    "        run_name = f\"{environment}_DDQN_gamma-{gamma}\"\n",
    "\n",
    "        model_path, config = run_experiment(\n",
    "            env_name=environment, is_ddqn=True,\n",
    "            gamma=gamma, # <-- Variable we are testing\n",
    "            wandb_run_name=run_name, wandb_group=group_name\n",
    "        )\n",
    "        test_and_record(\n",
    "            environment, run_name, model_path,\n",
    "            wandb_run_name=f\"test_{run_name}\", wandb_group=group_name, config=config\n",
    "        )\n",
    "\n",
    "    # 3. Test Epsilon Decay Rates (Crucial for this env)\n",
    "    for eps_decay in eps_decay_tests:\n",
    "        if environment == \"CartPole-v1\" and eps_decay in [1000, 500]:\n",
    "            continue;\n",
    "        group_name = f\"{environment}_EpsDecay_Test\"\n",
    "        run_name = f\"{environment}_DDQN_eps-{eps_decay}\"\n",
    "\n",
    "        model_path, config = run_experiment(\n",
    "            env_name=environment, is_ddqn=True,\n",
    "            eps_decay=eps_decay, # <-- Variable we are testing\n",
    "            wandb_run_name=run_name, wandb_group=group_name\n",
    "        )\n",
    "        test_and_record(\n",
    "            environment, run_name, model_path,\n",
    "            wandb_run_name=f\"test_{run_name}\", wandb_group=group_name, config=config\n",
    "        )\n",
    "\n",
    "    # 4. Test Replay Memory Sizes\n",
    "    for mem_size in memory_tests:\n",
    "        group_name = f\"{environment}_Memory_Test\"\n",
    "        run_name = f\"{environment}_DDQN_mem-{mem_size}\"\n",
    "\n",
    "        model_path, config = run_experiment(\n",
    "            env_name=environment, is_ddqn=True,\n",
    "            memory_size=mem_size, # <-- Variable we are testing\n",
    "            wandb_run_name=run_name, wandb_group=group_name\n",
    "        )\n",
    "        test_and_record(\n",
    "            environment, run_name, model_path,\n",
    "            wandb_run_name=f\"test_{run_name}\", wandb_group=group_name, config=config\n",
    "        )\n",
    "\n",
    "    # 5. Test Batch Sizes\n",
    "    for batch_size in batch_size_tests:\n",
    "        group_name = f\"{environment}_BatchSize_Test\"\n",
    "        run_name = f\"{environment}_DDQN_batch-{batch_size}\"\n",
    "\n",
    "        model_path, config = run_experiment(\n",
    "            env_name=environment, is_ddqn=True,\n",
    "            batch_size=batch_size, # <-- Variable we are testing\n",
    "            wandb_run_name=run_name, wandb_group=group_name\n",
    "        )\n",
    "        test_and_record(\n",
    "            environment, run_name, model_path,\n",
    "            wandb_run_name=f\"test_{run_name}\", wandb_group=group_name, config=config\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
